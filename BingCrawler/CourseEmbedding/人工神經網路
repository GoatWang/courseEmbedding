


人工神經網路 - 維基百科，自由的百科全書































 







人工神經網路

維基百科，自由的百科全書


					前往：					導覽，					搜尋








本條目介紹的是模擬生物神經網路的數學模型。關於生物的神經網路，請見「生物神經網路」。




機器學習與資料探勘





問題





分類
聚類
回歸
異常檢測
關聯規則
強化學習
結構預測（英語：Structured prediction）
特徵學習
線上學習（英語：Online machine learning）
半監督學習（英語：Semi-supervised learning）
語法歸納（英語：Grammar induction）






監督學習
(分類 · 回歸)






決策樹
表徵（裝袋, 提升，隨機森林）
k-NN
線性回歸
樸素貝葉斯
神經網路
邏輯回歸
感知器
支援向量機（SVM）
相關向量機（RVM）





聚類





BIRCH（英語：BIRCH）
層次（英語：Hierarchical clustering）
k平均
期望最大化（EM）

DBSCAN
OPTICS（英語：OPTICS）
均值飄移（英語：Mean shift）





降維





因子分析（英語：Factor analysis）
CCA
ICA
LDA
NMF（英語：Non-negative matrix factorization）
PCA
LASSO
t-SNE（英語：t-distributed stochastic neighbor embedding）





結構預測（英語：Structured prediction）





機率圖模型（貝葉斯網路，CRF, HMM）





異常檢測





k-NN
局部離群因子（英語：Local outlier factor）





神經網路





自編碼（英語：Autoencoder）
深度學習
多層感知機
RNN
受限玻爾茲曼機
SOM
CNN





理論





偏差/方差困境（英語：Bias–variance tradeoff）
計算學習理論（英語：Computational learning theory）
經驗風險最小化（英語：Empirical risk minimization）
PAC學習（英語：Probably approximately correct learning）
統計學習
VC理論








閱
論
編





在機器學習和認知科學領域，人工神經網路（英文：artificial neural network，縮寫ANN），簡稱神經網路（英文：neural network，縮寫NN）或類神經網路，是一種模仿生物神經網路(動物的中樞神經系統，特別是大腦)的結構和功能的數學模型或計算模型，用於對函式進行估計或近似。神經網路由大量的人工神經元聯結進行計算。大多數情況下人工神經網路能在外界資訊的基礎上改變內部結構，是一種自適應系統。[來源請求]現代神經網路是一種非線性統計性資料建模工具。典型的神經網路具有以下三個部分：

結構 （Architecture） 結構指定了網路中的變數和它們的拓撲關係。例如，神經網路中的變數可以是神經元連線的權重（weights）和神經元的激勵值（activities of the neurons）。
激勵函式（Activity Rule） 大部分神經網路模型具有一個短時間尺度的動力學規則，來定義神經元如何根據其他神經元的活動來改變自己的激勵值。一般激勵函式依賴於網路中的權重（即該網路的參數）。
學習規則（Learning Rule）學習規則指定了網路中的權重如何隨著時間推進而調整。這一般被看做是一種長時間尺度的動力學規則。一般情況下，學習規則依賴於神經元的激勵值。它也可能依賴於監督者提供的目標值和當前權重的值。

例如，用於手寫識別的一個神經網路，有一組輸入神經元。輸入神經元會被輸入圖像的資料所激發。在激勵值被加權並通過一個函式（由網路的設計者確定）後，這些神經元的激勵值被傳遞到其他神經元。這個過程不斷重複，直到輸出神經元被激發。最後，輸出神經元的激勵值決定了識別出來的是哪個字母。
神經網路的構築理念是受到生物（人或其他動物）神經網路功能的運作啟發而產生的。人工神經網路通常是通過一個基於數學統計學類型的學習方法（Learning Method）得以最佳化，所以人工神經網路也是數學統計學方法的一種實際應用，通過統計學的標準數學方法我們能夠得到大量的可以用函式來表達的局部結構空間，另一方面在人工智慧學的人工感知領域，我們通過數學統計學的應用可以來做人工感知方面的決定問題(也就是說通過統計學的方法，人工神經網路能夠類似人一樣具有簡單的決定能力和簡單的判斷能力)，這種方法比起正式的邏輯學推理演算更具有優勢。
和其他機器學習方法一樣，神經網路已經被用於解決各種各樣的問題，例如機器視覺和語音識別。這些問題都是很難被傳統基於規則的編程所解決的。



目錄


1 背景
2 歷史

2.1 赫布型學習
2.2 反向傳播演算法與復興
2.3 2006年之後的進展


3 神經元
4 神經元網路

4.1 單層神經元網路
4.2 多層神經元網路


5 人工神經網路的實用性
6 人工神經元網路模型
7 基本結構
8 學習過程
9 種類
10 理論性質

10.1 計算能力
10.2 容量
10.3 收斂性
10.4 綜合統計


11 參見
12 外部連結
13 參考文獻



背景[編輯]
對人類中樞神經系統的觀察啟發了人工神經網路這個概念。在人工神經網路中，簡單的人工節點，稱作神經元（neurons），連線在一起形成一個類似生物神經網路的網狀結構。
人工神經網路目前沒有一個統一的正式定義。不過，具有下列特點的統計模型可以被稱作是「神經化」的：

具有一組可以被調節的權重，換言之，被學習演算法調節的數值參數，並且
可以估計輸入資料的非線性函式關係

這些可調節的權重可以被看做神經元之間的連線強度。
人工神經網路與生物神經網路的相似之處在於，它可以集體地、並列地計算函式的各個部分，而不需要描述每一個單元的特定任務。神經網路這個詞一般指統計學、認知心理學和人工智慧領域使用的模型，而控制中央神經系統的神經網路屬於理論神經學和計算神經學。[1]
在神經網路的現代軟體實現中，被生物學啟發的那種方法已經很大程度上被拋棄了，取而代之的是基於統計學和訊號處理的更加實用的方法。在一些軟體系統中，神經網路或者神經網路的一部分（例如人工神經元）是大型系統中的一個部分。這些系統結合了適應性的和非適應性的元素。雖然這種系統使用的這種更加普遍的方法更適宜解決現實中的問題，但是這和傳統的連線主義人工智慧已經沒有什麼關聯了。不過它們還有一些共同點：非線性、分散式、並列化，局部性計算以及適應性。從歷史的角度講，神經網路模型的應用標誌著二十世紀八十年代後期從高度符號化的人工智慧（以用條件規則表達知識的專家系統為代表）向低符號化的機器學習（以用動力系統的參數列達知識為代表）的轉變。
歷史[編輯]
沃倫·麥卡洛克和沃爾特·皮茨（1943）[2]基於數學和一種稱為閾值邏輯的演算法創造了一種神經網路的計算模型。這種模型使得神經網路的研究分裂為兩種不同研究思路。一種主要關註大腦中的生物學過程，另一種主要關註神經網路在人工智慧里的應用。
赫布型學習[編輯]
二十世紀40年代後期，心理學家唐納德·赫布根據神經可塑性的機制創造了一種對學習的假說，現在稱作赫布型學習。赫布型學習被認為是一種典型的非監督式學習規則，它後來的變種是長期增強作用的早期模型。從1948年開始，研究人員將這種計算模型的思想應用到B型圖靈機上。
法利和韋斯利·A·克拉克（1954）[3]首次使用電腦，當時稱作計算機，在MIT模擬了一個赫布網路。
弗蘭克·羅森布拉特（1956）[4]創造了感知機。這是一種模式識別演算法，用簡單的加減法實現了兩層的電腦學習網路。羅森布拉特也用數學符號描述了基本感知機里沒有的迴路，例如異或迴路。這種迴路一直無法被神經網路處理，直到Paul Werbos(1975)創造了反向傳播演算法。
在馬文·明斯基和西摩·帕爾特（1969）發表了一項關於機器學習的研究以後，神經網路的研究停滯不前。他們發現了神經網路的兩個關鍵問題。第一是基本感知機無法處理異或迴路。第二個重要的問題是電腦沒有足夠的能力來處理大型神經網路所需要的很長的計算時間。直到電腦具有更強的計算能力之前，神經網路的研究進展緩慢。
反向傳播演算法與復興[編輯]
後來出現的一個關鍵的進展是反向傳播演算法（Werbos 1975）。這個演算法有效地解決了異或的問題，還有更普遍的訓練多層神經網路的問題。
在二十世紀80年代中期，分散式並列處理（當時稱作聯結主義）流行起來。David E. Rumelhart和James McClelland（1986）的教材對於聯結主義在電腦模擬神經活動中的應用提供了全面的論述。
神經網路傳統上被認為是大腦中的神經活動的簡化模型，雖然這個模型和大腦的生理結構之間的關聯存在爭議。人們不清楚人工神經網路能多大程度地反映大腦的功能。
支援向量機和其他更簡單的方法（例如線性分類器）在機器學習領域的流行度逐漸超過了神經網路，但是在2000年代後期出現的深度學習重新激發了人們對神經網路的興趣。
2006年之後的進展[編輯]
人們用CMOS創造了用於生物物理模擬和神經形態計算的計算裝置。最新的研究顯示了用於大型主成分分析和捲積神經網路的奈米裝置[5]具有良好的前景。如果成功的話，這會創造出一種新的神經計算裝置[6]，因為它依賴於學習而不是編程，並且它從根本上就是模擬的而不是數位化的，雖然它的第一個例項可能是數位化的CMOS裝置。
在2009到2012年之間，Jürgen Schmidhuber在Swiss AI Lab IDSIA的研究小組研發的遞迴神經網路和深前饋神經網路贏得了8項關於模式識別和機器學習的國際比賽。[7][8]例如，Alex Graves et al.的雙向、多維的LSTM贏得了2009年ICDAR的3項關於連筆字識別的比賽，而且之前並不知道關於將要學習的3種語言的資訊。[9][10][11][12]
IDSIA的Dan Ciresan和同事根據這個方法編寫的基於GPU的實現贏得了多項模式識別的比賽，包括IJCNN 2011交通標誌識別比賽等等。[13][14]他們的神經網路也是第一個在重要的基準測試中（例如IJCNN 2012交通標誌識別和NYU的揚·勒丘恩（Yann LeCun）的MNIST手寫數字問題）能達到或超過人類水平的人工模式識別器。
類似1980年Kunihiko Fukushima發明的neocognitron[15]和視覺標準結構[16]（由David H. Hubel和Torsten Wiesel在初級視皮層中發現的那些簡單而又複雜的細胞啟發）那樣有深度的、高度非線性的神經結構可以被多倫多大學傑夫·辛頓實驗室的非監督式學習方法所訓練。[17][18][19]
神經元[編輯]
神經元示意圖：




a1~an為輸入向量的各個分量
w1~wn為神經元各個突觸的權值
b為偏置
f為傳遞函式，通常為非線性函式。一般有traingd(),tansig(),hardlim()。以下預設為hardlim()
t為神經元輸出

數學表示 



t
=
f
(




W
′

→






A
→



+
b
)


{\displaystyle t=f({\vec {W'}}{\vec {A}}+b)}










W
→





{\displaystyle {\vec {W}}}

為權向量 ，







W
′

→





{\displaystyle {\vec {W'}}}

為






W
→





{\displaystyle {\vec {W}}}

的轉置







A
→





{\displaystyle {\vec {A}}}

為輸入向量




b


{\displaystyle b}

為偏置




f


{\displaystyle f}

為傳遞函式

可見，一個神經元的功能是求得輸入向量與權向量的內積後，經一個非線性傳遞函式得到一個純量結果。
單個神經元的作用：把一個n維向量空間用一個超平面分割成兩部分（稱之為判斷邊界），給定一個輸入向量，神經元可以判斷出這個向量位於超平面的哪一邊。
該超平面的方程:







W
′

→






p
→



+
b
=
0


{\displaystyle {\vec {W'}}{\vec {p}}+b=0}










W
→





{\displaystyle {\vec {W}}}

權向量




b


{\displaystyle b}

偏置







p
→





{\displaystyle {\vec {p}}}

超平面上的向量

[20]
神經元網路[編輯]
單層神經元網路[編輯]
是最基本的神經元網路形式，由有限個神經元構成，所有神經元的輸入向量都是同一個向量。由於每一個神經元都會產生一個純量結果，所以單層神經元的輸出是一個向量，向量的維數等於神經元的數目。
示意圖：



多層神經元網路[編輯]
人工神經網路的實用性[編輯]
人工神經網路是一個能夠學習，能夠總結歸納的系統，也就是說它能夠通過已知資料的實驗運用來學習和歸納總結。人工神經網路通過對局部情況的對照比較（而這些比較是基於不同情況下的自動學習和要實際解決問題的複雜性所決定的），它能夠推理產生一個可以自動識別的系統。與之不同的基於符號系統下的學習方法，它們也具有推理功能，只是它們是建立在邏輯演算法的基礎上，也就是說它們之所以能夠推理，基礎是需要有一個推理演算法則的集合。
人工神經元網路模型[編輯]
通常來說，一個人工神經元網路是由一個多層神經元結構組成，每一層神經元擁有輸入（它的輸入是前一層神經元的輸出）和輸出，每一層（我們用符號記做）Layer(i)是由Ni(Ni代表在第i層上的N)個網路神經元組成，每個Ni上的網路神經元把對應在Ni-1上的神經元輸出做為它的輸入，我們把神經元和與之對應的神經元之間的連線用生物學的名稱，叫做突觸（英語：Synapse），在數學模型中每個突觸有一個加權數值，我們稱做權重，那麼要計算第i層上的某個神經元所得到的勢能等於每一個權重乘以第i-1層上對應的神經元的輸出，然後全體求和得到了第i層上的某個神經元所得到的勢能，然後勢能數值通過該神經元上的活化函數（activation function，常是∑函式（英語：Sigmoid function）以控制輸出大小，因為其可微分且連續，方便差量規則（英語：Delta rule）處理。求出該神經元的輸出，註意的是該輸出是一個非線性的數值，也就是說通過激勵函式求的數值根據極限值來判斷是否要啟用該神經元，換句話說我們對一個神經元網路的輸出是否線性不感興趣。
基本結構[編輯]
一種常見的多層結構的前饋網路（Multilayer Feedforward Network）由三部分組成，

輸入層（Input layer），眾多神經元（Neuron）接受大量非線形輸入訊息。輸入的訊息稱為輸入向量。
輸出層（Output layer），訊息在神經元鏈接中傳輸、分析、權衡，形成輸出結果。輸出的訊息稱為輸出向量。
隱藏層（Hidden layer），簡稱「隱層」，是輸入層和輸出層之間眾多神經元和鏈接組成的各個層面。隱層可以有多層，習慣上會用一層。隱層的節點（神經元）數目不定，但數目越多神經網路的非線性越顯著，從而神經網路的強健性（robustness）（控制系統在一定結構、大小等的參數攝動下，維持某些效能的特性。）更顯著。習慣上會選輸入節點1.2至1.5倍的節點。

神經網路的類型已經演變出很多種，這種分層的結構也並不是對所有的神經網路都適用。
學習過程[編輯]
通過訓練樣本的校正，對各個層的權重進行校正（learning）而建立模型的過程，稱為自動學習過程（training algorithm）。具體的學習方法則因網路結構和模型不同而不同，常用反向傳播演算法(Backpropagation/倒傳遞/逆傳播，以output利用一次微分Delta rule（英語：Delta rule）來修正weight)來驗證。
參見：神經網路介紹
種類[編輯]
人工神經網路分類為以下兩種:
1.依學習策略（Algorithm）分類主要有：

監督式學習網路（Supervised Learning Network）為主
無監督式學習網路（Unsupervised Learning Network）
混合式學習網路（Hybrid Learning Network）
聯想式學習網路（Associate Learning Network）
最適化學習網路（Optimization Application Network）


2.依網路架構（Connectionism）分類主要有:

前向式架構（Feed Forward Network）
回饋式架構（Recurrent Network）
強化式架構（Reinforcement Network）

理論性質[編輯]
計算能力[編輯]
多層感知器（MLP）是一個通用的函式逼近器，由Cybenko定理證明。然而，證明不是由所要求的神經元數量或權重來推斷的。 Hava Siegelmann和Eduardo D. Sontag的工作證明瞭，一個具有有理數權重值的特定遞迴結構（與全精度實數權重值相對應）相當於一個具有有限數量的神經元和標準的線性關係的通用圖靈機。[21] 他們進一步表明，使用無理數權重值會產生一個超圖靈機。
容量[編輯]
人工神經網路模型有一個屬性，稱為「容量」，這大致相當於他們可以塑造任何函式的能力。它與可以被儲存在網路中的資訊的數量和複雜性相關。
收斂性[編輯]
沒有什麼通常意義上的收斂，因為它取決於一些因素。首先，函式可能存在許多局部極小值。這取決於成本函式和模型。其次，使用最佳化方法在遠離局部最小值時可能無法保證收斂。第三，對大量的資料或參數，一些方法變得不切實際。在一般情況下，我們發現，理論保證的收斂不能成為實際應用的一個可靠的指南。
綜合統計[編輯]
在目標是建立一個普遍系統的應用程式中，過度訓練的問題出現了。這出現在迴旋或過度具體的系統中當網路的容量大大超過所需的自由參數。為了避免這個問題，有兩個方向：第一個是使用交叉驗證和類似的技術來檢查過度訓練的存在和選擇最佳參數如最小化泛化誤差。二是使用某種形式的正規化。這是一個在機率化（貝葉斯）框架里出現的概念，其中的正則化可以通過為簡單模型選擇一個較大的先驗機率模型進行；而且在統計學習理論中，其目的是最大限度地減少了兩個數量：「風險」和「結構風險」，相當於誤差在訓練集和由於過度擬合造成的預測誤差。
參見[編輯]

生物神經網路
人工智慧
機器學習
感知機

外部連結[編輯]

Performance comparison of neural network algorithms tested on UCI data sets
A close view to Artificial Neural Networks Algorithms
開放式目錄計劃中和Neural Networks相關的內容
A Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.
Neural Networks in Materials Science
A practical tutorial on Neural Networks
Applications of neural networks
XOR 實例

參考文獻[編輯]


^ Hentrich, Michael William. Methodology and Coronary Artery Disease Cure. 2015-08-16. doi:10.1709/TIT.2015.1083925. 
^ McCulloch, Warren S.; Pitts, Walter. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics. 1943-12-01, 5 (4): 115–133. ISSN 0007-4985. doi:10.1007/BF02478259 （英語）. 
^ Farley, B.; Clark, W. Simulation of self-organizing systems by digital computer. Transactions of the IRE Professional Group on Information Theory. 1954-09-01, 4 (4): 76–84. ISSN 2168-2690. doi:10.1109/TIT.1954.1057468. 
^ Rochester, N.; Holland, J.; Haibt, L.; Duda, W. Tests on a cell assembly theory of the action of the brain, using a large digital computer. IRE Transactions on Information Theory. 1956-09-01, 2 (3): 80–93. ISSN 0096-1000. doi:10.1109/TIT.1956.1056810. 
^ Yang, J. J.; Pickett, M. D.; Li, X. M.; Ohlberg, D. A. A.; Stewart, D. R.; Williams, R. S. Memristive switching mechanism for metal/oxide/metal nanodevices. Nat. Nanotechnol. 2008, 3: 429–433. doi:10.1038/nnano.2008.160. 
^ Strukov, D. B.; Snider, G. S.; Stewart, D. R.; Williams, R. S. The missing memristor found. Nature. 2008, 453: 80–83. PMID 18451858. doi:10.1038/nature06932. 
^ 2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012
^ http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions 2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012
^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), 7–10 December 2009, Vancouver, BC, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552.
^ Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition (PDF). IEEE Transactions on Pattern Analysis and Machine Intelligence. 2009, 31 (5). 
^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552
^ Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2009, 31 (5). 
^ D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.
^ D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.
^ Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics. 1980, 36 (4): 93–202. PMID 7370364. doi:10.1007/BF00344251. 
^ M Riesenhuber, T Poggio. Hierarchical models of object recognition in cortex. Nature neuroscience, 1999.
^ Deep belief networks at Scholarpedia.
^ Hinton, G. E.; Osindero, S.; Teh, Y. W. A Fast Learning Algorithm for Deep Belief Nets (PDF). Neural Computation. 2006, 18 (7): 1527–1554. PMID 16764513. doi:10.1162/neco.2006.18.7.1527. 
^ Hinton, G. E.; Osindero, S.; Teh, Y. A fast learning algorithm for deep belief nets (PDF). Neural Computation. 2006, 18 (7): 1527–1554. PMID 16764513. doi:10.1162/neco.2006.18.7.1527. 
^ Hagan, Martin. Neural Network Design. PWS Publishing Company. 1996. ISBN 7-111-10841-8. 
^ Siegelmann, H.T.; Sontag, E.D. Turing computability with neural nets (PDF). Appl. Math. Lett. 1991, 4 (6): 77–80. doi:10.1016/0893-9659(91)90080-F. 







權威控制



GND: 4226127-2
NDL: 01165604












 
						取自 "https://zh.wikipedia.org/w/index.php?title=人工神經網絡&oldid=43795095"					
5 個分類：資訊科學人工智慧機器學習計算機科學神經網路隱藏分類：CS1英語來源 (en)有未列明來源語句的條目包含規範控制信息的維基百科條目 



導覽選單


個人工具

沒有登入對話貢獻建立帳號登入 



命名空間

條目
討論




台灣正體



不轉換
簡體
繁體
大陸簡體
香港繁體
澳門繁體
馬新簡體
台灣正體






查看

閱讀
編輯
檢視歷史



更多







搜尋



 







導航


首頁分類索引特色內容新聞動態近期變更隨機條目 



說明


說明維基社群方針與指引互助客棧知識問答字詞轉換IRC即時聊天聯絡我們關於維基百科資助維基百科 



其他專案


維基共享資源 



列印/匯出


下載成 PDF 



工具


連結至此的頁面相關變更上傳檔案特殊頁面可列印版靜態連結頁面資訊維基數據 項目引用此頁面 



其他語言


العربيةAzərbaycancaBosanskiCatalàČeštinaDanskDeutschEnglishEsperantoEspañolEestiفارسیFrançaisGaeilgeעבריתहिन्दीHrvatskiՀայերենBahasa IndonesiaÍslenskaItaliano日本語ქართული한국어LatinaLietuviųMalagasyМакедонскиമലയാളംNorsk nynorskNorsk bokmålPolskiPortuguêsРусскийSimple Englishதமிழ்ไทยTürkçeУкраїнськаاردوTiếng Việt 
編輯連結 





 本頁面最後修訂於2017年3月29日 (週三) 15:42。
本站的全部文字在創用CC 姓名標示-相同方式分享 3.0 協議之條款下提供，附加條款亦可能應用（請參閱使用條款）。
Wikipedia®和維基百科標誌是維基媒體基金會的註冊商標；維基™是維基媒體基金會的商標。
維基媒體基金會是在美國佛羅里達州登記的501(c)(3)免稅、非營利、慈善機構。


隱私政策
關於維基百科
免責聲明
開發人員
Cookie 聲明
手機版檢視



 

 
















人工神經網路 - MBA智庫百科









 
 















人工神經網路

出自 MBA智庫百科(http://wiki.mbalib.com/)


人工神經網路（ Artificial Neural Networks， 簡寫為ANNs）也簡稱為神經網路（NNs）或稱作連接模型（Connectionist Model）

目錄

1 人工神經網路概述
2 人工神經網路的特點
3 人工神經網路的特點與優越性
4 人工神經網路的主要研究方向
5 人工神經網路的應用分析


[編輯] 人工神經網路概述 
　　人工神經網路（ Artificial Neural Networks， 簡寫為ANNs）也簡稱為神經網路（NNs）或稱作連接模型（Connectionist Model） ，是對人腦或自然神經網路（Natural Neural Network）若幹基本特性的抽象和模擬。人工神經網路以對大腦的生理研究成果為基礎的，其目的在於模擬大腦的某些機理與機制，實現某個方面的功能。國際著名的神經網路研究專家，第一家神經電腦公司的創立者與領導人Hecht Nielsen給人工神經網路下的定義就是：“人工神經網路是由人工建立的以有向圖為拓撲結構的動態系統，它通過對連續或斷續的輸入作狀態相應而進行信息處理。” 這一定義是恰當的。 人工神經網路的研究，可以追溯到 1957年Rosenblatt提出的感知器模型(Perceptron) 。它幾乎與人工智慧——AI（Artificial Intelligence）同時起步，但30餘年來卻並未取得人工智慧那樣巨大的成功，中間經歷了一段長時間的蕭條。直到80年代，獲得了關於人工神經網路切實可行的演算法，以及以Von Neumann體系為依托的傳統演算法在知識處理方面日益顯露出其力不從心後，人們才重新對人工神經網路發生了興趣，導致神經網路的復興。 目前在神經網路研究方法上已形成多個流派，最富有成果的研究工作包括：多層網路BP演算法，Hopfield網路模型，自適應共振理論，自組織特征映射理論等。人工神經網路是在現代神經科學的基礎上提出來的。它雖然反映了人腦功能的基本特征，但遠不是自然神經網路的逼真描寫，而只是它的某種簡化抽象和模擬。

[編輯] 人工神經網路的特點 
　　人工神經網路的以下幾個突出的優點使它近年來引起人們的極大關註：
　　(1)可以充分逼近任意複雜的非線性關係；
　　(2)所有定量或定性的信息都等勢分佈貯存於網路內的各神經元，故有很強的魯棒性和容錯性；
　　(3)採用並行分佈處理方法，使得快速進行大量運算成為可能；
　　(4)可學習和自適應不知道或不確定的系統；
　　(5)能夠同時處理定量、定性知識。

[編輯] 人工神經網路的特點與優越性 
　　人工神經網路的特點和優越性，主要表現在三個方面：
　　第一，具有自學習功能。例如實現圖像識別時，只在先把許多不同的圖像樣板和對應的應識別的結果輸入人工神經網路，網路就會通過自學習功能，慢慢學會識別類似的圖像。自學習功能對於預測有特別重要的意義。預期未來的人工神經網路電腦將為人類提供經濟預測、市場預測、效益預測，其應用前途是很遠大的。
　　第二，具有聯想存儲功能。用人工神經網路的反饋網路就可以實現這種聯想。
　　第三，具有高速尋找優化解的能力。尋找一個複雜問題的優化解，往往需要很大的計算量，利用一個針對某問題而設計的反饋型人工神經網路，發揮電腦的高速運算能力，可能很快找到優化解。

[編輯] 人工神經網路的主要研究方向 
　　神經網路的研究可以分為理論研究和應用研究兩大方面。
　　理論研究可分為以下兩類：
　　1)．利用神經生理與認知科學研究人類思維以及智能機理。
　　2)．利用神經基礎理論的研究成果，用數理方法探索功能更加完善、性能更加優越的神經網路模型，深入研究網路演算法和性能， 如：穩定性、收斂性、容錯性、魯棒性等；開發新的網路數理理論，如：神經網路動力學、非線性神經場等。
　　應用研究可分為以下兩類：
　　1)．神經網路的軟體模擬和硬體實現的研究。
　　2)．神經網路在各個領域中應用的研究。這些領域主要包括：
 模式識別、信號處理、知識工程、專家系統、優化組合、機器人控制等。 隨著神經網路理論本身以及相關理論、相關技術的不斷發展，神經網路的應用定將更加深入。

[編輯] 人工神經網路的應用分析 
　　神經網路近來越來越受到人們的關註，因為它為解決大複雜度問題提供了一種相對來說比較有效的簡單方法。神經網路可以很容易的解決具有上百個參數的問題（當然實際生物體中存在的神經網路要比我們這裡所說的程式模擬的神經網路要複雜的多）。神經網路常用於兩類問題：分類和回歸。
　　在結構上，可以把一個神經網路劃分為輸入層、輸出層和隱含層（見圖1）。輸入層的每個節點對應一個個的預測變數。輸出層的節點對應目標變數，可有多個。在輸入層和輸出層之間是隱含層（對神經網路使用者來說不可見），隱含層的層數和每層節點的個數決定了神經網路的複雜度。
　　
　　圖1　一個神經元網路
　　除了輸入層的節點，神經網路的每個節點都與很多它前面的節點（稱為此節點的輸入節點）連接在一起，每個連接對應一個權重Wxy，此節點的值就是通過它所有輸入節點的值與對應連接權重乘積的和作為一個函數的輸入而得到，我們把這個函數稱為活動函數或擠壓函數。如圖2中節點4輸出到節點6的值可通過如下計算得到：
　　W14*節點1的值+W24*節點2的值
　　神經網路的每個節點都可表示成預測變數（節點1，2）的值或值的組合（節點3-6）。註意節點6的值已經不再是節點1、2的線性組合，因為數據在隱含層中傳遞時使用了活動函數。實際上如果沒有活動函數的話，神經元網路就等價於一個線性回歸函數，如果此活動函數是某種特定的非線性函數，那神經網路又等價於邏輯回歸。
　　調整節點間連接的權重就是在建立（也稱訓練）神經網路時要做的工作。最早的也是最基本的權重調整方法是錯誤回饋法，現在較新的有變化坡度法、類牛頓法、Levenberg-Marquardt法、和遺傳演算法等。無論採用那種訓練方法，都需要有一些參數來控制訓練的過程，如防止訓練過度和控制訓練的速度。
　　
　　圖2　帶權重Wxy的神經元網路
　　決定神經網路拓撲結構（或體繫結構）的是隱含層及其所含節點的個數，以及節點之間的連接方式。要從頭開始設計一個神經網路，必須要決定隱含層和節點的數目，活動函數的形式，以及對權重做那些限制等，當然如果採用成熟軟體工具的話，他會幫你決定這些事情。
　　在諸多類型的神經網路中，最常用的是前向傳播式神經網路，也就是我們前面圖示中所描繪的那種。我們下麵詳細討論一下，為討論方便假定只含有一層隱含節點。
　　可以認為錯誤回饋式訓練法是變化坡度法的簡化，其過程如下：
　　前向傳播：數據從輸入到輸出的過程是一個從前向後的傳播過程，後一節點的值通過它前面相連的節點傳過來，然後把值按照各個連接權重的大小加權輸入活動函數再得到新的值，進一步傳播到下一個節點。
　　回饋：當節點的輸出值與我們預期的值不同，也就是發生錯誤時，神經網路就要 “學習”（從錯誤中學習）。我們可以把節點間連接的權重看成後一節點對前一節點的“信任” 程度（他自己向下一節點的輸出更容易受他前面哪個節點輸入的影響）。學習的方法是採用懲罰的方法，過程如下：如果一節點輸出發生錯誤，那麼他看他的錯誤是受哪個（些）輸入節點的影響而造成的，是不是他最信任的節點（權重最高的節點）陷害了他（使他出錯），如果是則要降低對他的信任值（降低權重），懲罰他們，同時升高那些做出正確建議節點的信任值。對那些收到懲罰的節點來說，他也需要用同樣的方法來進一步懲罰它前面的節點。就這樣把懲罰一步步向前傳播直到輸入節點為止。
　　對訓練集中的每一條記錄都要重覆這個步驟，用前向傳播得到輸出值，如果發生錯誤，則用回饋法進行學習。當把訓練集中的每一條記錄都運行過一遍之後，我們稱完成一個訓練周期。要完成神經網路的訓練可能需要很多個訓練周期，經常是幾百個。訓練完成之後得到的神經網路就是在通過訓練集發現的模型，描述了訓練集中響應變數受預測變數影響的變化規律。
　　由於神經網路隱含層中的可變參數太多，如果訓練時間足夠長的話，神經網路很可能把訓練集的所有細節信息都“記”下來，而不是建立一個忽略細節只具有規律性的模型，我們稱這種情況為訓練過度。顯然這種“模型”對訓練集會有很高的準確率，而一旦離開訓練集應用到其他數據，很可能準確度急劇下降。為了防止這種訓練過度的情況，我們必須知道在什麼時候要停止訓練。在有些軟體實現中會在訓練的同時用一個測試集來計算神經網路在此測試集上的正確率，一旦這個正確率不再升高甚至開始下降時，那麼就認為現在神經網路已經達到做好的狀態了可以停止訓練。
　　圖3中的曲線可以幫我們理解為什麼利用測試集能防止訓練過度的出現。在圖中可以看到訓練集和測試集的錯誤率在一開始都隨著訓練周期的增加不斷降低，而測試集的錯誤率在達到一個谷底後反而開始上升，我們認為這個開始上升的時刻就是應該停止訓練的時刻。
　　
　　圖3　　神經網路在訓練周期增加時準確度的變化情況
　　神經元網路和統計方法在本質上有很多差別。神經網路的參數可以比統計方法多很多。如圖1中就有13個參數（9個權重和4個限制條件）。由於參數如此之多，參數通過各種各樣的組合方式來影響輸出結果，以至於很難對一個神經網路表示的模型做出直觀的解釋。實際上神經網路也正是當作“黑盒”來用的，不用去管 “盒子”裡面是什麼，只管用就行了。在大部分情況下，這種限制條件是可以接受的。比如銀行可能需要一個筆跡識別軟體，但他沒必要知道為什麼這些線條組合在一起就是一個人的簽名，而另外一個相似的則不是。在很多複雜度很高的問題如化學試驗、機器人、金融市場的模擬、和語言圖像的識別，等領域神經網路都取得了很好的效果。
　　神經網路的另一個優點是很容易在並行電腦上實現，可以把他的節點分配到不同的CPU上並行計算。
　　在使用神經網路時有幾點需要註意：第一，神經網路很難解釋，目前還沒有能對神經網路做出顯而易見解釋的方法學。
　　第二，神經網路會學習過度，在訓練神經網路時一定要恰當的使用一些能嚴格衡量神經網路的方法，如前面提到的測試集方法和交叉驗證法等。這主要是由於神經網路太靈活、可變參數太多，如果給足夠的時間，他幾乎可以“記住”任何事情。
　　第三，除非問題非常簡單，訓練一個神經網路可能需要相當可觀的時間才能完成。當然，一旦神經網路建立好了，在用它做預測時運行時還是很快得。
　　第四，建立神經網路需要做的數據準備工作量很大。一個很有誤導性的神話就是不管用什麼數據神經網路都能很好的工作並做出準確的預測。這是不確切的，要想得到準確度高的模型必須認真的進行數據清洗、整理、轉換、選擇等工作，對任何數據挖掘技術都是這樣，神經網路尤其註重這一點。比如神經網路要求所有的輸入變數都必須是0-1（或-1 -- +1）之間的實數，因此像“地區”之類文本數據必須先做必要的處理之後才能用作神經網路的輸入。




取自"http://wiki.mbalib.com/zh-tw/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"

本條目對我有幫助129  分享到：














   如果您認為本條目還有待完善，需要補充新內容或修改錯誤內容，請編輯條目。

本條目相關文檔
 人工神經網路 69頁 人工神經網路——金融數據處理 16頁 第8章人工神經網路 25頁 人工神經網路——HZAU 數模基地 85頁 第六章 人工神經網路 275頁 第4章 人工神經網路 32頁 一種新型人工神經網路模型 7頁 基於人工神經網路旅游競爭力分析 2頁 石油需求預測的人工神經網路模型 4頁 基於人工神經網路的企業衰敗預警 3頁更多相關文檔

本條目相關資訊
人工智慧+金融=？ 從最近爆紅的AlphaGo說起 2016年3月11日
本條目由以下用戶參與貢獻
Angle Roh,村姑,Cabbage,Dan,Zfj3000,徐廣廈,Vulture,Yixi,KAER,方小莉,Tracy,Mis銘,林曉辰,劉維燎. 頁面分類: 信息技術 





評論(共4條)提示:評論內容為網友針對條目"人工神經網路"展開的討論，與本站觀點立場無關。

 112.122.157.*  在 2013年1月1日 21:18 發表    


中文神經網路(BP、RBF、PNN、GRNN、Elman、SOM、LVQ)+偏最小二乘+Excel.市面上最易用的神經網路和偏最小二乘軟體包（Excel一鍵操作）.


 回複評論
 發表評論﻿請文明上網，理性發言並遵守有關規定。




 




 114.255.122.*  在 2015年3月25日 09:27 發表    


請問圖4在哪裡？


 回複評論
 發表評論﻿請文明上網，理性發言並遵守有關規定。




 




 Laiqinjing (討論 | 貢獻) 在 2015年3月25日 13:43 發表    


 114.255.122.*  在 2015年3月25日 09:27 發表

請問圖4在哪裡？





標錯了，應該是指圖1


 回複評論
 發表評論﻿請文明上網，理性發言並遵守有關規定。




 




 58.60.1.*  在 2015年5月12日 15:27 發表    


很棒，多謝！


 回複評論
 發表評論﻿請文明上網，理性發言並遵守有關規定。




 



發表評論﻿請文明上網，理性發言並遵守有關規定。




 




導航


首頁
文檔
百科
商學院
資訊
培訓
幫幫




個人工具


用戶登錄創建新帳號 









搜索



 
 

全球最大的中文經管百科，由121,994位網友共同編寫而成，共計414,113個條目








 
首頁
 
管理
 
營銷
 
經濟
 
金融
 
人力資源
 
咨詢
 
財務
 
品牌
 
證券
 
物流
 
貿易
 
商學院
 
法律
 
人物
 
分類索引
 




查看

條目討論編輯簡體中文繁體中文 


工具▼


鏈入頁面
鏈出更改
上載文件 特殊頁面 可列印版
永久鏈接 













導航


最新資訊
最新評論
最新推薦
熱門推薦
編輯實驗
使用幫助
創建條目
隨便看看












本周推薦
最多推薦



影響世界的100個經典管理定律垃圾人定律產品定位五步法MECE分析法吳曉波DISC個性測驗六項精進巴納姆效應PPP模式管理百科 

蘑菇管理定律猴子管理法則情緒ABC理論100個最流行的管理辭彙21天效應破窗效應懶螞蟻效應SWOT分析模型墨菲定律踢貓效應 

以上內容根據網友推薦自動排序生成









x



	   最後更改16:52, 2016年10月13日.	  
智庫首頁 - 
百科首頁 - 
關於百科 -
	   客戶端 -
	   人才招聘 -
	   廣告合作 - 
權利通知 -
	   聯繫我們 -
	   免責聲明
 - 友情鏈接

©2006-2017 MBAlib.com, All rights reserved. 


 







WELCOME TO MY WORLD - 維基知識






















 












WELCOME TO MY WORLD20&Life







 

帳號: 

密碼: 
















WePortfolio

關於我
生活札記
生活歷程 維基知識 相簿 資源中心 
登入








 



功能建立新文章全部文章熱門文章統計資訊進階功能導覽最近平臺活動沒有任何活動 
搜索













個人工具






Views

文章
歷程





人工神經網路

From WisdomMasterWiki

Jump to: navigation, search 
人工神經網路是一種應用類似於大腦神經突觸聯接的結構進行信息處理的數學模型。在工程與學術界也常直接簡稱為「神經網路」或類神經網路。神經網路是一種運算模型[1]，由大量的節點（或稱「神經元」，或「單元」）和之間相互聯接構成。每個節點代表一種特定的輸出函數，稱為激勵函數（activation function）。每兩個節點間的連接都代表一個對於通過該連接信號的加權值，稱之為權重（weight），這相當於人工神經網路的記憶。網路的輸出則依網路的連接方式，權重值和激勵函數的不同而不同。而網路自身通常都是對自然界某種演算法或者函數的逼近，也可能是對一種邏輯策略的表達。
它的構築理念是受到生物（人或其他動物）神經網路功能的運作啟發而產生的。人工神經網路通常是通過一個基於數學統計學類型的學習方法（Learning Method）得以優化，所以人工神經網路也是數學統計學方法的一種實際應用，通過統計學的標準數學方法我們能夠得到大量的可以用函數來表達的局部結構空間，另一方面在人工智慧學的人工感知領域，我們通過數學統計學的應用可以來做人工感知方面的決定問題(也就是說通過統計學的方法，人工神經網路能夠類似人一樣具有簡單的決定能力和簡單的判斷能力)，這種方法比起正式的邏輯學推理演算更具有優勢。






目錄[隱藏]

1 神經元
2 神經元網路

2.1 單層神經元網路
2.2 多層神經元網路
3 人工神經網路的使用性
4 人工神經元網路模型
5 基本結構
6 學習過程
7 種類
8 參考條目
9 外部連結
10 參考文獻
[編輯] 神經元
神經元示意圖：



a1~an為輸入向量的各個分量
w1~wn為神經元各個突觸的權值
b為偏置
f為傳遞函數，通常為非線性函數。以下默認為hardlim()
t為神經元輸出
數學表示 t=f(WA'+b)

W為權向量
A為輸入向量，A'為A向量的轉置
b為偏置
f為傳遞函數
可見，一個神經元的功能是求得輸入向量與權向量的內積後，經一個非線性傳遞函數得到一個純量結果。
單個神經元的作用：把一個n維向量空間用一個超平面分割成兩部分（稱之為判斷邊界），給定一個輸入向量，神經元可以判斷出這個向量位於超平面的哪一邊。
該超平面的方程: Wp+b=0

W權向量
b偏置
p超平面上的向量
[2]
[編輯] 神經元網路
[編輯] 單層神經元網路
是最基本的神經元網路形式，由有限個神經元構成，所有神經元的輸入向量都是同一個向量。由於每一個神經元都會產生一個純量結果，所以單層神經元的輸出是一個向量，向量的維數等於神經元的數目。 示意圖：


[編輯] 多層神經元網路
[編輯] 人工神經網路的使用性
人工神經網路是一個能夠學習，能夠總結歸納的系統，也就是說它能夠通過已知數據的實驗運用來學習和歸納總結。人工神經網路通過對局部情況的對照比較（而這些比較是基於不同情況下的自動學習和要實際解決問題的複雜性所決定的），它能夠推理產生一個可以自動識別的系統。與之不同的基於符號系統下的學習方法，它們也具有推理功能，只是它們是建立在邏輯演算演算法的基礎上，也就是說它們之所以能夠推理，基礎是需要有一個推理演算法則的集合。
[編輯] 人工神經元網路模型
通常來說，一個人工神經元網路是由一個多層神經元結構組成，每一層神經元擁有輸入（它的輸入是前一層神經元的輸出）和輸出，每一層（我們用符號記做）Layer(i)是由Ni(Ni代表在第i層上的N)個網路神經元組成，每個Ni上的網路神經元把對應在Ni-1上的神經元輸出做為它的輸入，我們把神經元和與之對應的神經元之間的連線用生物學的名稱，叫做神經軸突的突觸，在數學模型中每個突觸有一個加權數值，我們稱做權重，那麼要計算第i層上的某個神經元所得到的勢能等於每一個權重乘以第i-1層上對應的神經元的輸出，然後全體求和得到了第i層上的某個神經元所得到的勢能，然後勢能數值通過該神經元上的激勵函數（activation function，常是en:Sigmoid function以控制輸出大小，因為其可微分且連續，方便en:Delta rule處理。）求出該神經元的輸出，註意的是該輸出是一個非線性的數值，也就是說通過激勵函數求的數值根據極限值來判斷是否要激活該神經元，換句話說我們對一個神經元網路的輸出是線性不感興趣。
[編輯] 基本結構
一種常見的多層結構的前饋網路（Multilayer Feedforward Network）由三部分組成，

輸入層（Input layer），眾多神經元（Neuron）接受大量非線形輸入信息。輸入的信息稱為輸入向量。
輸出層（Output layer），信息在神經元鏈接中傳輸、分析、權衡，形成輸出結果。輸出的信息稱為輸出向量。
隱藏層（Hidden layer），簡稱「隱層」，是輸入層和輸出層之間眾多神經元和鏈接組成的各個層面。隱層可以有多層，習慣上會用一層。隱層的節點（神經元）數目不定，但數目越多神經網路的非線性越顯著，從而神經網路的強健性（robustness）（控制系統在一定結構、大小等的參數攝動下，維持某些性能的特性。）更顯著。習慣上會選輸入節點1.2至1.5倍的節點。
神經網路的類型已經演變出很多種，這種分層的結構也並不是對所有的神經網路都適用。
[編輯] 學習過程
通過訓練樣本的校正，對各個層的權重進行校正（learning）而建立模型的過程，稱為自動學習過程（training algorithm）。具體的學習方法則因網路結構和模型不通而不同，常用en:Backpropagation(the back-propagation algorithm/倒傳遞/逆傳播，以output利用一次微分en:Delta rule來修正weight)來驗證。
參見：神經網路介紹
[編輯] 種類
人工神經網路分類為以下兩種:1.依學習策略（Algorithm）分類主要有：

監督式學習網路（Supervised Learning Network）為主
無監督式學習網路（Unsupervised Learning Network）
混合式學習網路（Hybrid Learning Network）
聯想式學習網路（Associate Learning Network）
最適化學習網路（Optimization Application Network）
2.依網路架構（Connectionism）分類主要有:

前向式架構（Feed Forward Network）
回饋式架構（Recurrent Network）
強化式架構（Reinforcement Network）

取自"http://eportfolio.lib.ksu.edu.tw/~4960H024/wiki/index.php/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF"









 最後更改17:15 2010年十月4日.
本頁面已經被瀏覽9次。









 請輸入文章名稱，以建立新文章。

Name:

















            神經網路 -人工神經網路-華人百科                       神經網路    人工神經網路  更多義項 ▼ 收起更多 ▲    人工神經網路生物神經網路同名書籍      人工神經網路（Artificial Neural Networks，簡寫為ANNs）是一種模仿動物神經網路行為360百科 糾錯反饋      基本介紹人工神經網路是一種套用類似於大腦神經突觸聯接的結構進行信息處理的數學模型。在工程與學術界也常直接簡稱為神經網路或類神經網路。神經網路是一種運算模型，由大量的節點（或稱神經元）和之間相互聯接構成。每個節點代表一種特定的輸出函式，稱為激勵函式（activation function）。每兩個節點間的連線都代表一個對於通過該連線信號的加權值，稱之為權重，這相當於人工神經網路的記憶。網路的輸出則依網路的連線方式，權重值和激勵函式的不同而不同。而網路自身通常都是對自然界某種演算法或者函式的逼近，也可能是對一種邏輯策略的表達。它的構築理念是受到生物（人或其他動物）神經網路功能的運作啓發而產生的。人工神經網路通常是通過一個基於數學統計學類型的學習方法（Learning Method）得以最佳化，所以人工神經網路也是數學統計學方法的一種實際套用，通過統計學的標準數學方法我們能夠得到大量的可以用函式來表達的局部結構空間，另一方面在人工智慧學的人工感知領域，我們通過數學統計學的套用可以來做人工感知方面的決定問題(也就是說通過統計學的方法，人工神經網路能夠類似人一樣具有簡單的決定能力和簡單的判斷能力)，這種方法比起正式的邏輯學推理演算更具有優勢。 基本特征人工神經網路是由大量處理單元互聯組成的非線性、自適應信息處理系統。它是在現代神經科學研究成果的基礎上提出的，嘗試通過模擬大腦神經網路處理、記憶信息的方式進行信息處理。人工神經網路具有四個基本特征：非線性非線性關系是自然界的普遍特徵。大腦的智慧就是一種非線性現象。人工神經元處於激活或抑製二種不同的狀態，這種行為在數學上表現為一種非線性關系。具有閾值的神經元構成的網路具有更好的性能，可以提高容錯性和存儲容量。非局限性一個神經網路通常由多個神經元廣泛連線而成。一個系統的整體行為不僅取決於單個神經元的特征，而且可能主要由單元之間的相互作用、相互連線所決定。通過單元之間的大量連線模擬大腦的非局限性。聯想記憶是非局限性的典型例子。非常定性人工神經網路具有自適應、自組織、自學習能力。神經網路不但處理的信息可以有各種變化，而且在處理信息的同時，非線性動力系統本身也在不斷變化。經常採用迭代過程描寫動力系統的演化過程。非凸性一個系統的演化方向，在一定條件下將取決於某個特定的狀態函式。例如能量函式，它的極值相應於系統比較穩定的狀態。非凸性是指這種函式有多個極值，故系統具有多個較穩定的平衡態，這將導致系統演化的多樣性。人工神經網路中，神經元處理單元可表示不同的對象，例如特征、字母、概念，或者一些有意義的抽象模式。網路中處理單元的類型分為三類：輸入單元、輸出單元和隱單元。輸入單元接受外部世界的信號與資料；輸出單元實現系統處理結果的輸出；隱單元是處在輸入和輸出單元之間，不能由系統外部觀察的單元。神經元間的連線權值反映了單元間的連線強度，信息的表示和處理體現在網路處理單元的連線關系中。人工神經網路是一種非程式化、適應性、大腦風格的信息處理 ，其本質是通過網路的變換和動力學行為得到一種並行分散式的信息處理功能，並在不同程度和層次上模仿人腦神經系統的信息處理功能。它是涉及神經科學、思維科學、人工智慧、電腦科學等多個領域的交叉學科。人工神經網路是並行分散式系統，採用了與傳統人工智慧和信息處理技術完全不同的機理，剋服了傳統的基於邏輯符號的人工智慧在處理直覺、非結構化信息方面的缺陷，具有自適應、自組織和即時學習的特點。特點和優越性人工神經網路的特點和優越性，主要表現在三個方面：第一，具有自學習功能。例如實現圖像識別時，隻在先把許多不同的圖像樣板和對應的應識別的結果輸入人工神經網路，網路就會通過自學習功能，慢慢學會識別類似的圖像。自學習功能對於預測有特別重要的意義。預期未來的人工神經網路電腦將為人類提供經濟預測、市場預測、效益預測，其套用前途是很遠大的。第二，具有聯想存儲功能。用人工神經網路的反饋網路就可以實現這種聯想。第三，具有高速尋找最佳化解的能力。尋找一個復雜問題的最佳化解，往往需要很大的計算量，利用一個針對某問題而設計的反饋型人工神經網路，發揮電腦的高速運算能力，可能很快找到最佳化解。 發展歷史1943年，心理學家W.S.McCulloch和數理邏輯學家W.Pitts建立了神經網路和數學模型，稱為MP模型。他們通過MP模型提出了神經元的形式化數學描述和網路結構方法，證明瞭單個神經元能執行邏輯功能，從而開創了人工神經網路研究的時代。1949年，心理學家提出了突觸聯系強度可變的構想。 心理學家W.S.McCulloch和數理邏輯學家W.Pitts建立了神經網路和數學模型，稱為MP模型。.jpg 60年代，人工神經網路得到了進一步發展，更完善的神經網路模型被提出，其中包括感知器和自適應線性元件等。M.Minsky等仔細分析了以感知器為代表的神經網路系統的功能及局限後，於1969年出版了《Perceptron》一書，指出感知器不能解決高階謂詞問題。他們的論點極大地影響了神經網路的研究，加之當時串列電腦和人工智慧所取得的成就，掩蓋了發展新型電腦和人工智慧新途徑的必要性和迫切性，使人工神經網路的研究處於低潮。在此期間，一些人工神經網路的研究者仍然致力於這一研究，提出了適應諧振理論（ART網）、自組織對應、認知機網路，同時進行了神經網路數學理論的研究。以上研究為神經網路的研究和發展奠定了基礎。1982年，美國加州工學院物理學家J.J.Hopfield提出了Hopfield神經格線模型，引入了“計算能量”概念，給出了網路穩定性判斷。 1984年，他又提出了連續時間Hopfield神經網路模型，為神經電腦的研究做了開拓性的工作，開創了神經網路用於聯想記憶和最佳化計算的新途徑，有力地推動了神經網路的研究，1985年，又有學者提出了波耳茲曼模型，在學習中採用統計熱力學模擬退火技術，保證整個系統趨於全局穩定點。1986年進行認知微觀結構地研究，提出了並行分佈處理的理論。人工神經網路的研究受到了各個發達國家的重視，美國國會通過決議將1990年1月5日開始的十年定為“腦的十年”，國際研究組織號召它的成員國將“腦的十年”變為全球行為。在日本的“真實世界計算（RWC）”項目中，人工智慧的研究成了一個重要的組成部分。 基本結構一種常見的多層結構的前饋網路（Multilayer Feedforward Network）由三部分組成，輸入層（Input layer），眾多神經元（Neuron）接受大量非線形輸入信息。輸入的信息稱為輸入向量。輸出層（Output layer），信息在神經元連結中傳輸、分析、權衡，形成輸出結果。輸出的信息稱為輸出向量。隱藏層（Hidden layer），簡稱“隱層”，是輸入層和輸出層之間眾多神經元和連結組成的各個層面。隱層可以有多層，習慣上會用一層。隱層的節點（神經元）數目不定，但數目越多神經網路的非線性越顯著，從而神經網路的強健性（robustness）（控製系統在一定結構、大小等的參數攝動下，維持某些性能的特徵。）更顯著。習慣上會選輸入節點1.2至1.5倍的節點。神經網路的類型已經演變出很多種，這種分層的結構也並不是對所有的神經網路都適用。 具體分類人工神經網路分類為以下兩種:1.依學習策略（Algorithm）分類主要有：監督式學習網路（Supervised Learning Network）為主無監督式學習網路（Unsupervised Learning Network）混合式學習網路（Hybrid Learning Network）聯想式學習網路（Associate Learning Network）最適化學習網路（Optimization Application Network）2.依網路架構（Connectionism）分類主要有:前向式架構（Feed Forward Network）回饋式架構（Recurrent Network）強化式架構（Reinforcement Network） 分析方法研究神經網路的非線性動力學性質，主要採用動力學系統理論、非線性規劃理論和統計理論，來分析神經網路的演化過程和吸引子的性質，探索神經網路的協同行為和集體計算功能，瞭解神經信息處理機製。為了探討神經網路在整體性和模糊性方面處理信息的可能，混沌理論的概念和方法將會發揮作用。混沌是一個相當難以精確定義的數學概念。一般而言，“混沌”是指由確定性方程描述的動力學系統中表現出的非確定性行為，或稱之為確定的隨機性。“確定性”是因為它由內在的原因而不是外來的噪聲或幹擾所產生，而“隨機性”是指其不規則的、不能預測的行為，隻可能用統計的方法描述。混沌動力學系統的主要特征是其狀態對初始條件的靈敏依賴性，混沌反映其內在的隨機性。混沌理論是指描述具有混沌行為的非線性動力學系統的基本理論、概念、方法，它把動力學系統的復雜行為理解為其自身與其在同外界進行物質、能量和信息交換過程中內在的有結構的行為，而不是外來的和偶然的行為，混沌狀態是一種定態。混沌動力學系統的定態包括：靜止、平穩量、周期性、準同 期性和混沌解。混沌軌線是整體上穩定與局部不穩定相結合的結果，稱之為奇異吸引子。一個奇異吸引子有如下一些特征：（1）奇異吸引子是一個吸引子，但它既不是不動點，也不是周期解；（2）奇異吸引子是不可分割的，即不能分為兩個以及兩個以上的吸引子；（3）它對初始值十分敏感，不同的初始值會導致極不相同的行為。 神經元素神經元示意圖（見右圖，可點擊放大）a1~an為輸入向量的各個分量  w1~wn為神經元各個突觸的權值b為偏置f為傳遞函式，通常為非線性函式。一般有traingd(),tansig(),hardlim()。以下默認為hardlim()t為神經元輸出數學表示 為權向量為輸入向量，為的轉置為偏置為傳遞函式可見，一個神經元的功能是求得輸入向量與權向量的內積後，經一個非線性傳遞函式得到一個標量結果。單個神經元的作用：把一個n維向量空間用一個超平面分割成兩部分（稱之為判斷邊界），給定一個輸入向量，神經元可以判斷出這個向量位於超平面的哪一邊。該超平面的方程:權向量偏置超平面上的向量   相關詞條     神經網路演算法      bp神經網路      神經網路原理      支持向量機      遺傳演算法      BP神經網路演算法      人工神經網路基礎      人工神經網路理論·模型·演算法與套用      人工神經網路原理及套用      模擬退火演算法      灰色預測      蟻群演算法      前饋神經網路      模糊推理      蟻群最佳化演算法      金融資料挖掘      MATLAB神經網路      模糊理論      灰色模型      智慧型控製      d-s證據理論      深度學習      知識表示      聚類分析      貝葉斯分類演算法      計算智慧型      回溯演算法      關聯規則      模糊綜合評價法      時間序列模型      小波分析      粗糙集理論      神經元      最優控製      聚類      鄧聚龍     相關搜尋   神經網路演算法     神經網路模型     神經網路電腦     神經網路原理     神經網路控製     人工神經網路     bp神經網路     模糊神經網路     深度神經網路     elman神經網路           神經網路 -人工神經網路@ 華人百科  勵志人生 國語字典辭典 詩詞大全 小語 腦筋急轉彎                人工神經網路 -華人百科                       人工神經網路  人工神經網路（Artificial Neural Networks，簡寫為ANNs）是一種模仿動物神經網路行為特徵，進行分散式並行信息處理的算法數學模型。這種網路依靠系統的複雜程度，通過調整內部大量節點之間相互連線的關係，從而達到處理信息的目的，並具有自學習和自適應的能力。  中文名稱 人工神經網路 外文名稱 Artificial Neural Networks 簡稱  ANNs  定義  是一種模仿動物神經網路行為特徵，進行分散式並行信息處理的算法數學模型。  特點和優越性  1.具有自學習功能。2.具有聯想存儲功能。3.具有高速尋找最佳化解的能力。       基本介紹人工神經網路是一種套用類似於大腦神經突觸聯接的結構進行信息處理的數學模型。在工程與學術界也常直接簡稱為神經網路或類神經網路。神經網路是一種運算模型，由大量的節點（或稱神經元）和之間相互聯接構成。每個節點代表一種特定的輸出函式，稱為激勵函式（activation function）。每兩個節點間的連線都代表一個對於通過該連線信號的加權值，稱之為權重，這相當於人工神經網路的記憶。網路的輸出則依網路的連線方式，權重值和激勵函式的不同而不同。而網路自身通常都是對自然界某種算法或者函式的逼近，也可能是對一種邏輯策略的表達。它的構築理念是受到生物（人或其他動物）神經網路功能的運作啟發而產生的。人工神經網路通常是通過一個基於數學統計學類型的學習方法（Learning Method）得以最佳化，所以人工神經網路也是數學統計學方法的一種實際套用，通過統計學的標準數學方法我們能夠得到大量的可以用函式來表達的局部結構空間，另一方面在人工智慧學的人工感知領域，我們通過數學統計學的套用可以來做人工感知方面的決定問題(也就是說通過統計學的方法，人工神經網路能夠類似人一樣具有簡單的決定能力和簡單的判斷能力)，這種方法比起正式的邏輯學推理演算更具有優勢。 基本特徵人工神經網路是由大量處理單元互聯組成的非線性、自適應信息處理系統。它是在現代神經科學研究成果的基礎上提出的，試圖通過模擬大腦神經網路處理、記憶信息的方式進行信息處理。人工神經網路具有四個基本特徵：非線性非線性關係是自然界的普遍特性。大腦的智慧就是一種非線性現象。人工神經元處於激活或抑制二種不同的狀態，這種行為在數學上表現為一種非線性關係。具有閾值的神經元構成的網路具有更好的性能，可以提高容錯性和存儲容量。非局限性一個神經網路通常由多個神經元廣泛連線而成。一個系統的整體行為不僅取決於單個神經元的特徵，而且可能主要由單元之間的相互作用、相互連線所決定。通過單元之間的大量連線模擬大腦的非局限性。聯想記憶是非局限性的典型例子。非常定性人工神經網路具有自適應、自組織、自學習能力。神經網路不但處理的信息可以有各種變化，而且在處理信息的同時，非線性動力系統本身也在不斷變化。經常採用疊代過程描寫動力系統的演化過程。非凸性一個系統的演化方向，在一定條件下將取決於某個特定的狀態函式。例如能量函式，它的極值相應於系統比較穩定的狀態。非凸性是指這種函式有多個極值，故系統具有多個較穩定的平衡態，這將導致系統演化的多樣性。人工神經網路中，神經元處理單元可表示不同的對象，例如特徵、字母、概念，或者一些有意義的抽象模式。網路中處理單元的類型分為三類：輸入單元、輸出單元和隱單元。輸入單元接受外部世界的信號與數據；輸出單元實現系統處理結果的輸出；隱單元是處在輸入和輸出單元之間，不能由系統外部觀察的單元。神經元間的連線權值反映了單元間的連線強度，信息的表示和處理體現在網路處理單元的連線關係中。人工神經網路是一種非程式化、適應性、大腦風格的信息處理 ，其本質是通過網路的變換和動力學行為得到一種並行分散式的信息處理功能，並在不同程度和層次上模仿人腦神經系統的信息處理功能。它是涉及神經科學、思維科學、人工智慧、計算機科學等多個領域的交叉學科。人工神經網路是並行分散式系統，採用了與傳統人工智慧和信息處理技術完全不同的機理，剋服了傳統的基於邏輯符號的人工智慧在處理直覺、非結構化信息方面的缺陷，具有自適應、自組織和實時學習的特點。特點和優越性人工神經網路的特點和優越性，主要表現在三個方面：第一，具有自學習功能。例如實現圖像識別時，只在先把許多不同的圖像樣板和對應的應識別的結果輸入人工神經網路，網路就會通過自學習功能，慢慢學會識別類似的圖像。自學習功能對於預測有特別重要的意義。預期未來的人工神經網路計算機將為人類提供經濟預測、市場預測、效益預測，其套用前途是很遠大的。第二，具有聯想存儲功能。用人工神經網路的反饋網路就可以實現這種聯想。第三，具有高速尋找最佳化解的能力。尋找一個複雜問題的最佳化解，往往需要很大的計算量，利用一個針對某問題而設計的反饋型人工神經網路，發揮計算機的高速運算能力，可能很快找到最佳化解。 發展歷史1943年，心理學家W.S.McCulloch和數理邏輯學家W.Pitts建立了神經網路和數學模型，稱為MP模型。他們通過MP模型提出了神經元的形式化數學描述和網路結構方法，證明瞭單個神經元能執行邏輯功能，從而開創了人工神經網路研究的時代。1949年，心理學家提出了突觸聯繫強度可變的構想。 心理學家W.S.McCulloch和數理邏輯學家W.Pitts建立了神經網路和數學模型，稱為MP模型。.jpg 60年代，人工神經網路得到了進一步發展，更完善的神經網路模型被提出，其中包括感知器和自適應線性元件等。M.Minsky等仔細分析了以感知器為代表的神經網路系統的功能及局限後，於1969年出版了《Perceptron》一書，指出感知器不能解決高階謂詞問題。他們的論點極大地影響了神經網路的研究，加之當時串列計算機和人工智慧所取得的成就，掩蓋了發展新型計算機和人工智慧新途徑的必要性和迫切性，使人工神經網路的研究處於低潮。在此期間，一些人工神經網路的研究者仍然致力於這一研究，提出了適應諧振理論（ART網）、自組織映射、認知機網路，同時進行了神經網路數學理論的研究。以上研究為神經網路的研究和發展奠定了基礎。1982年，美國加州工學院物理學家J.J.Hopfield提出了Hopfield神經格線模型，引入了“計算能量”概念，給出了網路穩定性判斷。 1984年，他又提出了連續時間Hopfield神經網路模型，為神經計算機的研究做了開拓性的工作，開創了神經網路用於聯想記憶和最佳化計算的新途徑，有力地推動了神經網路的研究，1985年，又有學者提出了波耳茲曼模型，在學習中採用統計熱力學模擬退火技術，保證整個系統趨於全局穩定點。1986年進行認知微觀結構地研究，提出了並行分佈處理的理論。人工神經網路的研究受到了各個已開發國家的重視，美國國會通過決議將1990年1月5日開始的十年定為“腦的十年”，國際研究組織號召它的成員國將“腦的十年”變為全球行為。在日本的“真實世界計算（RWC）”項目中，人工智慧的研究成了一個重要的組成部分。 基本結構一種常見的多層結構的前饋網路（Multilayer Feedforward Network）由三部分組成，輸入層（Input layer），眾多神經元（Neuron）接受大量非線形輸入信息。輸入的信息稱為輸入向量。輸出層（Output layer），信息在神經元連結中傳輸、分析、權衡，形成輸出結果。輸出的信息稱為輸出向量。隱藏層（Hidden layer），簡稱“隱層”，是輸入層和輸出層之間眾多神經元和連結組成的各個層面。隱層可以有多層，習慣上會用一層。隱層的節點（神經元）數目不定，但數目越多神經網路的非線性越顯著，從而神經網路的強健性（robustness）（控制系統在一定結構、大小等的參數攝動下，維持某些性能的特性。）更顯著。習慣上會選輸入節點1.2至1.5倍的節點。神經網路的類型已經演變出很多種，這種分層的結構也並不是對所有的神經網路都適用。 具體分類人工神經網路分類為以下兩種:1.依學習策略（Algorithm）分類主要有：監督式學習網路（Supervised Learning Network）為主無監督式學習網路（Unsupervised Learning Network）混合式學習網路（Hybrid Learning Network）聯想式學習網路（Associate Learning Network）最適化學習網路（Optimization Application Network）2.依網路架構（Connectionism）分類主要有:前向式架構（Feed Forward Network）回饋式架構（Recurrent Network）強化式架構（Reinforcement Network） 分析方法研究神經網路的非線性動力學性質，主要採用動力學系統理論、非線性規劃理論和統計理論，來分析神經網路的演化過程和吸引子的性質，探索神經網路的協同行為和集體計算功能，瞭解神經信息處理機制。為了探討神經網路在整體性和模糊性方面處理信息的可能，混沌理論的概念和方法將會發揮作用。混沌是一個相當難以精確定義的數學概念。一般而言，“混沌”是指由確定性方程描述的動力學系統中表現出的非確定性行為，或稱之為確定的隨機性。“確定性”是因為它由內在的原因而不是外來的噪聲或乾擾所產生，而“隨機性”是指其不規則的、不能預測的行為，只可能用統計的方法描述。混沌動力學系統的主要特徵是其狀態對初始條件的靈敏依賴性，混沌反映其內在的隨機性。混沌理論是指描述具有混沌行為的非線性動力學系統的基本理論、概念、方法，它把動力學系統的複雜行為理解為其自身與其在同外界進行物質、能量和信息交換過程中內在的有結構的行為，而不是外來的和偶然的行為，混沌狀態是一種定態。混沌動力學系統的定態包括：靜止、平穩量、周期性、準同 期性和混沌解。混沌軌線是整體上穩定與局部不穩定相結合的結果，稱之為奇異吸引子。一個奇異吸引子有如下一些特徵：（1）奇異吸引子是一個吸引子，但它既不是不動點，也不是周期解；（2）奇異吸引子是不可分割的，即不能分為兩個以及兩個以上的吸引子；（3）它對初始值十分敏感，不同的初始值會導致極不相同的行為。 神經元素神經元示意圖（見右圖，可點擊放大）a1~an為輸入向量的各個分量  w1~wn為神經元各個突觸的權值b為偏置f為傳遞函式，通常為非線性函式。一般有traingd(),tansig(),hardlim()。以下默認為hardlim()t為神經元輸出數學表示 為權向量為輸入向量，為的轉置為偏置為傳遞函式可見，一個神經元的功能是求得輸入向量與權向量的內積後，經一個非線性傳遞函式得到一個標量結果。單個神經元的作用：把一個n維向量空間用一個超平面分割成兩部分（稱之為判斷邊界），給定一個輸入向量，神經元可以判斷出這個向量位於超平面的哪一邊。該超平面的方程:權向量偏置超平面上的向量    相關搜尋   人工神經網路原理           人工神經網路 @ 華人百科  勵志人生 國語字典辭典 詩詞大全 小語 腦筋急轉彎    


人工神經網路-金石堂網路書店














































會員功能列

 


加入會員
登入




您好
登出


會員專區
客服中心
查訂單


 
（ 0 件）0 元
結帳












購物車
（ 0 件）0 元
結帳



金石堂及銀行均不會請您操作ATM!  如接獲電話要求您前往ATM提款機，請不要聽從指示，以免受騙上當!
    (詳情)




人工神經網路－金石堂網路書店歡迎您加入博客和facebook 臉書粉絲團！The largest retail books store chains in Taiwan! 





TOP關鍵字

2017希望書包
怪獸的產地
七龍珠
肆一 
蠟筆小新套票
抽EC






全館搜尋
中文書
英文書
簡體書
雜誌
MOOK
文具
玩具親子
美妝配飾
3C
家電
運動休閒
居家生活
動漫部屋
日用清潔
食品




 

天花版bn

12345678910





購物目錄


首頁
3h快送
中文書
英文書
簡體書
雜誌
MOOK
文具
美妝配飾
玩具親子
3C
家電
運動休閒
居家生活
日用清潔
動漫精品
食品






商品次目錄


新書
預購書
推薦書
套書
暢銷書排行榜
書展 / 特惠
讀者書評
出版社專區
香港出版品




 

    您的位置：金石堂網路書店
中文書
電腦資訊
網路／安全／概論
網路技術
商品詳情

 


人工神經網路
                        










作者：張維哲
追蹤



出版社：全欣      
出版社追蹤


出版日：1992/10/2

語言：中文繁體
適讀年齡：全齡適讀


定價：290 元

特價：89 折 258 元 (可得紅利2點)
紅利優惠價：86 折 250 元 (折抵說明)紅利可抵：8 元 
信用卡紅利：可折抵多家銀行 (扣抵說明)
運送方式：全球配送　香港到店　國內宅配國內店取　












<<



>>


https://cdn.kingstone.com.tw/book/images/product/20147/2014710958047/2014710958047b.jpg














                           商品暫時無法購買
                           


金石堂讀者好評
 
0 個人說讚，看排行 >






電腦資訊分類追蹤
使用此功能請先登入金石堂網路書店正式會員。這是什麼？




分享：
                            
                           	  











書籍介紹
其他讀者也買
強力推薦
讀者好評(0)
下標籤
團體專屬服務






詳細資料







詳細資料top 

                               語言：中文繁體規格：平裝分級：普級















網路技術相關書籍
延伸閱讀推薦
延伸推薦









延伸推薦top 





Minitel...

特價 1764元





The Darkening ...

特價 945元





Ipv6 Fundament...

特價 3023元





Unsolved!...

特價 1103元





Cisco Ise for ...

特價 3527元





電網防災減災應急管理系統建設...

特價 194元
立即購買 




局域網組建與維護案例教程（簡...

特價 162元
立即購買 




輕量級Web應用開發寶典：S...

特價 403元
立即購買 











共0篇好評top 
寫書評去 >










商品標籤 (什麼是標籤？)

UML



我的標籤新增









團體專屬服務top

團體專屬服務














訂購須知top 
防治詐騙，提醒您!!金石堂及銀行均不會請您操作ATM! 如接獲電話要求您前往ATM提款機，請不要聽從指示，以免受騙上當! 

商品運送說明：
當商品送達金石堂門市或便利商店後，您會收到E-mail及APP出貨/到貨通知，您也可透過【訂單查詢】確認到貨情況。
建議您可下載『金石堂APP』並開啟推撥設定，即可收到相關出貨/到貨通知訊息。
並請您於指定期限內取貨付款，若逾期未取，您取貨的金石堂門市或便利商店將會辦理退貨作業。
產品顏色可能會因網頁呈現與拍攝關係產生色差，圖片僅供參考，商品依實際供貨樣式為準。 

退換貨說明：
依據「消費者保護法」第19條及行政院消費者保護處公告之「通訊交易解除權合理例外情事適用準則」，以下商品購買後，除商品本身有瑕疵外，將不提供7天的猶豫期：

                            1、 易於腐敗、保存期限較短或解約時即將逾期。（如：生鮮食品）
                            2、 依消費者要求所為之客製化給付。（客製化商品）
                            3、 報紙、期刊或雜誌。（含MOOK、外文雜誌）
                            4、 經消費者拆封之影音商品或電腦軟體。
                            5、 非以有形媒介提供之數位內容或一經提供即為完成之線上服務，經消費者事先同意始提供。（如：電子書、電子雜誌、下載版軟體、虛擬商品…等）
                            6、 已拆封之個人衛生用品。（如：內衣褲、刮鬍刀、除毛刀…等）
若非上列種類商品，商品均享有到貨7天的猶豫期（含例假日）。
辦理退換貨時，商品（組合商品恕無法接受單獨退貨）必須是您收到商品時的原始狀態（包含商品本體、配件、贈品、保證書、所有附隨資料文件及原廠內外包裝…等），請勿直接使用原廠包裝寄送，或於原廠包裝上黏貼紙張或書寫文字。退回商品若無法回復原狀，將請您負擔回復原狀所需費用，嚴重時將影響您的退貨權益。










中文書籍分類

文學財經企管生活風格飲食料理心理勵志醫療保健旅遊宗教命理教育/親子教養童書羅曼史輕小說漫畫語言／字辭典藝術設計電腦資訊
程式設計
網頁設計
繪圖／影音／多媒體
辦公軟體／應用軟體
作業系統
資料庫
3C數位生活
APPLE專區
認證考試
網路／安全／概論
網路技術
雲端技術
系統分析／設計
計算機概論
電腦專業辭典


其他電腦資訊

自然科普人文歷史社會哲思考試書／政府出版品參考書全部的分類 >>




相關商品


雜誌



PC DIY! 7月2017第245期
數位狂潮DIGITREND 2017第44期
PC HOME 電腦家庭 7月2017第258期
MAXIMUM PC Spcl: BUILD IT:The Perfect PC Vol.2 2017
PC DIY! 6月2017第244期









外嵌連結
人工神經網路



在金石堂門市找此書
                         選擇縣市：

請選擇
基隆市
臺北市
新北市
桃園市
新竹市
新竹縣
宜蘭縣
苗慄市
苗慄縣
南投縣
臺中市
雲林縣
彰化縣
嘉義市
臺南市
高雄市
屏東縣

查詢












↑回上方

金石堂網路書店

首頁
關於金石堂網路書店
人才招募
客服中心
異業合作
出版情報
手機版
關於金石堂書店
金石堂書店全台門市



客服專線：02-2364-9989
傳真：02-2364-4672
客服時間：週一至週五 9：00∼12：30 及 13：30∼18：00（例假日除外）
地址：100 台灣臺北市中正區汀州路三段 160 巷 3 號 2 樓
Copyright©2016, Digital Kingstone Co., Ltd. 金石網絡股份有限公司






瀏覽本站建議使用：Internet Explorer 8.0 以上或 FireFox、Google Chrome、Safari 等瀏覽器。
本網站已依台灣網站內容分級規定處理且符合電子商務、安全交易















金石堂APP出/到貨提醒不漏接，讓您便利隨行
















.14


人工神經網路- 台灣Wiki 首頁百科圖片百科分類TwWiki台灣Wiki>百科分類>物理化學>術語>電子術語>電子技術>電子>能源科學>自動化>管理理論>人工神經網路評論（0）分享到：分享到Facebook分享到Twitter分享到Google+分享到噗浪人工神經網路 標籤： 暫無標籤  人工神經網路（artificial neural network，縮寫ANN），簡稱神經網路（neural network，縮寫NN），是一種模仿生物神經網路的結構和功能的數學模型或計算模型。神經網路由大量的人工神經元聯結進行計算。大多數情況下人工神經網路能在外界信息的基礎上改變內部結構，是一種自適應系統。現代神經網路是一種非線性統計性數據建模工具，常用來對輸入和輸出間複雜的關係進行建模，或用來探索數據的模式。它的構築理念是受到生物（人或其他動物）神經網路功能的運作啟發而產生的。人工神經網路通常是通過一個基於數學統計學類型的學習方法（Learning Method）得以優化，所以人工神經網路也是數學統計學方法的一種實際應用，通過統計學的標準數學方法我們能夠得到大量的可以用函數來表達的局部結構空間，另一方面在人工智慧學的人工感知領域，我們通過數學統計學的應用可以來做人工感知方面的決定問題(也就是說通過統計學的方法，人工神經網路能夠類似人一樣具有簡單的決定能力和簡單的判斷能力)，這種方法比起正式的邏輯學推理演算更具有優勢。 目錄1. 生理原理2. 特徵3. 歷史沿革4. 內容5. 優越性6. 研究方向7. 應用分析8. 發展趨勢回目錄1 人工神經網路 -生理原理 神經元與神經元通過突觸建立了廣泛的聯繫，構成了極端複雜的神經網路，從而保證實現對信息的接收、傳遞和處理的功能。據估計，大腦皮層每個神經細胞可有30000個突觸。神經網路的基本聯繫方式主要有三種：(1) 輻射式。即一個神經元的軸突通過它的末梢分支與許多神經元建立突觸聯繫。這種聯繫可以使一個神經元的興奮引起多個神經元的同時性興奮或抑制。傳入神經元主要按照輻射式建立突觸聯繫。(2) 聚合式。即許多神經元的神經末梢共同與一個神經元建立突觸聯繫。這許多神經元可能都是興奮的或都是抑制的，也可能有的引起興奮，有的引起抑制，它們聚合起來共同決定著突觸後神經元的活動狀態。這種聯繫表現了神經興奮在時間和空間上的整合作用。傳出神經元主要按照聚合式建立突觸聯繫。(3) 環式。即一個神經元發出的神經衝動經過幾個中間神經元，又傳回至原發衝動的神經元。它使神經衝動在這個迴路內往返傳遞，形成時間上的多次加強。以上神經元的各種聯繫方式，是神經系統協調反射活動的基礎。回目錄2 人工神經網路 -特徵 人工神經網路由大量處理單元互聯組成的非線性、自適應信息處理系統。它是在現代神經科學研究成果的基礎上提出的，試圖通過模擬大腦神經網路處理、記憶信息的方式進行信息處理。人工神經網路具有四個基本特徵：（1）非線性 非線性關係是自然界的普遍特性。大腦的智慧就是一種非線性現象。人工神經元處於激活或抑制二種不同的狀態，這種行為在數學上表現為一種非線性關係。具有閾值的神經元構成的網路具有更好的性能，可以提高容錯性和存儲容量。（2）非局限性 一個神經網路通常由多個神經元廣泛連接而成。一個系統的整體行為不僅取決於單個神經元的特徵，而且可能主要由單元之間的相互作用、相互連接所決定。通過單元之間的大量連接模擬大腦的非局限性。聯想記憶是非局限性的典型例子（3）非常定性 人工神經網路具有自適應、自組織、自學習能力。神經網路不但處理的信息可以有各種變化，而且在處理信息的同時，非線性動力系統本身也在不斷變化。經常採用迭代過程描寫動力系統的演化過程。（4）非凸性 一個系統的演化方向，在一定條件下將取決於某個特定的狀態函數。例如能量函數，它的極值相應於系統比較穩定的狀態。非凸性是指這種函數有多個極值，故系統具有多個較穩定的平衡態，這將導致系統演化的多樣性。人工神經網路中，神經元處理單元可表示不同的對象，例如特徵、字母、概念，或者一些有意義的抽象模式。網路中處理單元的類型分為三類：輸入單元、輸出單元和隱單元。輸入單元接受外部世界的信號與數據；輸出單元實現系統處理結果的輸出；隱單元是處在輸入和輸出單元之間，不能由系統外部觀察的單元。神經元間的連接權值反映了單元間的連接強度，信息的表示和處理體現在網路處理單元的連接關係中。人工神經網路是一種非程序化、適應性、大腦風格的信息處理，其本質是通過網路的變換和動力學行為得到一種並行分散式的信息處理功能，並在不同程度和層次上模仿人腦神經系統的信息處理功能。它是涉及神經科學、思維科學、人工智慧、計算機科學等多個領域的交叉學科。人工神經網路是並行分散式系統，採用了與傳統人工智慧和信息處理技術完全不同的機理，剋服了傳統的基於邏輯符號的人工智慧在處理直覺、非結構化信息方面的缺陷，具有自適應、自組織和實時學習的特點。回目錄3 人工神經網路 -歷史沿革 人工神經網路1943年，心理學家W.S.McCulloch和數理邏輯學家W.Pitts建立了神經網路和數學模型，稱為MP模型。他們通過MP模型提出了神經元的形式化數學描述和網路結構方法，證明瞭單個神經元能執行邏輯功能，從而開創了人工神經網路研究的時代。1949年，心理學家提出了突觸聯繫強度可變的設想。60年代，人工神經網路的到了進一步發展，更完善的神經網路模型被提出，其中包括感知器和自適應線性元件等。M.Minsky等仔細分析了以感知器為代表的神經網路系統的功能及局限後，於1969年出版了《Perceptron》一書，指出感知器不能解決高階謂詞問題。他們的論點極大地影響了神經網路的研究，加之當時串列計算機和人工智慧所取得的成就，掩蓋了發展新型計算機和人工智慧新途徑的必要性和迫切性，使人工神經網路的研究處於低潮。在此期間，一些人工神經網路的研究者仍然致力於這一研究，提出了適應諧振理論（ART網）、自組織映射、認知機網路，同時進行了神經網路數學理論的研究。以上研究為神經網路的研究和發展奠定了基礎。1982年，美國加州工學院物理學家J.J.Hopfield提出了Hopfield神經網格模型，引入了「計算能量」概念，給出了網路穩定性判斷。 1984年，他又提出了連續時間Hopfield神經網路模型，為神經計算機的研究做了開拓性的工作，開創了神經網路用於聯想記憶和優化計算的新途徑，有力地推動了神經網路的研究，1985年，又有學者提出了波耳茲曼模型，在學習中採用統計熱力學模擬退火技術，保證整個系統趨於全局穩定點。1986年進行認知微觀結構地研究，提出了並行分佈處理的理論。人工神經網路的研究受到了各個發達國家的重視，美國國會通過決議將1990年1月5日開始的十年定為「腦的十年」，國際研究組織號召它的成員國將「腦的十年」變為全球行為。在日本的「真實世界計算（RWC）」項目中，人工智慧的研究成了一個重要的組成部分。回目錄4 人工神經網路 -內容 人工神經網路人工神經網路模型主要考慮網路連接的拓撲結構、神經元的特徵、學習規則等。目前，已有近40種神經網路模型，其中有反傳網路、感知器、自組織映射、Hopfield網路、波耳茲曼機、適應諧振理論等。根據連接的拓撲結構，神經網路模型可以分為：（1）前向網路 網路中各個神經元接受前一級的輸入，並輸出到下一級，網路中沒有反饋，可以用一個有向無環路圖表示。這種網路實現信號從輸入空間到輸出空間的變換，它的信息處理能力來自於簡單非線性函數的多次複合。網路結構簡單，易於實現。反傳網路是一種典型的前向網路。（2）反饋網路 網路內神經元間有反饋，可以用一個無向的完備圖表示。這種神經網路的信息處理是狀態的變換，可以用動力學系統理論處理。系統的穩定性與聯想記憶功能有密切關係。Hopfield網路、波耳茲曼機均屬於這種類型。學習是神經網路研究的一個重要內容，它的適應性是通過學習實現的。根據環境的變化，對權值進行調整，改善系統的行為。由Hebb提出的Hebb學習規則為神經網路的學習演算法奠定了基礎。Hebb規則認為學習過程最終發生在神經元之間的突觸部位，突觸的聯繫強度隨著突觸前後神經元的活動而變化。在此基礎上，人們提出了各種學習規則和演算法，以適應不同網路模型的需要。有效的學習演算法，使得神經網路能夠通過連接權值的調整，構造客觀世界的內在表示，形成具有特色的信息處理方法，信息存儲和處理體現在網路的連接中。根據學習環境不同，神經網路的學習方式可分為監督學習和非監督學習。在監督學習中，將訓練樣本的數據加到網路輸入端，同時將相應的期望輸出與網路輸出相比較，得到誤差信號，以此控制權值連接強度的調整，經多次訓練後收斂到一個確定的權值。當樣本情況發生變化時，經學習可以修改權值以適應新的環境。使用監督學習的神經網路模型有反傳網路、感知器等。非監督學習時，事先不給定標準樣本，直接將網路置於環境之中，學習階段與工作階段成為一體。此時，學習規律的變化服從連接權值的演變方程。非監督學習最簡單的例子是Hebb學習規則。競爭學習規則是一個更複雜的非監督學習的例子，它是根據已建立的聚類進行權值調整。自組織映射、適應諧振理論網路等都是與競爭學習有關的典型模型。研究神經網路的非線性動力學性質，主要採用動力學系統理論、非線性規劃理論和統計理論，來分析神經網路的演化過程和吸引子的性質，探索神經網路的協同行為和集體計算功能，瞭解神經信息處理機制。為了探討神經網路在整體性和模糊性方面處理信息的可能，混沌理論的概念和方法將會發揮作用。混沌是一個相當難以精確定義的數學概念。一般而言，「混沌」是指由確定性方程描述的動力學系統中表現出的非確定性行為，或稱之為確定的隨機性。「確定性」是因為它由內在的原因而不是外來的雜訊或乾擾所產生，而「隨機性」是指其不規則的、不能預測的行為，只可能用統計的方法描述。混沌動力學系統的主要特徵是其狀態對初始條件的靈敏依賴性，混沌反映其內在的隨機性。混沌理論是指描述具有混沌行為的非線性動力學系統的基本理論、概念、方法，它把動力學系統的複雜行為理解為其自身與其在同外界進行物質、能量和信息交換過程中內在的有結構的行為，而不是外來的和偶然的行為，混沌狀態是一種定態。混沌動力學系統的定態包括：靜止、平穩量、周期性、準同期性和混沌解。混沌軌線是整體上穩定與局部不穩定相結合的結果，稱之為奇異吸引子。一個奇異吸引子有如下一些特徵：（1）奇異吸引子是一個吸引子，但它既不是不動點，也不是周期解；（2）奇異吸引子是不可分割的，即不能分為兩個以及兩個以上的吸引子；（3）它對初始值十分敏感，不同的初始值會導致極不相同的行為。回目錄5 人工神經網路 -優越性 人工神經網路人工神經網路的特點和優越性，主要表現在三個方面：第一，具有自學習功能。例如實現圖像識別時，只在先把許多不同的圖像樣板和對應的應識別的結果輸入人工神經網路，網路就會通過自學習功能，慢慢學會識別類似的圖像。自學習功能對於預測有特別重要的意義。預期未來的人工神經網路計算機將為人類提供經濟預測、市場預測、效益預測，其應用前途是很遠大的。第二，具有聯想存儲功能。用人工神經網路的反饋網路就可以實現這種聯想。第三，具有高速尋找優化解的能力。尋找一個複雜問題的優化解，往往需要很大的計算量，利用一個針對某問題而設計的反饋型人工神經網路，發揮計算機的高速運算能力，可能很快找到優化解。回目錄6 人工神經網路 -研究方向 人工神經網路神經網路的研究可以分為理論研究和應用研究兩大方面。理論研究可分為以下兩類：1、利用神經生理與認知科學研究人類思維以及智能機理。2、利用神經基礎理論的研究成果，用數理方法探索功能更加完善、性能更加優越的神經網路模型，深入研究網路演算法和性能，如：穩定性、收斂性、容錯性、魯棒性等；開發新的網路數理理論，如：神經網路動力學、非線性神經場等。應用研究可分為以下兩類：1、神經網路的軟體模擬和硬體實現的研究。2、神經網路在各個領域中應用的研究。這些領域主要包括：模式識別、信號處理、知識工程、專家系統、優化組合、機器人控制等。隨著神經網路理論本身以及相關理論、相關技術的不斷發展，神經網路的應用定將更加深入。回目錄7 人工神經網路 -應用分析 神經網路近來越來越受到人們的關註，因為它為解決大複雜度問題提供了一種相對來說比較有效的簡單方法。神經網路可以很容易的解決具有上百個參數的問題（當然實際生物體中存在的神經網路要比我們這裡所說的程序模擬的神經網路要複雜的多）。神經網路常用於兩類問題：分類和回歸。在結構上，可以把一個神經網路劃分為輸入層、輸出層和隱含層（見圖1）。輸入層的每個節點對應一個個的預測變數。輸出層的節點對應目標變數，可有多個。在輸入層和輸出層之間是隱含層（對神經網路使用者來說不可見），隱含層的層數和每層節點的個數決定了神經網路的複雜度。圖1　一個神經元網路除了輸入層的節點，神經網路的每個節點都與很多它前面的節點（稱為此節點的輸入節點）連接在一起，每個連接對應一個權重Wxy，此節點的值就是通過它所有輸入節點的值與對應連接權重乘積的和作為一個函數的輸入而得到，我們把這個函數稱為活動函數或擠壓函數。如圖2中節點4輸出到節點6的值可通過如下計算得到：W14*節點1的值+W24*節點2的值神經網路的每個節點都可表示成預測變數（節點1，2）的值或值的組合（節點3-6）。註意節點6的值已經不再是節點1、2的線性組合，因為數據在隱含層中傳遞時使用了活動函數。實際上如果沒有活動函數的話，神經元網路就等價於一個線性回歸函數，如果此活動函數是某種特定的非線性函數，那神經網路又等價於邏輯回歸。調整節點間連接的權重就是在建立（也稱訓練）神經網路時要做的工作。最早的也是最基本的權重調整方法是錯誤回饋法，現在較新的有變化坡度法、類牛頓法、Levenberg-Marquardt法、和遺傳演算法等。無論採用那種訓練方法，都需要有一些參數來控制訓練的過程，如防止訓練過度和控制訓練的速度。圖2　帶權重Wxy的神經元網路決定神經網路拓撲結構（或體系結構）的是隱含層及其所含節點的個數，以及節點之間的連接方式。要從頭開始設計一個神經網路，必須要決定隱含層和節點的數目，活動函數的形式，以及對權重做那些限制等，當然如果採用成熟軟體工具的話，他會幫你決定這些事情。在諸多類型的神經網路中，最常用的是前向傳播式神經網路，也就是我們前面圖示中所描繪的那種。人們下麵詳細討論一下，為討論方便假定只含有一層隱含節點。可以認為錯誤回饋式訓練法是變化坡度法的簡化，其過程如下：前向傳播：數據從輸入到輸出的過程是一個從前向後的傳播過程，後一節點的值通過它前面相連的節點傳過來，然後把值按照各個連接權重的大小加權輸入活動函數再得到新的值，進一步傳播到下一個節點。回饋：當節點的輸出值與我們預期的值不同，也就是發生錯誤時，神經網路就要「學習」（從錯誤中學習）。我們可以把節點間連接的權重看成後一節點對前一節點的「信任」程度（他自己向下一節點的輸出更容易受他前面哪個節點輸入的影響）。學習的方法是採用懲罰的方法，過程如下：如果一節點輸出發生錯誤，那麼他看他的錯誤是受哪個（些）輸入節點的影響而造成的，是不是他最信任的節點（權重最高的節點）陷害了他（使他出錯），如果是則要降低對他的信任值（降低權重），懲罰他們，同時升高那些做出正確建議節點的信任值。對那些收到懲罰的節點來說，他也需要用同樣的方法來進一步懲罰它前面的節點。就這樣把懲罰一步步向前傳播直到輸入節點為止。對訓練集中的每一條記錄都要重複這個步驟，用前向傳播得到輸出值，如果發生錯誤，則用回饋法進行學習。當把訓練集中的每一條記錄都運行過一遍之後，人們稱完成一個訓練周期。要完成神經網路的訓練可能需要很多個訓練周期，經常是幾百個。訓練完成之後得到的神經網路就是在通過訓練集發現的模型，描述了訓練集中響應變數受預測變數影響的變化規律。由於神經網路隱含層中的可變參數太多，如果訓練時間足夠長的話，神經網路很可能把訓練集的所有細節信息都「記」下來，而不是建立一個忽略細節只具有規律性的模型，我們稱這種情況為訓練過度。顯然這種「模型」對訓練集會有很高的準確率，而一旦離開訓練集應用到其他數據，很可能準確度急劇下降。為了防止這種訓練過度的情況，人們必須知道在什麼時候要停止訓練。在有些軟體實現中會在訓練的同時用一個測試集來計算神經網路在此測試集上的正確率，一旦這個正確率不再升高甚至開始下降時，那麼就認為現在神經網路已經達到做好的狀態了可以停止訓練。圖3中的曲線可以幫人們理解為什麼利用測試集能防止訓練過度的出現。在圖中可以看到訓練集和測試集的錯誤率在一開始都隨著訓練周期的增加不斷降低，而測試集的錯誤率在達到一個谷底後反而開始上升，人們認為這個開始上升的時刻就是應該停止訓練的時刻。人工神經網路神經網路在訓練周期增加時準確度的變化情況神經元網路和統計方法在本質上有很多差別。神經網路的參數可以比統計方法多很多。如圖4中就有13個參數（9個權重和4個限制條件）。由於參數如此之多，參數通過各種各樣的組合方式來影響輸出結果，以至於很難對一個神經網路表示的模型做出直觀的解釋。實際上神經網路也正是當作「黑盒」來用的，不用去管「盒子」裡面是什麼，只管用就行了。在大部分情況下，這種限制條件是可以接受的。比如銀行可能需要一個筆跡識別軟體，但他沒必要知道為什麼這些線條組合在一起就是一個人的簽名，而另外一個相似的則不是。在很多複雜度很高的問題如化學試驗、機器人、金融市場的模擬、和語言圖像的識別，等領域神經網路都取得了很好的效果。神經網路的另一個優點是很容易在並行計算機上實現，可以把他的節點分配到不同的CPU上並行計算。在使用神經網路時有幾點需要註意：第一，神經網路很難解釋，目前還沒有能對神經網路做出顯而易見解釋的方法學。第二，神經網路會學習過度，在訓練神經網路時一定要恰當的使用一些能嚴格衡量神經網路的方法，如前面提到的測試集方法和交叉驗證法等。這主要是由於神經網路太靈活、可變參數太多，如果給足夠的時間，他幾乎可以「記住」任何事情。第三，除非問題非常簡單，訓練一個神經網路可能需要相當可觀的時間才能完成。當然，一旦神經網路建立好了，在用它做預測時運行時還是很快得。第四，建立神經網路需要做的數據準備工作量很大。一個很有誤導性的神話就是不管用什麼數據神經網路都能很好的工作並做出準確的預測。這是不確切的，要想得到準確度高的模型必須認真的進行數據清洗、整理、轉換、選擇等工作，對任何數據挖掘技術都是這樣，神經網路尤其註重這一點。比如神經網路要求所有的輸入變數都必須是0-1（或-1--+1）之間的實數，因此像「地區」之類文本數據必須先做必要的處理之後才能用作神經網路的輸入。回目錄8 人工神經網路 -發展趨勢 人工神經網路特有的非線性適應性信息處理能力，剋服了傳統人工智慧方法對於直覺，如模式、語音識別、非結構化信息處理方面的缺陷，使之在神經專家系統、模式識別、智能控制、組合優化、預測等領域得到成功應用。人工神經網路與其它傳統方法相結合，將推動人工智慧和信息處理技術不斷發展。近年來，人工神經網路正向模擬人類認知的道路上更加深入發展，與模糊系統、遺傳演算法、進化機制等結合，形成計算智能，成為人工智慧的一個重要方向，將在實際應用中得到發展。將信息幾何應用於人工神經網路的研究，為人工神經網路的理論研究開闢了新的途徑。神經計算機的研究發展很快，已有產品進入市場。光電結合的神經計算機為人工神經網路的發展提供了良好條件。上一篇[廸]    下一篇 [康迪斯·斯瓦內普爾]相關評論同義詞：暫無同義詞收藏到：收藏到Facebook收藏到Twitter收藏到Google+收藏到噗浪詞條信息瀏覽次數: 419 次更新時間: 2013-07-15相關詞條精彩詞條More>安流鎮七星鄉金源鄉蒸熊掌國際賽艇聯腹腔膿腫分享到分享到Facebook分享到Twitter分享到Google+分享到噗浪1生理原理2特徵3歷史沿革4內容5優越性6研究方向7應用分析8發展趨勢


















Ashing's Blog: 人工神經網路(2)--使用Python實作後向傳遞神經網路演算法(Backprogation artificial neature network)
































































































Ashing's Blog




機器學習及深度學習心得筆記，及使用3D印表機及Arduino製作機器人,機械手臂,自走車...

























































































2016年11月6日 星期日








人工神經網路(2)--使用Python實作後向傳遞神經網路演算法(Backprogation artificial neature network)






     這篇文章介紹後向傳遞神經網路演算法(Backprogation artificial neature network)，並使用Python語言實作實現一XOR邏輯功能的多層網路模型。

     在底下前一篇文章單一神經元感知器的實作上知道，單一感知器無法實作出具XOR邏輯運算的功能，在這篇會改用多層網路模型並使用後向傳遞神經網路演算法(Backprogation artificial neature network)來實現XOR的邏輯功能。



人工神經網路(1)--使用Python實作perceptron(感知器)

http://arbu00.blogspot.tw/2016/11/1-pythonperceptron.html





多層神經網路：

          多層感知器是有一個或是多個隱含層的神經網路，通常網路包含一個來源神經元的輸入層，至少包含一個計算元的隱含層，以及一個計算神經元的輸出層，輸入號信一層一層的向前傳遞，這動作也稱之為前饋式傳遞神經網路，其模型如下圖<一>所示

          多層神經網路，各層多有其特定的功能，輸入層為接受外部的輸入信號，輸出層從隱含層接受輸出信號，為整個網路建立輸出形樣類別。

          隱含層的神經元發現特徵，其權重表示了其輸入型樣中的特徵，輸出層再根據這些特徵確定輸出型樣。

          利用一個隱含層，可以表示輸入信號的任何連續ˋ函數，利用兩個隱含層甚至可以表示不連續的函數，換言之，多個隱含層也可以解決單一感知器只能做單一線性分割的問題。





<圖一>有兩個隱含層的多層感知器神經網路









多層神經網路如何學習?

           最常用的是後向傳遞神經網路演算法，在1969年被首次提出(Bryson和Ho)，但是由於對計算要求過於嚴苛而被忽略，直到20世紀80年代這種演算法才又被重新重視。

          多層網路的學習過程與感知器類似，提供輸入信號經過權重調節加總，計算出實際輸出，跟期望輸出比較算出誤差，再藉由調整權重來減小收斂誤差。

         在感知器中，每個輸入Xi僅有一個相對應的權重Wi和一個輸出Y，但在多層神經網路中，每一個權重對每一個輸出都有貢獻，而每一個輸入信號Xi，連接到各隱含層的神經元都有相對應的權重Wij，每個隱含層輸出也會有相對應的Wjk連結到每個輸出Yk，如下<圖二>所示：

         其中i為輸入層第i個輸入，j表示隱含層第j個神經元感知器，k為輸出層第k個輸出。

在後向傳遞網路中，學習演算法過程分為兩個階段，第一階段與感知器前饋式演算雷同，信號由輸入端向隱含層一層一層傳遞直到輸出層，如果實際輸出與預期的輸出不同，則計算其誤差，而第二階段則是將此誤差反向從輸出端經過隱含層再傳遞回輸入層，在這過程則同時調整其權重Wjk和Wij。反覆這樣的動作直到誤差收斂到一定的滿足條件值。





<圖二>三層後向傳遞網路







後向傳遞神經網路演算法：

       

























接下來實作一個範例，利用<下圖三>的神經網路結構訓練可以具有邏輯XOR的功能：

<圖三>三層神經網路





首先在STEP 1：

先初始以下參數w13=0.5，w14=0.9，w23=0.4，w24=1.0，w35=-1.2，w45=1.1，θ3=0.8，θ4=-0.1，θ5=0.3，α=0.1。

在這裡隱含層或輸出層的某個神經元的臨界值的作用可以用權重θt來表示，這裡t=3，4，5，其固定與一個-1輸入相連接當初始值。

而設定X13=X14=X1，X23=X24=X2，意即神經元輸入編號1及2直接接受X1，X2的輸入不做任何前置處理即分配給隱含層神經元編號3，4。



故假設輸入樣本矩陣為

 #  x1 ,序列為[1.,0.,1.,0.,1.,0.,1.,0.,1.,0.,1.,0.,........]

 #  x2 ,序列為[1.,1.,0.,0.,1.,1.,0.,0.,1.,1.,0.,0.,........]

其對應的X1 XOR X2 的每次疊代預期輸出Yd應為

序列[0.,1.,1.,0., 0.,1.,1.,0.,0.,1.,1.,0.,....]



接著啟動程式反覆執行上述演算法的STEP2~4，直到誤差的平方含小於0.05認定為收斂。



    底下<圖四><圖五>為範例程式跑出的結果，在<圖四>左上角即為激勵函數Sigmoid的函數圖，而右上角紅色線為誤差平方和收斂至0.05的過程，X軸為周期,在此範例四次疊代為一週期，Y軸即為每次疊代的誤差平方和。

    底下<圖五>可以看出每次疊代的權重及誤差變化，並算出最後收斂的各權重值，最後則是利用訓練得到的權重結果代回隱含層的兩個神經元可以得到如下<圖四>左下角的結果與決策邊界,在兩決策邊界藍線及紅色所圍成的區域即為訓練XOR邏輯所得出的正確結果，可以看出在此範圍內的P1(0,1)及P4(1,0)做XOR運算可以得到1，另外兩點P0(0,0)及P3(1,1)做XOR運算可以得到0。可以看出此三層神經網路模型確實分類出結果，從左下圖的點分佈也可以解釋為何單一感知器無法訓練出XOR的功能。



<圖四>三層神經網路訓練邏輯XOR的結果1







在底下<圖五>可以看出總共疊代了28832次才讓誤差平方和收斂至0.05以下，之後會在介紹如何加快訓練速度以提升訓練效率，其中也可以加大學習率α數值，如改成0.2 or 0.3...可以加快收斂速度,但是設定過大學習率α數值也可能使誤差上下大幅搖擺，而不易收斂。另外也可以使用別的激勵函數來加快收斂速度。

        在這範例程式最後直接使用訓練完的網路重新預測輸入X1 XOR X2的結果，結果如<圖五>最下方的列表，當然此一模型也可以重新訓練上一篇感知器做過的AND 及Or邏輯功能，只需將對應的輸入X1，X2，預期輸出Yd重新給相對應的值即可以算出結果。



<圖五>三層神經網路訓練邏輯XOR的結果2











<Python 完整範例程式>


import cv2
import math
import numpy as np
from matplotlib import pyplot as plt
from decimal import *

getcontext().prec = 5 #設定湖點數精度4位
getcontext().rounding = ROUND_HALF_UP #4捨5入

#畫出激勵函數函數 Ysigmoid=1/(1+e^(-x)) ,
def sigmoid(x):
 r = 1.00000/(1+math.pow(math.e,-x))
 return r
#畫出激勵函數函數 Ysigmoid=1/(1+e^(-x)) ,
def EE(k):
 r1 = Decimal(math.pow(E[k],2))+Decimal(math.pow(E[k+1],2))+Decimal(math.pow(E[k+2],2))+Decimal(math.pow(E[k+3],2))
 #print(r1)
 return r1
#畫出函數 x13w13+x23w23-s3=0 , x14w14+x24w24-s4=0 ,
def Fx2(x0,s,w1,w2):
 r2 = (s-x0*w1)/w2
 #print(r1)
 return r2
#print(getcontext())
#最大疊代次數Pmax =預設為最大訓練次數
Pmax=50000
 # 產生輸入x1=1,x2=1
  # 產生輸入一維矩陣 x1 ,序列為[1.,0.,1.,0.,1.,0.,1.,0.,1.,0.,1.,0.,........]
xi1=[1.,0.,1.,0.]
x1=xi1*(Pmax//4)
#print(x1)
 # 產生輸入一維矩陣 x2 ,序列為[1.,1.,0.,0.,1.,1.,0.,0.,1.,1.,0.,0.,........]
xi2=[1.,1.,0.,0.]
x2=xi2*(Pmax//4)      
#print(x2)               
#Y5d ,為x1 or x2 的預期輸出 ,故每次疊代後的預期輸出應為序列[0.,1.,1.,0., 0.,1.,1.,0.,0.,1.,1.,0.,....]
Yt=[0.,1.,1.,0. ]
Y5d=Yt*(Pmax//4)   
x13=x1
x14=x1
x23=x2
x24=x2      

#初始權重wij,及臨界值sx ,學習率a=0.1
w13=np.zeros(Pmax) #初始為0.0 
w14=np.zeros(Pmax) #初始為0.0 
w23=np.zeros(Pmax) #初始為0.0 
w24=np.zeros(Pmax) #初始為0.0 
w35=np.zeros(Pmax) #初始為0.0 
w45=np.zeros(Pmax) #初始為0.0 
s3=np.zeros(Pmax) #初始為0.0  
s4=np.zeros(Pmax) #初始為0.0  
s5=np.zeros(Pmax) #初始為0.0  
w13[0]=0.5
w14[0]=0.9
w23[0]=0.4
w24[0]=1.0
w35[0]=-1.2
w45[0]=1.1
s3[0]=0.8
s4[0]=-0.1
s5[0]=0.3
a=0.2
#宣告初始権重差值矩陣為0,只是for程式設計使用預設值
#Dx 用來修正每次疊代後須修正的權值
DW13 =np.zeros(Pmax) #初始為0.0 0
DW14=np.zeros(Pmax) #初始為0.0  
DW23 =np.zeros(Pmax) #初始為0.0 0
DW24=np.zeros(Pmax) #初始為0.0  
DW35 =np.zeros(Pmax) #初始為0.0 0
DW45=np.zeros(Pmax) #初始為0.0 0
DS3=np.zeros(Pmax) #初始為0.0 
DS4=np.zeros(Pmax) #初始為0.0 
DS5=np.zeros(Pmax) #初始為0.0 
#宣告初始誤差E為0,只是for程式設計使用預設值
#E 為每次疊代後,期望值與實際輸出值的誤差
E =np.zeros(Pmax) #初始為0.0 
#為每次疊代後,誤差梯度
e3 =np.zeros(Pmax) #初始為0.0 
e4 =np.zeros(Pmax) #初始為0.0 
e5 =np.zeros(Pmax) #初始為0.0 
#宣告初始實際輸出值Y矩陣為0,只是for程式設計使用預設值
#第p次疊代實際輸出Y3,Y4,Y5
Y3 =np.zeros(Pmax) #初始為0.0
Y4 =np.zeros(Pmax) #初始為0.0
Y5 =np.zeros(Pmax) #初始為0.0
#Epoch ,疊代次數p
print("疊代次數|輸入x1|輸入x2|期望輸出Yd|實際輸出Y| 權重w35  | 權重w45 | 誤差E  | ")
for p in range(Pmax-1):  #from  0,1... to Pmax
 #print("疊代次數:",p)
 #w13=w13[p]
 #w23=w23[p]
 #s3=s3[p]
 #w14=w14[p]
 #w24=w24[p]
 #s4=s4[p]
 Y3[p]=Decimal(sigmoid(x1[p]*w13[p]+x2[p]*w23[p]-s3[p]))
 #print(Y3[p])
 Y4[p]=Decimal(sigmoid(x1[p]*w14[p]+x2[p]*w24[p]-s4[p]))
 #print(Y4[p])
 Y5[p]=Decimal(sigmoid(Y3[p]*w35[p]+Y4[p]*w45[p]-s5[p]))
 #print(Y5[p])
 E[p]=Decimal(Y5d[p]-Y5[p])
 #print(E[p])
 e5[p]=Decimal(Y5[p])*Decimal(1-Y5[p])*Decimal(E[p])
 #print(e5[p])
 DW35[p]=Decimal(a)*Decimal(Y3[p])*Decimal(e5[p])
 #print(DW35[p])
 DW45[p]=Decimal(a)*Decimal(Y4[p])*Decimal(e5[p])
 #print(DW45[p])
 DS5[p]=Decimal(a)*Decimal(-1)*Decimal(e5[p])
 #print("DS5[p]=",DS5[p])
 e3[p]=Decimal(Y3[p])*Decimal(1-Y3[p])*Decimal(e5[p])*Decimal(w35[p])
 #print(e3[p])
 DW13[p]=Decimal(a)*Decimal(x1[p])*Decimal(e3[p])
 #print(DW13[p])
 DW23[p]=Decimal(a)*Decimal(x2[p])*Decimal(e3[p])
 #print(DW23[p])
 DS3[p]=Decimal(a)*Decimal('-1.00000')*Decimal(e3[p])
 #print("DS3[p]=",DS3[p])
 e4[p]=Decimal(Y4[p])*Decimal(1-Y4[p])*Decimal(e5[p])*Decimal(w45[p])
 #print(e4[p])
 DW14[p]=Decimal(a)*Decimal(x1[p])*Decimal(e4[p])
 #print(DW14[p])
 DW24[p]=Decimal(a)*Decimal(x2[p])*Decimal(e4[p])
 #print(DW24[p])
 DS4[p]=Decimal(a)*Decimal('-1.00000')*Decimal(e4[p])
 #print("DS4[p]=",DS4[p])
 
 w13[p+1]=Decimal(w13[p]+DW13[p])
 #print(w13[p])
 w14[p+1]=Decimal(w14[p]+DW14[p])
 #print(w14[p])
 w23[p+1]=Decimal(w23[p]+DW23[p])
 #print(w23[p])
 w24[p+1]=Decimal(w24[p]+DW24[p])
 #print(w24[p])
 w35[p+1]=Decimal(w35[p]+DW35[p])
 #print(w35[p])
 w45[p+1]=Decimal(w45[p]+DW45[p])
 #print(w45[p])
 s3[p+1]=Decimal(s3[p]+DS3[p])
 #print(s3[p+1])
 s4[p+1]=Decimal(s4[p]+DS4[p])
 #print(s4[p+1])
 s5[p+1]=Decimal(s5[p]+DS5[p])
 #print(s5[p+1])

 #print("疊代次數|輸入x1|輸入x2|期望輸出Yd|實際輸出Y| 權重w35  | 權重w45 | 誤差E  |  ")
 print('    {0:1d}      {1:1d}      {2:1d}       {3:1d}        {4:1.5f}       {5:1.5f}       {6:1.5f}     {7:1.5f}   '\
              .format(p, int(x1[p]),int(x2[p]),int(Y5d[p]),Y5[p],w35[p+1],w45[p+1],E[p]))
 if p>0 & int(p%4)==0: #每4次唯一週期,計算依次誤差平方和
  ee=EE(int(p-4)) 
  if ee<0.005:     #如果小於0.01 結束
   #print("誤差平方和=",ee)
   Pend=p
   break
  else:
   Pend=Pmax


print("訓練結果==========================")
ee=EE(int(Pend-4)) 
print("疊代次數=",Pend)
print("權重w13=",w13[p+1])
print("權重w14=",w14[p+1])
print("權重w23=",w23[p+1])
print("權重w24=",w24[p+1])
print("權重w35=",w35[p+1])
print("權重w45=",w45[p+1])
print("權重s3=",s3[p+1])
print("權重s4=",s4[p+1])
print("權重s5=",s5[p+1])
print("誤差平方和=",ee)

#====Prediction==========================
Xr1=[1.,0.,1.,0.]
Xr2=[1.,1.,0.,0.]
Yrt=[0.,1.,1.,0.]
Yr3=[0.,0.,0.,0.]
Yr4=[0.,0.,0.,0.]
Yr5=[0.,0.,0.,0.]
Er=[0.,0.,0.,0.]
eer=0
Wr13=w13[p+1]
Wr14=w14[p+1]
Wr23=w13[p+1]
Wr24=w14[p+1]
Wr35=w35[p+1]
Wr45=w45[p+1]
sr3=s3[p+1]
sr4=s4[p+1]
sr5=s5[p+1]
print("測試結果======================================================")
print("輸入X1|輸入X2 |期望輸出Yd| 實際輸出Y |  誤差E  | 誤差平方和EE|")
for j in range(4):  #from  0,1... to Pmax
 Yr3[j]=sigmoid(Xr1[j]*Wr13+Xr2[j]*Wr23-sr3)
 Yr4[j]=sigmoid(Xr1[j]*Wr14+Xr2[j]*Wr24-sr4)
 Yr5[j]=sigmoid(Yr3[j]*Wr35+Yr4[j]*Wr45-sr5)
 Er[j]=Yrt[j]-Yr5[j]
 if j==3: #每4次唯一週期,計算依次誤差平方和
  eer= Decimal(math.pow(Er[0],2))+Decimal(math.pow(Er[1],2))+Decimal(math.pow(Er[2],2))+Decimal(math.pow(Er[3],2)) 
 print('   {0:1d}    {1:1d}          {2:1d}       {3:1.5f}     {4:1.5f}    {5:1.5f} '.format(int(Xr1[j]),int(Xr2[j]),int(Yrt[j]),Yr5[j],Er[j],eer))

#畫出激勵函數函數 Ysigmoid=1/(1+e^(-x)) ,
sx=np.linspace(-10.0, 10.0, 50)
sg=np.array([sigmoid(t) for t in sx])     

plt.subplot(2,2,1)    
plt.plot(sx, sg, 'b-',linewidth=2.0)   #紅色線寬可以改成2.0或其他數值
plt.axis([-10.0,10.0, -0.5,1.5])   #分別設定X,Y軸的最小最大值
plt.title(' Ysigmoid=1/(1+e^(-x)) ',fontsize=10)
plt.ylabel('Ysigmoid=1/(1+e^(-x))')
plt.xlabel('x')
plt.grid(True)

sp=np.arange(int(Pend/4))
se=np.array([EE(k) for k in range(0,Pend-4,4)])  
#if se.shape>sp.shape:
sp=sp[0:se.shape[0]]
#print("sp.shape=",sp.shape)
#print("se.shape=",se.shape)

plt.subplot(2,2,2)    
plt.plot(sp, se, 'r-',linewidth=2.0)   #紅色線寬可以改成2.0或其他數值
plt.axis([0,Pend/4, 0.0,1.1])   #分別設定X,Y軸的最小最大值
plt.title('Sum(E)^2 ',fontsize=10)
plt.ylabel('Sum(E)^2')
plt.xlabel('Period')
plt.grid(True)


Px=np.linspace(-10.0, 10.0, 100)
p23=np.array([Fx2(k,s3[p+1],w13[p+1],w23[p+1]) for k in Px])  
p24=np.array([Fx2(k,s4[p+1],w14[p+1],w24[p+1]) for k in Px])  
 
plt.subplot(2,2,3) 
plt.plot(Px, p23, 'b-',linewidth=2.0)  #紅色線寬可以改成2.0或其他數值
plt.plot(Px, p24, 'r-',linewidth=2.0)  #紅色線寬可以改成2.0或其他數值
plt.axis([-0.5,2.0,-0.5,2.0])     #分別設定X,Y軸的最小最大值
plt.title(' X1 xor X2 Result: ',fontsize=10)
plt.ylabel('X2')
plt.xlabel('X1')
plt.grid(True)
plt.plot(0,0,'ro')
plt.text(0, 0, r'P0(0,0)')
plt.plot(0,1,'ro')
plt.text(0, 1, r'P1(0,1)')
plt.plot(1,1,'ro')
plt.text(1, 1, r'P3(1,1)')
plt.plot(1,0,'ro')
plt.text(1, 0, r'P4(1,0)')
plt.fill_between(Px, p23, p24, color='green', alpha='0.5')

plt.show()







歡迎一起討論,請加入FB--AI人工智慧與機器人社團
https://www.facebook.com/groups/1852135541678378/2068782386680358/?notif_t=like&notif_id=1477094593187641


加入阿布拉機的3D列印與機器人的FB專頁
https://www.facebook.com/arbu00/



<參考書籍及引用資料>
人工智慧:智慧型系統導論 ，作者：NEGNEVITSKY；謝正勳，廖珗洲，李聯旺編譯，全華圖書





張貼者：



Ashing Tsai




於

上午12:05















以電子郵件傳送這篇文章BlogThis！分享至 Twitter分享至 Facebook分享到 Pinterest




標籤：
人工神經網路,
人工智慧,
機械人,
機器人， Arduino,
類神經網路,
artificial neature network,
Backprogation,
OPENCV,
python










1 則留言:




Angel美食記2017年3月8日 下午7:44讚哦~~回覆刪除新增留言載入更多…


























較新的文章


較舊的文章

首頁




訂閱：
張貼留言 (Atom)
















關於我自己





Ashing Tsai





檢視我的完整簡介











熱門文章









深度學習(1)-如何在windows安裝Theano +Keras +Tensorflow並使用GPU加速訓練神經網路
 本篇文章介紹如何安裝Theano 及Keras， Tensorflow深度學習的框架在windows環境上，並快速的使用Keras的內建範例來執行人工神經網路的訓練。        之前也有實作Tensorflow 及caffe在VM+ubuntu16.04環境安裝的經驗，甚至...










人工神經網路(2)--使用Python實作後向傳遞神經網路演算法(Backprogation artificial neature network)
      這篇文章介紹後向傳遞神經網路演算法(Backprogation artificial neature network)，並使用Python語言實作實現一XOR邏輯功能的多層網路模型。        在底下前一篇文章單一神經元感知器的實作上知道，單一感知器無法實作出具X...










 深度學習(2)--使用Tensorflow實作捲積神經網路(Convolutional neural network，CNN)
     捲積神經網路(Convolutional neural network，CNN) ，是一多層的神經網路架構，是以類神經網路實現的深度學習，在許多實際應用上取得優異的成績，尤其在影像物件識別的領域上表現優異。        傳統的 多層感知器(Multilayer Per...










使用python與arduino連接控制LED 
         這篇文章示範如何使用Python跟Arduino連結.並簡單的控制LED ON/OFF.  由於Python 是跨平臺的程式語言所以可以安裝在一般電腦的windows上,也可以安裝在  使用Linux的系統如樹梅派.                底下的範例程式...










ESP8266 WIFI(2) -update firmware and modify baud rate.
         這篇介紹如何更新升級ESP8266的韌體及如何更改Baud rate.       1.首先請參考,底下這篇如何接線跟電腦連線   http://arbu00.blogspot.tw/2015/12/esp8266-wifi-1-connected-with-p...










Arduino 5 Axis Robotic_Arm V2.0 (Arduino 五軸機械手臂)
   This is a 5 Axis Arduino Robotic Arm with 2 controlled method.  1. one is auto mode by programming in arduino nano chip  2 another is man...










使用Python透過Bluetooth與Arduino連結
         這篇文章示範如何使用Python透過Bluetooth跟Arduino連結.並使用Python建立出來的圖形介面來控制Arduino上LED 的ON/OFF.   底下實驗概念接線圖      Python 因為是跨平臺的程式語言所以也執行在windows或是L...










機器學習(1)--使用OPENCV KNN實作手寫辨識
         這一篇我們要來利用OPENCV 所提供的kNN(k-Nearest Neighbour ）來實作手寫辨識。在程式實作中，我稍微改變了OPENCV官版原本的範例程式，除了修正在Python3.5+OPENCV3.x  build code 會error以外，程式最...















網誌存檔




網誌存檔
七月 2017 (1)
五月 2017 (2)
三月 2017 (4)
二月 2017 (6)
一月 2017 (2)
十二月 2016 (2)
十一月 2016 (10)
十月 2016 (5)
九月 2016 (3)
七月 2016 (5)
五月 2016 (3)
四月 2016 (4)
三月 2016 (2)
二月 2016 (1)
一月 2016 (2)
十二月 2015 (6)
十一月 2015 (2)
十月 2015 (4)
九月 2015 (5)
八月 2015 (3)
七月 2015 (11)


















































































































































AI 突破關係推理，DeepMind 人工神經網路擊敗人類！ - INSIDE 硬塞的網路趨勢觀察
















































































INSIDE 硬塞的網路趨勢觀察

選單






新創


趨勢


娛樂


評論


焦點Hightlight

硬塞科技字典
物聯網






硬塞報童


主機與網站應用




訂閱


隨時關註最新創業、科技、網路、工作訊息。























趨勢

AI 突破關係推理，DeepMind 人工神經網路擊敗人類！ 


2017/6/16



【合作媒體】雷鋒網 

AI、DeepMind、關係推理


評論






評論



 
Photo Credit：meet sally
本文來自合作媒體 雷鋒網 ，INSIDE 經授權轉載。
Google 旗下的 DeepMind 最近開發出了一種用於關係推理的人工神經網路，縮小了人工智慧與人類在關係推理方面的差距。
你正考慮入手的房子附近有多少個公園？某家餐廳最好的晚餐和紅酒搭配是什麼？這些日常問題都需要用到關係推理。關係推理是高級思維的重要組成部分，而 AI 目前還難以掌握。不過， DeepMind 的研究人員已經開發出了一種簡單算法來進行關係推理，而且該算法已經在複雜圖像的理解測試中擊敗了人類。
關係推理是一種運用邏輯，聯繫和比較位置、順序以及其他實體的思維過程。人類通常十分擅長關係推理，但人工智慧的兩種主要模式，基於統計和基於符號計算的算法，在開發類似能力時進展十分緩慢。基於統計的 AI 算法或者說機器學習，在圖像識別領域表現十分出色，但它並沒有運用到邏輯能力。基於符號計算的 AI 算法可以使用預定的規則進行關係推理，但在學習能力方面表現不佳。
神經網路的結構與神經元在大腦中的連接方式相似。它將簡單的程序組合在一起，彼此協同，分析數據間的關係和規律。針對處理圖像、分析語言和學習遊戲等不同用途，神經網路具有不同的專門架構。DeepMind 開發出了全新的關係網路，以分析比較某一特定場景中的每一組對象。DeepMind 在倫敦的計算機科學家 Timothy Lillicrap 表示：「我們的目的很明確，就是推動該網路發現物體間存在的關係」。
Timothy 和他的團隊讓該網路挑戰了幾項任務，以測試其效力。第一項任務是分析某張圖片中幾個物體，比如立方體、球和圓柱體間的關係。測試人員會向該網路提問，比如：藍色的物體前面的物體，和灰色金屬球右邊的微小青色物體形狀是否相同。據雷鋒網瞭解，為完成這一任務，關係網路結合了其他兩種神經網路的能力：一種用於識別圖片中的物體，另一種用於理解測試人員的提問。根據研究人員上周發表的一篇報告（論文下載）：在一系列測試中，其他機器學習算法的正確率只有 42%-77%，人類的正確率可達到 92%，而該關係網路的正確率高達 96%，已經超越了人類。
 DeepMind 團隊還用該網路挑戰了基於語言的任務。測試中，該網路首先會接收到一些語句，比如「Sandra 撿起足球」「Sandra 去辦公室」。然後測試人員向其提問「足球在哪裡」。在回答大多數問題時，該算法與其競爭算法表現相當，不過它在處理「Lily 是一隻天鵝，Lily 是白色的，Greg 也是一隻天鵝，那麼 Greg 是什麼顏色」之類的推理問題時表現更加出色。面對類似問題時，該算法的正確率高達 98%，而其他算法的正確率只有 45%。最後，研究團隊還讓該算法分析了一段動畫，動畫中有十個球彈來彈去，其中一些球通過不可見的彈簧或槓桿連接在一起。僅僅通過運動軌跡，該算法就能準確判斷 90%的連接。研究人員還通過這種方式訓練該算法識別用移動的點代替的人群。
波士頓大學計算機科學家凱特·桑科（Kate Saenko）並沒有參與這該算法的開發，但他聯合開發了另一個能夠回答關於圖像的複雜問題的算法。桑科表示，該算法的優勢之一就是它在概念上非常簡單。該算法取得的進步主要歸功於一個方程式，後者允許該算法與其他網路相結合，正如它在完成比較物體的任務中所做的那樣。研究人員發表的報告中稱該算法為「一個簡單的即插即用模塊」，它可以讓系統中的其他部分專註於它們各自擅長的領域。
加利福尼亞州史丹福大學的計算機科學家賈斯汀·詹森（Justin Johnson）共同參與了第一項測試任務的設計，同時他也聯合開發了一種在該任務中表現出色的算法。他說道：「測試結果令我影響深刻。」桑科則補充道：「未來關係網路可以幫助學習社交網路，分析監控畫面，或者控制自動駕駛汽車。」
詹森表示，要像人類一樣靈活，該算法還必須學會回答更具挑戰性的問題。要實現這一點，該算法不僅要學會比較兩個物體，還要能比較三個物體，甚至多對物體，或者大集合中的某幾組物體。他說道，「我正致力於開發能擁有自己的策略的模型」。 DeepMind 正在開發的是一種特定類型，而非普適性型的關係推理網路，不過它仍然是人類發展上邁出的重要一步。

 






分享文章或觀看評論

評論






【合作媒體】雷鋒網

雷鋒網是深圳英鵬信息技術有限公司於2011年創辦，從起始的科技博客，逐步發展成一個科技信息與產品服務平臺。雷鋒網擁有三大業務單元，深入移動互聯網與智能硬件行業，為廠商及用戶提供了涵蓋媒體資訊，社會化傳播，產品改進和預售等多元系統服務。在智能硬件領域，雷鋒網已當之無愧處於領先媒體。 












訂閱 INSIDE


隨時關註最新創業、科技、網路、工作訊息。






訂閱





相關文章





評論







知名廠商強力徵才中






Android Engineer (APP, SDK) *赴日工作* 

XFLAG (Mixi旗下遊戲公司) 




創業夥伴- 資料分析工程師 

旅型資訊股份有限公司 




Ruby on Rails Full stack developer 

Zooe Inc. 




測試工程師 Test Engineer 

LINEFukuoka [Momozono代徵] 











新創


踏出「休閒運動的一小步」，Reebok 這款鞋將要登上外太空！



2017/7/20



【合作媒體】iFanr愛範兒 

reebok、太空鞋



台灣新創 Pointimize 獨霸亞洲簡報大賽 挺進歐洲科技盛會



2017/7/14



INSIDE 硬塞的網路趨勢觀察 

Pointimize、RISE、歐洲科技盛會



用電商賣吐司　冷凍包子王轉型豪賭！



2017/7/14



商業周刊 

B2C、烘焙、電商



目標時速 1126 公里，「超級高鐵」Hyperloop One 真空試跑成功！



2017/7/13



Chris 

Hyperloop、超級高鐵



創業不再孤獨，SLP Taipei 創業培訓課程即將開跑！



2017/7/4



INSIDE 硬塞的網路趨勢觀察 

SLP、創業、新創


趨勢


比特幣跌價拖累，顯示卡廠 Q3 動能降溫！



2017/7/21



聯合新聞網 

bitcoin、graphics card、比特幣、顯示卡



實現環島目標，特斯拉台灣第二座超級充電站現身臺南奇美博物館



2017/7/20



INSIDE 硬塞的網路趨勢觀察 

Tesla、臺南、特斯拉、超級充電站



告別斷線，捷運車廂有 WiFi！北捷免費網路年底前全面升級



2017/7/20



INSIDE 硬塞的網路趨勢觀察 

WiFi、臺北、捷運、無線網路



微軟開發 GLAS 自動恆溫器，結合 Azure 雲端省能源！



2017/7/20



Claire 

Azure、GLAS、微軟、恆溫器



YouTube 公佈第 2 季最成功廣告，前五皆由和泰、統一一手包辦！



2017/7/20



INSIDE 硬塞的網路趨勢觀察 

Google、Youtube


娛樂


【Cheng lap 的資訊通鑑】打電話玩 RPG：勇者鬥惡婆、修羅戰記與七座金城



2017/7/20



Chenglap 

BBS、RPG、修羅戰記、撥號、遊戲



花錢當課長 SSR 老抽不到？立委籲修法，明示手遊抽獎機率！



2017/7/20



Chris 

SSR、手遊、課金



《冰與火之歌》第七季熱燒，能為 HBO 在網路趕上 Netflix 車尾燈嗎？



2017/7/19



Chris 

HBO、streaming、冰與火之歌



華文首部 VR 電影怎麼拍？蔡明亮說沒有構圖、特寫，要當劇場去規劃



2017/7/18



Chris 

VR、蔡明亮



你的下一部遊戲機，會是一臺 Mac 電腦嗎？



2017/7/18



【合作媒體】iFanr愛範兒 

game、MAC


評論


兩週募破千萬，達成率 1300%！ALPHA Camp 和慕課專案要培養「國際級全端工程師」



2017/7/20



Mia 

ALPHA Camp、全端工程師、募資、慕課、課程



oBike 來台引爭議，問題到底出在哪？



2017/7/20



Mashdigi 

oBike、共享單車、市容問題



在萬物聯網的新時代下，區塊鏈才是撐起一切的安全關鍵！



2017/7/20



精選轉貼 

區塊鏈、萬物聯網



從 Google 出走的這五家「創業車隊」，其命運各自為何？



2017/7/19



【合作媒體】36kr 

Google、無人車創業



Google「你畫我猜」數據分析：你畫圈是順時針還是逆時針？台、日和其他國家大不同



2017/7/18



Mia 

Google、文化、文字、畫圈、畫圓









訂閱 INSIDE


隨時關註最新創業、科技、網路、工作訊息。






訂閱





關於 INSIDE


聯絡我們


隱私權聲明


著作權聲明



© 2009-2017 INSIDE 硬塞的網路趨勢觀察 











































2017-a-Spring - Short Courses for Scientific Computing (SCSC)




 








Short Courses for Scientific Computing (SCSC)












All Courses2017-c-summer2017-d-Chow2018-a-Anzt3. Poster9. Previous Activities2008 Summer2010 Spring2011 Summer2013 Winter2014 Summer2015-a Winter2015-b Spring2016-a Winter2016-b Spring2017-a-Spring2017-b-SpringSitemap



 

9. Previous Activities‎ > ‎
  

2017-a-Spring



NCTS 2017  Courses on High-Performance Computing and Deep Learning國家理論科學研究中心 2017 年春季課程：「高效能計算與深度學習」重要訊息[2017/04/18] The slides of Keras hands-on have been uploaded to the google drive. [2017/04/13] The slides of CNN and Installation of Keras (20170420_Installing Keras.pptx) have been uploaded to the google drive. [2017/04/12] (1) The slides of RBM and DBN have been uploaded to the google drive. (2) Possible problems are listed here.[2017/3/30] (1) Today's slides have been uploaded to the google drive. (2) Please read Chapters 3, 4, 5 of Michael Nielsen's ebook in the next two weeks. (3) Remember to do 3-4 exercises.[2017/3/23] (1) Michael Nielsen's website, e-book with question ID and exercises. (2) Course materials are available online. (3) Homework due on 3/30: [a] Read Chapters 1 and 2 of Nielsen's ebook. [b] Read and work on the python codes. [c] Derive the backpropagation formula of a network containing an input layer, one hidden layer, and an output layer with 10 neurons.[2017/1/12] 本課程「 Part A. 高效能計算」部分是集中課程 (2/21-2/24, 英文授課，詳細資訊：https://goo.gl/Im6v98) 。「 Part A. 高效能計算」的報名網址是 https://goo.gl/forms/Gsi5AXujUZtTZrcf1 (報名截止日期： 2017/01/20。本課程不收費用，但名額有限，獲得錄取學員將另行通知。)[2017/2/28] 本課程加開助教課，時間是2017/3/1 (星期三) 下午 13:00 至 15:00，地點在天數301，助教會帶大家補做2/24的習題以及回答關於程式的疑問，請大家把握機會。上課時間與地點A. 高效能計算 (英文授課，詳細資訊：https://goo.gl/Im6v98)2017/2/21 (二), 2/22 (三), 2/23 (四), 2/24 (五) 9:10-12:00, 13:10-17:00 @臺大天文數學館 301教室授課教師：Kengo Nakajima (The University of Tokyo)，Takahiro Katagiri (Nagoya University)B. 深度學習2017/3/23, 3/30, 4/6, 4/13, 4/20, 4/27 (四) 12:20-15:10 @臺大天文數學館 202教室授課教師:王偉仲 (臺大數學系)、陳素雲 (中研院統計所)、陳定立 (中研院統計所)課程內容高效能計算與深度學習，都是當今快速發展的重要課題。透過對大型問題與巨量資料的高速計算與分析能力， 不僅可以引領科學與工程的新洞見與新應用，對人類如何發現新知識，更產生了典範轉移的效應。我們希望透 過這個課程，讓學生能使用平行計算的觀念來思考問題，進而設計高效能的平行演算法，實作平行程式，並解 決實際應用問題。此外，我們也將協助學生對深度學習有基本的認識，然後在電腦上設計與實作深度學習模型， 利用巨量資訊訓練模型，並將訓練好的模型，應用在實際問題。學習主題包含兩項主軸:一、在高效能計算的 部分，我們將簡介 MPI 與 OpenMP 的平行計算環境，說明如何在此平行環境求解稠密矩陣的特徵值問題，並將 有限體積法以及大型線性系統疊代法平行化，求解三維 Poisson 方程。課程中將使用最先進的超級電腦實機操作。二、在深度學習的部分，我們將介紹 CNN, RNN, DNN等類神經網路的原理與數學推導，以及regularization, early stopping, dropout, parameter tuning 等模型訓練技巧。並實作深度學習模型，處理醫學影像、圖畫風格擷取、 語音與影像辨識等應用問題。上課教材[1] 教師自編講義[2] High Performance Scientific Computing by Randall J. LeVeque (Coursera edition)[3] Introduction to Parallel Programming: Using CUDA to Harness the Power of GPUs by David Luebke, John Owens, Mike Roberts, Cheng-Han Lee (Udacity edition)[4] Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (2016), http://www.deeplearningbook.org/ [5] Neural Networks and Deep Learning by Michael Nielsen (2016), http://neuralnetworksanddeeplearning.com/[6] Keras: Deep Learning library for Theano and TensorFlow, https://keras.io/主持人：王偉仲 (台灣大學數學系與應用數學科學研究所) 聯絡人：冼汶霖 (02-3366-8814, loreinahsien@ncts.ntu.edu.tw)Sponsor:The National Center for Theoretical Sciences (website)



 


















Sign in|Recent Site Activity|Report Abuse|Print Page|Powered By Google Sites



















 










【培訓資訊】高效能計算雲端平臺建置與管理 與 人工智慧(AI)機器深度學習 課程，惠請鼓勵所屬學生踴躍報名 | 輔仁大學生命科學系


























Jump to navigation






             English｜網站地圖







 




高中生專區 

大學部資訊 

畢業系友專區 

陸生/交換生 

博士班 

新生專區 











系所沿革特色系所特色系徽介紹在學術領域之畢業系友本校生物相關研究表現系所位置圖系所宗旨目標系目標之具體做法所目標之具體做法系所各規章辦法系所實驗安全系所儀器設備高教評鑑中心評鑑結果 - 通過系所師資陣容系主任教授群副教授群助理教授群兼任教授群實驗室助教離職退休老師系所課程規劃修業規則各學年度必修科目表課程地圖各年級課程輔系雙主修選修課理工學院學程抵免學分規則通識排除科目畢業學分註意事項本系課程代碼查詢數位課程英語自學網站喬治城大學4+1橋梁計畫系所招生資訊大學部碩士班學生學習成果系所學生事務導師制度系學會學習輔導獎、助學金未來出路學習成效檢核課程講義下載表格下載在校生更新資料置物櫃借用系統學生學習成果新理工大樓新大樓計畫簡介新大樓規劃新大樓募款進度捐款方式50週年慶













訊息分類
最近公告
活動訊息
實習就業機會
科普知識
高中生相關訊息
大學部相關訊息
碩士生相關訊息
系友相關訊息














			【培訓資訊】高效能計算雲端平臺建置與管理 與 人工智慧(AI)機器深度學習 課程，惠請鼓勵所屬學生踴躍報名			





 敬啟者：
 
    鑑於高速計算（HPC, high performance computing）已是科學研究的主要方法之一，其重要性已與理論及實驗鼎足而立。國立臺灣師範大學特斥資籌設雲端運算平臺，並已啟用營運（雲端運算平臺之設備、效能及收費等相關詳情請見網址：https://hpc.ntnu.edu.tw/?url=hardware）。
 
    為因應本校雲端運算平臺之設立及大數據時代的到來，本校特於2017暑假開設下列課程，惠請鼓勵所屬學生踴躍報名。
一、「高效能計算雲端平臺建置與管理課程」。
該50小時的課程（包含上課、實作及實習），可以讓學員學會平臺管理的知識及能力。讓你在大數據的時代，不會輸在起跑點。
上課時間、地點資訊如下：
]上課時間：2017.07.03 ~ 2017.07.21（不含假日）
]上課地點：本校公館校區（北市汀州路4段88號）資訊中心（圖書館8樓）
本次課程費用＄3,000
二、「人工智慧(AI)機器深度學習」。
該56小時的課程（包含上課、實作及實習），可以讓學員學會機器深度學習的知識及能力，且在學完後能回去自行應用的課程。。讓你在人工智慧時代的時代，不會輸在起跑點。
上課時間、地點資訊如下：
]上課時間：2017.08.21 ~ 2017.09.08（不含假日）
]上課地點：本校公館校區（北市汀州路4段88號）資訊中心（圖書館8樓）
本次課程費用＄10,000
 
        各項報名優惠活動，請洽課程報名網址：https://hpc.ntnu.edu.tw
 

（備註：報名及繳費請於進入上述網址後，點選欲報名之課程，進入課程資訊頁後，再點選課程頁上方「課程報名及繳費」之Tag，依網頁指示操作）
 
隨函檢附招生海報如附檔，惠請轉發師生週知，萬分感謝您的協助。
 
國立臺灣師範大學 理學院雲端運算平臺116 臺北市汀州路4段88號
附件:  00_2017-06_台師大高效能計算管理＆AI_課程招生海報.pdf













連絡生科系辦公室｜生科系位置
輔仁大學理工學院生命科學系版權所有© 2013 電話:(02)2905-2468 傳真:(02)2905-2193






 






2016 暑期高效能運算課程-臺大計資中心 










2016 暑期高效能運算課程

High performance computing and Big data.










GPU Deep Learning深度學習工作坊




 
基本資訊
 


【日期】 2016/8/30、8/31
【時間】  ■09:10~12:00  ■13:20~16:10
【地點】 計中 106 教室

【費用】 臺灣大學及國立臺灣大學系統1000 元，其他 2000 元
【主辦單位】國立臺灣大學計算機及資訊網路中心（臺大計中）
【協辦單位】NVIDIA 及 兌全有限公司




 
師資
 


兌全有限公司 / 鄭羽熙 博士
兌全有限公司 總經理
		學歷：國立台灣大學 電機系博士 (1992)
		經歷：1. 中山科學研究院簡聘技正 2. 德碩半導體股份有限公司晶片設計經理 3. 麗臺科技股份有限公司副總工程師
		專長：1. Multi-core parallel processing and GPU massive processing  2. Video codec and algorithm 3. Embedded SOC system  4. FPGA and Chip design  5. ECG and HRV application




 
課程簡介
 


深度學習（Deep Learning）的技術採用了神經網路（Neural Network）多節點及分層的結構來訓練及分析數據的特徵，進而達成可自動對大量數據的辨識與分類。近年來，Google運用深度學習的技術實現了許多智慧型的服務，例如Google Now語音辨識、Gmail自動回信、Gmail垃圾郵件判斷、Google相簿自動分類與辨識、Google翻譯等。因此，深度學習的技術已漸漸地實現了許多我們期待已久的人工智慧( Artificial Intelligence )應用系統；加上現今CPU與GPU平行處理能力的大幅增長，更促進了目前深度學習被開發的腳步。
在兩天的深度學習課程中，第一天所安排的課程內容將由淺而深，從神經網路的基礎模型開始說明，包含訓練與辨識的方法，使學員瞭解基礎模型的運作方式。接著將介紹在深度學習技術中常用的Convolutional Neural Networks(CNN)模型，配合Computer Vision及Image classification等應用實例，可幫助學員實際的瞭解深度學習的訓練及辨識概念；然而運算速度終將成為實際應用的瓶頸，因此運用GPU加速已成為深度學習項目中不可不知的關鍵技術。NVIDIA提供cuDNN等加速程式庫，讓使用者可以運用所提供的加速模型來加速深度學習的計算，因此本課程也將介紹cuDNN等加速程式庫的使用方法。
經過第一天深度學習的基礎訓練後，第二天的課程內容將著重於Berkeley CAFFE、Google TENSORFLOW及NVIDIA DIGITS深度學習平臺的使用。本課程使用國立臺灣大學計算機及資訊網路中心配備的Tesla K80 GPUs作為訓練平臺，讓學員上機練習深度學習的簡單案例，進而使學員能夠在最短的時間內著手深度學習在各領域的研究與開發。
誠摯的歡迎您來參加這場不能錯過的GPU深度學習之旅。




 
課程內容
 





第一天
第二天




Introduction to Neural Network Modeling and Training
Why Deep Learning?
Convolutional Neural Networks(CNN)
					Modeling and Training
					Case Study: LeNet-5
DNN, CNN, and RNN
Deep Learning for Computer Vision, Image classification, and analysis
					Supervised Learning
					Unsupervised Learning
Deep learning in high performance computing
NVIDIA cuDNN Deep Leaning Primitives
					- Accelerating Artificial Intelligence




1.The Engine of Modern AI (Lab Hands on and Demo)
					Berkeley CAFFE and Google TENSORFLOW
					Select Training Dataset
					Select Solver
					Design the Neural Network
					Train
					Profile and Debug
NVIDIA DIGITS - Interactive Deep Learning GPU Training System  (Lab Hands on)
					Process Data
					Configure DNN
					Monitor Progress
					Visualize Layers




 



 
對象
 


具備基本CUDA，C Programming知識，未來想從事高階軟體開發工程師、財務分析工程師、電子/電機相關工程師、測試/量測工程師、醫學界研發人員、生物科技研究人員，以及統計/數值分析人員等




 
備註
 



本課程使用階梯式教室，備有電源插座，建議學員自行攜帶筆記型電腦，以隨時配合講師操作實習。請預先安裝SSH client軟體（如putty）。
本課程提供午餐，限當日準時報到學員。
如因報名人數過多，本中心得更換教室。請學員務必於上課前三日，上網確認上課地點。










立即前往報名














在人工智慧與深度學習的商業市場上，NVIDIA 目前沒有敵手｜數位時代About us廣告刊登商店場地租借EN 新聞
新聞分類最新新聞科技物聯網人工智慧機器人金融科技虛擬實境大數據交通運輸電信通訊資訊安全實驗室裡的科技企業阿裡巴巴Amazon蘋果FacebookGoogleIBM微軟Yahoo商業產業創新策略IPO併購合作創業新創團隊募資創業活動創投創業人物新創管理行銷流量數據廣告創意品牌經營內容行銷社群行銷技能職場未來工作數位工作術產品開發開發者電子商務數位內容新聞媒體遊戲電子書音樂影視人物產品觀點專題PX酷品 活動
活動未來商務展Meet Taipei社群活動雜誌創業小聚數位行銷學院全都為了人工智慧的 NVIDIANVIDIA 黃仁勳：人工智慧需求策動了GPU運算革命1秀肌肉！更小、更快，從端到端覆蓋的 NVIDIA 硬體策略2雲佈局！GPU AAS 佈滿全球公有雲，並結合 Docker 推出 GPU Cloud3NVIDIA創辦人黃仁勳：機器人是終極的AI4在人工智慧與深度學習的商業市場上，NVIDIA 目前沒有敵手5專題故事從遊戲產業裡釣到 GPU 商機大魚的 NVIDIA ，因為意外的佈局捕獲新的重要商業應用：人工智慧。從無人機、機器人、自動駕駛乃至於圖像辨識、輔助診斷醫療、作畫與作曲等。如今深度學習的商業應用看似無所不在，卻也等 NVIDIA 更積極在每一個端點佈局，就是要讓你輕易取得 GPU 提供的豐沛運算資源。人工智慧大數據1 NVIDIA 黃仁勳：人工智慧需求策動了GPU運算革命 
 
by
 James Huang
2017.05.31分享James Huang 攝影分享 近年人工智慧不斷在公眾目光中出現，深度學習的世代彷彿立刻來臨。NVIDIA 創辦人黃仁勳，如今被稱為人工智慧教父；從他的眼中，人工智慧將帶來什麼樣的運算革命，NVIDIA又將往哪裡去？
人工智慧可以是什麼？NVIDIA 2017 年的主軸形象廣告：「我就是人工智慧（I am AI）」，AI形容自己是夢想家，協助蘇黎世聯邦理工學院的太空學者進行光學辨識與數據分析探索未知宇宙。AI也說自己是治療師，協助醫師用一滴血測出白血病患者。AI同時也是保護者，協助 COTSBot 在海底辨識，並滅除破壞珊瑚的海星。AI也像是個助手，協助 Fellow 的機器人 NAVii 幫助人們在賣場挑選、找到自己所需要的物品。AI也可能是作曲家，你所聽到的廣告配樂事實上就是 AI 做的。由 NVIDIA 輝達（NASDAQ：NVDA）所主導、贊助支持之 2017 年 GPU Technology Comference （圖形處理器技術研討會，後簡稱 GTC），在美西時間 2017 年 5 月 8 日起於美國舊金山灣區南灣的聖荷西會議中心展開。第三天一早的重頭戲為台裔美籍、 NVIDIA 創辦人兼執行長黃仁勳（Jensen Huang）的主題演說：「為人工智慧革命提供動力（Powering the AI revolution）」。（編按：與其隨後在臺北 Computex 2017 的演說同題名，內容則分別稍有增刪）GPU Technology Conference 2017 入口現場James Huang 攝影黃仁勳在 2017 GTC大會演講的重點，主要都環繞在人工智慧的一切。演講伊始，黃仁勳就以輕鬆的口吻介紹 CPU （電腦處理器）核心在摩爾定律（Moore's Law）的描述下， 40 年來的成長限制。黃仁勳話風一轉，提及 NVIDIA 在 1993 年創立後，製作出 GPU （圖形處理器）產品，並在 2007 年左右，發現 GPU 相對於 CPU 的演算特別適合用來大量平行運算數學運算，因此 NVIDIA 推出 CUDA 框架，以基礎程式語言 C 的延伸集，透過 CPU 作為指導 GPU 分拆平行運算的架構，來加速特殊程式需求的演算時間。黃仁勳引用了史丹佛大學校長 John L. Hennessy 在校內課程介紹並引用其研究處理器效能成長的數據指出，GPU 的演算效能將以每年 1.5 倍速成長，相對於 CPU 每年 1.1 倍速的成長幅度，到 2025 年將達 1000 倍的差距。「GPU 將比 CPU 對於特定領域的演算擁有更大優勢！」黃仁勳強調。NVIDIA 創辦人黃仁勳解釋摩爾定律之後的生活James Huang 攝影程式，其實是一大堆由程式設計師所寫好的工作流程，電腦必須按照工作流程規則執行工作。電腦科學家很快發現一個電腦一次只能執行一個指令是很慢的事，人類最好能讓電腦同時執行很多事，因此我們開始在應用層次至演算層次把工作流程拆開，使之可以平行執行。由於科學家與工程師、甚至藝術家（讀者不妨先想像動畫、遊戲與電影等領域之創作）對於演算法與應用程式的需求，我們對於軟體平行運算的要求越來越高，也帶動了 GPU 運算的需求出現與成長。（所以最早做 GPU 為了遊戲效果一點都不意外）從 GPU 演算帶動人工智慧發展隨著人類對演算的需求因為大數據、機器學習與人工智慧等應用越來越高，我們發現這些領域的演算也都可以依賴 GPU 執行，造就人們越來越重視 GPU 演算。黃仁勳提及，對電腦科學家而言，仰賴GPU加速的演算至少可以分成四個層次，從基礎架構（Architecture）、系統（System）、演算法（Alorithm）至應用程式（Application），NVIDIA是從下至上（改進演算架構，並提供更好、更快的系統與演算法生態系）、乃至從上至下（隨著應用程式的需求，改進演算法、系統甚至架構），往復不斷地改進、強化每一層次對 GPU 運算的需求，甚至有人稱 NVIDIA 對於 GPU 演算的精進達成了摩爾定律平方的效果。NVIDIA 創辦人黃仁勳解釋GPU運算的成長動能James Huang 攝影GPU 有多重要？ 8 年前 NVIDIA 第一次辦 GTC，參加者人數剛破千，會場就在 2017 年 GTC 舉辦地聖荷西會議中心斜對面的 Fairmont 飯店。這個會議每年的參加者數以 30 % 成長，今年已經有超過 7000 人與會。黃仁勳並引用內部數字，說明 11 年來開發者數成長超過 110 倍，目前已經有超過 50 萬人註冊成為 GPU 的開發者，2016 年下載 CUDA 與相關的 SDK 更突破百萬次以上。GPU 應用轉變幅度有多大？第一年的 GTC，拍攝星際大戰成名的製作公司盧卡斯影業的技術長，首次在此揭露如何運用電腦運算產生真實火焰的影像。但 2017 年的 GTC，雖有 10 家的 VR 公司直接拿成品掩飾準備搶奪由高盛、富達、軟銀與其他公司所提供共六個每名 150 萬美金的投資現金，也有 20 家 VR 新創在會場裡直接做展示。但在全會場共四天近 600 場演說裡，有 60% 都與人工智慧、深度學習等主題與工具有關、全球前 15 大的科技公司（如 IBM、Google、Microsoft、Amazon 等）全部都參加 GTC，更有高達 80 家的 AI 新創團隊群聚於此。現代商用人工智慧於 2012 年後大規模爆發今日所談的人工智慧，往往與機器學習或深度學習等字眼混合運用，事實上，人工智慧比較接近大眾所熟悉、能夠掌握理解的字眼，雖然目前字詞上運用的定義，往往讓大眾想像早超越實際技術討論的範圍，但目前整體來說仍大都是這類技術應用的結晶。人工智慧的發展曾經在歷史上多次因為運算瓶頸而停滯，最近的快速進展主因卻為大量實體世界的數位化資料累積總成本快速降低，與運算速度的快速提升，其中後者與 GPU 運算發展緊密相連。NVIDIA 創辦人黃仁勳解釋摩爾定律James Huang 攝影從在人工智慧研究知名的瑞士人工智慧實驗室 IDSIA (Istituto Dalle Molle di Studi sull'Intelligenza Artificiale) ，發現在神經網絡的研究上可以透過 GPU 運算來進行加速開始（透過 CUDA 對捲積神經網絡，Convolutional Neural Network, CNN 的運算進行最佳化）；到多倫多大學的 Alex Krizhevsky 用 GPU 加速自己透過 CNN 模型改進所做出的 AlexNet 在 2012 年圖形分辨的知名比賽 ImageNet 中獲勝。人工智慧或深度學習的應用終於在商業上有了劇烈爆發性的應用與成長。Google Photo 能夠輕易分辨出人、貓、海邊等圖片資訊、NVIDIA如今與許多車廠合作改裝汽車實驗自動駕駛（Autonumous Driving）的可能性、AlphaGo在圍棋比賽中打敗人類、跨語言翻譯應用乃至自然語言處理的人工智慧助理等。James Huang 攝影人工智慧的學術研究相關進展也在 2012 年後大規模爆發。2016 年參加人工智慧相關的學術研討會（NIPS、ICML、CVPR 與 ICLR）人數較 2014 年成長兩倍來到 13,000 人。（在電腦科學的學術領域，由於學術研討會論文審查、發表與交流速度較期刊更快，許多前瞻研究都追求研討會發表更甚於期刊發表，因此在相關領域觀察各研討會人員參加數、投稿量與文章錄取率，往往遠比學術期刊更有應用價值）Udacity 上已經有超過 20000 人修過人工智慧相關的學程，矽谷在人工智慧新創方面的投資在四年間則增加了 9 倍。NVIDIA 在人工智慧的一系列佈局本次 GTC NVIDIA 主要為以下六個領域超過 1300 家 AI 新創合作，涵蓋：健康、物聯網與製造、零售與電子零售業、自動駕駛、金融、網路應用（Cyber）、建築、工程與營建（AEC, Architecture, Engineering and Construction）、安全與智慧影像分析（IVA, Intelligent Video Analysis）、平臺與API、資料管理、開發平臺、商業智慧與視覺化。以下，《數位時代》將為你全面剖析，整理 NVIDIA 在 GTC、Computex 所宣佈的一連串新產品與服務，並分別針對不同服務細節進行追索訪問，分別看 NVIDIA 在人工智慧領域中，關於硬體、軟體與雲和汽車與機器人市場的深度佈局。數說新語NextPedia圖形處理器GPU（Graphics Processing Unit） 「GPU」又稱顯示核心、視覺處理器、顯示晶片，是一種專門在個人電腦、遊戲機和一些行動裝置上執行繪圖運算工作的微處理器。圖形處理器是Nvidia在1999年首先提出的概念，在此之前，電腦中處理影像輸出的顯示晶片，通常很少被視為是一個獨立的運算單元。圖形處理器使顯示卡減少了對中央處理器（CPU）的依賴，並分擔了部分原本是由中央處理器所擔當的工作，尤其是在進行3D繪圖運算時，功效更加明顯。
(來源：
NVIDIA 、
維基百科 
)
#雲端運算#Computex#Nvidia#人工智慧#GTC#黃仁勳#gpu分享AmazonGoogle微軟物聯網人工智慧機器人大數據開發者2 秀肌肉！更小、更快，從端到端覆蓋的 NVIDIA 硬體策略 
 
by
 James Huang
2017.05.31分享James Huang 攝影分享 從遊戲應用、整合、全新人工智慧運算用微架構 Volta 晶片，到基於 Volta 的一系列主機與資料中心解決方案。再到開發板與自動駕駛解決方案與開源計畫。NVIDIA 這次的硬體策略簡直鋪天蓋地而來。
從遊戲應用走向人工智慧NVIDIA 在 GPU 領域有多厲害？黃仁勳說，2016 年，電動遊戲是一個超過 1000 億美金市值的產業，NVIDIA 出貨超過 1,000 萬個 GeForce GTX 相關的裝置，整個市場安裝 GeForce GTX 至少超過 2 億次。如果根據微軟 2017 財年第二季報告，大約是 XBox Live 每月活躍人數 5,500 的四倍之多。別忘記！許多 GPU 裝置同時也是使用者進入網路世界的第一裝置。黃仁勳在 COMPUTEX 2017 的開幕演說中進一步表示，如果沒有 NVIDIA 台灣合作伙伴的全力支持，全球的遊戲玩家不可能可以這麼順利地得到完整的遊戲平臺，目前，這個。至少包含有 GeForce GTX 旗下將進一步在 Computex 2017 全面更新至少 40 款產品線，包含：全新的遊戲顯示卡、VR背包、遊戲主機、顯示器與遊戲筆電。NVIDIA 創辦人黃仁勳手持 ASUS ROG 筆電現場演示即時運算效能，黃仁勳笑稱這款電腦比那款稱之為 Pro 的電腦（意指 Mac Book Pro）效能還快 60%。James Huang 攝影GeForce GTX 產品線如此亮眼，但每年消費者總是希望裝置可以更小、更輕更薄，運算效能卻可以逐步成長。NVIDIA 選擇直接與製造廠合作，讓雙方的工程師可以彼此直接合作工作，將原本 51mm 的架構，縮小為 18 mm。重量減輕一半（僅剩 5 磅），並帶有三倍效能。NVIDIA 將新的設計稱為 MAX-Q。在 Computex 2017 現場，還直接以與華碩合作的筆電展示了 BANDAI 年底才要上市的 Project Cars 2。Max-Q 可說是 NVIDIA 與電腦製造廠系統合作、整合設計的結晶。其中涉及了硬體設計整合，包含散熱設計與使用低壓電源並調整線路設計，並透過軟硬體同時提升並改善 GPU 的效能與功耗。晶片商與製造商通力合作改善系統整合設計是少見的業界合作，這也表示 NVIDIA 正在加深與遊戲領域相關的製造合作伙伴關係。NVIDIA 創辦人黃仁勳於 Computex 2017 手持 ASUS ROG 系列筆電比較整合前後差異James Huang 攝影30 億美元的結晶！深度學習運算的暴力美學：Volta如果說 NVIDIA 於 2016 年首次推出以 Pascal 微架構為主的 TESLA P100 晶片，是基於人工智慧與深度學習應用的第一次重大硬體策略轉型，則你可以將黃仁勳於 2017 年 5 月宣佈的 TESLA V100 視為 NVIDIA 這家公司更積極在人工智慧產業方向上耕耘的第二發秘密武器。確立了 GPU 除了圖形演算、科學演算，並找到願意為這樣演算需求付費的基礎用途產業後，NVIDIA 需要為 GPU 這樣的運算結構找尋下一個應用成長引擎。NVIDIA 創辦人黃仁勳展示 Volta 微架構晶片James Huang 攝影繼 2016 年 GTC ，NVIDIA 推出了 GPU 架構 Pascal 之後，NVIDIA 於今年再度更新了其最新一代 GPU 運算微架構（ NVIDIA microarchitecture）Volta。如果讀者對 Intel CPU 微架構的歷史熟悉，對 NVIDIA 現今的微架構命名策略大概也不會感到太陌生，兩者對微架構發展與推進有類似的代號包裝，並分別對之推出對應的架構規範。NVIDIA 微架構發表時間、製程工藝與常見商用產品名NVIDIA代號推出時間晶圓製程規格商用產品號Tesla2006年90 nm, 80 nm, 65 nm, and 55 nmGeForce 8, 9, 100, 200, 300 系列Fermi2010年4月40 nm and 28 nmGeForce 400, 500 系列Kepler2012年4月28 nmGeForce 600 與部分 700系列Maxwell2014年2月28 nm晚期的 GeForce 700 與 800, 900 系列Pascal2016年4月14 nm and 16 nmTesla P100 晶片（GP100）、GeForce 1000 seriesVolta2017年5月12 nmTesla V100 晶片《數位時代》整理首次引入 Tensor Core 的 Volta 微架構黃仁勳強調，為了新一代的 GPU 運算架構，NVIDIA 投入了近 5,000 名工程師、為期三年，約 30 億美金的研發費用。這顆應用台積電 12 奈米 FFN 製程的 GPU，整合了相當於 210 億顆電晶體的 TESLA V100 ，相當於 5120 顆 CUDA 核心，擁有 120 TFLOPS的運算能力。撇開當然更快、更大的 HBM2 記憶體通道與攸關 聯絡 GPU 平行運算能力，擁有 300 GB/s 傳輸速度的 NVLink 2.0 不談，這次 Volta 架構最大的變化，來自於 GPU 中 Streaming Multiprocessor （SM）架構的重新設計。NVIDIA Volta 微架構晶片James Huang 攝影Streaming Multiprocessor, SM 是 NVidia GPU 設計得以進行分散式運算 CUDA 的重要設計。原本每個 multiprocessors 就包含有若干個 stream processor ，每個 stream processor 都包含一個 fused-multiply-add, FMA 單元，每個一次（per clock）可以進行一次加法與一次乘法。Volta 架構重新設計了 SM，除了改善由 4 個 L0 所組成為一個 L1 的SM中，每個 L0 內用於整數運算（16 * INT）與浮點運算（8 * FP64 與 16 * FP32 ）的單位。並在每個 L0 裡面引入了 2 個 Tensor Core。NVIDIA 說明 GPU 的 SM 中 Tensor Core 核心加速的 4*4*4 矩陣運算演示NVIDIA 開發者部落格Tensor 其實指的就是方塊，魔術方塊或貨櫃堆疊其實都是方塊的一種。幾乎所有高維度的數學運算都是各種矩陣相乘、相加而成，如果可以透過加速矩陣運算，則幾乎確定可以有效加速所有高維度的數學運算。NVidia 這次設計的 Tensor Core 是一個特化、專門進行 444 矩陣運算的運算單位，可以將原本數學運算需求中 444 的運算需求一次做完，因此可將原本需要 64 次運算，加上暫存共約 80 次的步驟減至 7 次，運算速率接近原本所需的 12 倍。NVIDIA TESLA V100James Huang 攝影所以，雖可見引入 Volta 架構的 TESLA V100 事實上在 SM 數並沒有增加太多，CUDA 核心數增加也不到一倍，但透過增加在 80 個 SM 裡，共 640 個 Tensor Core 可以直接增加 120 個 TFLOPS，直接達成加速浮點運算的目的。藉由改善原本每個 SM 中浮點運算核心（FP32/FP64）的效能，加上運用台積電 12 奈米 FFN 生產技術，在每個標準單位塞入更多的電晶體，Volta 架構就可以實現同樣瓦數下搾出更多運算效能的成績。NVIDIA 也透過 CUDA 9 直接提供各種不同深度學習運算架構例如 Caffe 2 或 MXNet 的使用者可以直接呼叫 CUDA，拆解高維矩陣成低維演算並拼接。這樣設計所帶來的好處是 Tensor Core 可以直接對固定需要高維運算的運算需求進行加速，但一個 SM 應該配置幾個 Tensor Core 就是實際運算應用所會面臨的第一個問題。Tensor Core 勢必佔用 GPU 設計中，原本的整數運算與浮點運算器空間，將原本空間所可以帶來的計算量，換成多少的 Tensor Core 對大型演算所帶來的效益仍有待進一步驗證（目前 NVIDIA 的例子主要都針對容易降維的某些 CNN 進行驗證），在 GPU 內應該有 Tensor Core 的專屬空間或進一步將之分開、Tensor Core 與傳統整數運算與浮點運算器比例配置為何，並每個 Tensor Core 為什麼只選擇 444 作為加速基礎單元，都有賴更多案例，來進一步應用來驗證並最佳化相關的運算需求。以 TESLA V100，推出硬體 DGX-1V 與 DGX StationNVIDIA 也不意外地更新了 2016 年宣佈，搭載 TESLA P100 的 DGX-1 系列。DGX-1 系列是 NVIDIA 針對資料中心級所需處理深度學習、人工智慧等運算所首次推出的主機。2016年首次推出時，DGX-11搭載 8 張 Tesla P100 的計算卡，共有 28672 個 CUDA core。相當於傳統資料中心250台左右的主機運算量。NVIDIA 創辦人黃仁勳展示 Volta 微架構晶片所成的 TESLA V100James Huang 攝影本次基於 Volta 所推出的 DGX-1（或稱 DGX-1V），同樣搭載 8 組 TESLA V100，透過更多的 CUDA 核心，更快的平行傳輸資料能力 （NVLink 2.0），帶來更驚人的運算能力。在 NVIDIA 針對 DGX-1V 的技術報告中，以微軟的 ResNet 做實驗甚至可將原本需要 739 小時訓練的資料運算，縮短至不到 8 小時以內完成。DGX-1V 主要提供給資料中心，要價約 14.9 萬美金。NVIDIA 也為小型新創公司或部分公司的創新部門提供小型的 AI 用計算電腦 DGX-Station，內建 4 張 TESLA V100 的小型水冷深度學習電腦，要價約 6.9 萬美金。NVIDIA DGX StationNVIDIA 開發者部落格NVIDIA 也將 TESLA V100 供應給 Cisco、DELL、HPE、IBM 與 LENOVO 推出相應的計算產品。同時也和微軟合作，透過將一群 DGX 系列合併，提供完整資料中心級 AI 雲計算服務 HGX-1，NVIDIA 也在 Computex 宣佈將與鴻海、英業達、廣達與緯創合作，為這個年化成長三倍的市場提供 HGX 解決方案。開發板與車，NVIDIA 也在人工智慧需求的第一線提供解決方案熟知 NVIDIA 的讀者一定對於 NVIDIA 推出的某些特殊開發套件並不陌生，例如從 Tegra 1 時期就開始的 Jetson 系列，如今，Jetson 也隨著核心的演進一路從 Jetson TK1 走道 Jetson TX1 或最新的 Jetson TX2，一塊小如信用卡大小，僅需電池即可供電的開發套件，可以容許使用者在終端應用（例如無人機、小型遙控車等）上進行神經網路、影像便是、導航或語音辨識的獨立計算。NVIDIA 與 Audi 合作開發自動駕駛James Huang 攝影NVIDIA 也推出了 Drive PX 系列（目前最新版本為 NVIDIA Drive PX 2），提供車廠整合開發的自動駕駛計算硬體。Drive PX 耗費電量極低，可自動處理高速公路自動駕駛，並透過深度神經網路處理多相機與感應器所收集來的資料，在 HD 地圖上定位規劃安全路線達成自動巡航功能。對車廠來說，NVIDIA 所提供的方案除了硬體還包含稱做 NVIDIA Drivework 的軟體開發套件（SDK），可以幫助車廠快速開發自動駕駛功能。在 2016 年宣佈砸下 10 億美金投資在自動駕駛領域的 TOYOTA，在密西根州 Ann Anbor、加州 Palo Alto 與麻州 Cambridge 等地的 TOYOTA 研究所（TRI, TOYOTA Research Insititue）與測試基地，成立稱為「守護天使（Guardian Angel）」自動駕駛系統的開發計畫，最近也宣佈將採用 NVIDIA Drive PX 系列作為開發選項。NVIDIA 創辦人黃仁勳宣佈 TOYOTA 將選用 NVIDIA DRIVE PX 作為其自動駕駛配件James Huang 攝影NVIDIA 表示，要完成自動駕駛任務，生產整車的車廠事實上有非常多系統需要整合。NVIDIA 所提供的解決方案主要幫助車商直接解決感應器接收資訊、定位與運算的問題，其他各車設定的部分則有賴與車廠共同合作來解決問題。也因此，NVIDIA 目前與 Audi、TOYOTA 等每間遍佈全球的車廠合作模式都不一樣，不是僅提供 Drive PX 的解決方案而已。黃仁勳也提及 NVIDIA Drive PX 的下一階段是可以達到自動駕駛等級 4/5 的 Drive PX Xavier，Xavier 是基於領域特化的微架構的整合產品，整合板上包含有傳統 CPU、Volta GPU與特殊的深度學習加速器（Deep Learning Accelerator, DLA）。NVIDIA 創辦人黃仁勳宣佈將開源 DRIVE PX 下的 Xavier DLA，後圖為 DLA 硬體架構James Huang 攝影黃仁勳直接在 2017 年 GTC 宣佈將在 7 月開源 Xavier DLA ，是一個值得關註的話題。通常由於硬體開發週期通常長於軟體，硬體（尤其是晶片）的開源失敗率很高（回頭看看 Open RISC），NVIDIA 選在此時開源一個特規（整合 ARM、GPU 與 DLA 的方案）但對開發者來說還算不難理解的設計，NVIDIA 自身給出的理由是 NVIDIA 明白硬體不可能由一個公司獨力完成，需要業界更多伙伴同行。是否表示 NVIDIA 已經有了新的合作伙伴，又預備投入多少資源在這個開源專案上，都有賴 NVIDIA 在 7 月公佈更多詳細開源資訊後有更多討論。數說新語NextPediaFLOPS每秒浮點運算次數 處理器在每秒所執行的浮點運算次數（Floating-point operations per second, FLOPS），常被用來估算處理器效能，尤其是在使用到大量浮點運算的計算領域中。大部分的處理器都有浮點運算器，用以執行比整數運算更耗時的浮點運算。通常浮點運算量測就是測量浮點運算器的執行速度。
(來源：
 )
#Computex#Nvidia#人工智慧#GTC分享AmazonGoogle微軟阿裡巴巴人工智慧大數據3 雲佈局！GPU AAS 佈滿全球公有雲，並結合 Docker 推出 GPU Cloud 
 
by
 James Huang
2017.05.31分享NVIDIA 開發者部落格分享 透過 Docker 將 CUDA 與深度學習開發框架整合還不夠，NVIDIA 乾脆自己推出佈署工具，協助開發者快速跨平臺佈署深度學習運算服務。NVIDIA 這次要讓開發者沒理由碰不到GPU運算資源。
推出 CUDA 9 進一步深耕深度學習社群對應著此次 TESLA V100 的推出，NVIDIA 的 GPU 解決方案其實已經因為在架構上引入 Tensor Core 而產生巨幅變化，因應著 Google 自行提出了 TPU 架構，NVIDIA 除了推出引入 Tensor Core 的 Volta 微架構外，也進一步隨著微架構產品更新 CUDA ，提供市場主流的深度學習開發者套件最佳化環境。(在 2017 年 GTC，包含 Caffe 2、Microsoft Cognative Toolkit、mxnet、PyTorch、TensorFlow 與 theano 等主流深度學習開發框架都有教學或分享應用議程)NVIDIA 創辦人黃仁勳解說 NVIDIA 深度學習 Docker 架構James Huang 攝影對 NVIDIA 來說，每一個目前主流發展中的深度學習框架都各有優缺點，也幾乎各有擁護者。作為提供主要運算資源的 NVIDIA 方，希望能夠提供給所有開發者一個良好、方便的環境，讓開發者可以輕鬆做深度學習訓練模型的開發工作。CUDA 9 已經針對深度學習目前的主流開發者套件，如 Caffe 2、 Microsoft Cognative Toolkit、mxnet、PyTorch、TensorFlow 與 theano 在 Volta 架構下，因應引入的 Tensor Core 的最佳化運算做了調整。GPU AAS，要讓 GPU 雲端計算無所不在對於應用人工智慧或深度學習來說有兩個端點，雲端與終端，在硬體篇中，我們已經見識到 NVIDIA 做為一家硬體起家的公司，在兩端之間分別下了多少功夫與努力。為瞭解決人工智慧或深度學習，方便運算應用無所不在，NVIDIA 除了與 CISCO、DELL、HPE、IBM 與 Lenovo 等大廠合作推出相應的 GPU 系統外，也必須同時解決開發框架與運算資源取得的難題。NVIDIA 發表包含深度學習架構的 NV dockerNVIDIA 開發者部落格NVIDIA 因此針對個別開發者提出了針對深度學習運算環境難以移機異地運算的問題，提出了 NV Docker 解決方案。NVIDIA 將深度學習開發框架（如 Caffe 2）與 CUDA Toolkit 和開發者所撰寫的運算程式、資料包裝成完整容器（Container）後，即可在不同運算環境下移轉，理論上可以方便開發者快速移轉運算環境。（編按：此時資料大小與網路速度，其實可能會是移轉順利與否的主要關鍵。）NVIDIA 也與全球主流的公有雲計算廠商合作，在阿裡雲、AWS（Amazon）、GCP（Google）、IBM Bluemix、Microsoft Azure 與百度雲等都建有 GPU 加速的高效能運算服務。除了直接佈署 GPU 配合開發框架來達成深度學習的運算環境外，也可以透過 NV Docker 來做轉移。NVIDIA GPU Cloud 架構NVIDIA 開發者部落格NVIDIA 甚至研發了一套軟體佈署運算工具 NVIDIA GPU Cloud，可以透過介面簡單選擇所需的資料、模型程式、運算框架、運算資源等，透過按鈕就可以完成佈署。預計七月 Beta 公眾測試！#雲端運算#Computex#Nvidia#人工智慧#GTC#Docker#黃仁勳#gpu分享人工智慧機器人合作策略4 NVIDIA創辦人黃仁勳：機器人是終極的AI 
 
by
 翁書婷
2017.05.30分享NVIDIA分享 「人類要如何讓機器人學習？」這是這領域的研發人員遇到的重要大挑戰，因為機器人在學會如何恰當執行一件任務前，就會毀壞所有物品了。
「機器人是終極的AI。」NVIDIA創辦人黃仁勳今日在Computex活動中發表主題演說。NVIDIA創辦人黃仁勳點出現行機器人研發最大的挑戰：如何讓機器人學習卻又不搗亂？（資料照片）NVIDIA提供黃仁勳點出現行機器人研發最大的挑戰是如何讓機器人學習卻又不搗亂？人類要如何讓機器人學習？這是這領域的研發人員遇到的重大挑戰，因為「機器人在學會如何恰當執行一件任務前，就會毀壞所有物品了。」人類可以很容易透過直覺和運動技巧做到很多事，但這些對於機器人來說都是非常複雜難以達成的程式運作。黃仁勳給了以上問題一個答案：在「虛擬世界」裡執行。ISAAC機器人平臺NVIDIA為機器人創造了一個平臺ISAAC。平臺中有一個AI超級電腦Jetson TX2，這個超級電腦專為深度學習和平行運算設計，適合用在無人機與機器人這種，以「電池」供電的自動裝置中，「功能相當於平臺高階PC，但功耗卻和平板電腦一樣。」黃仁勳說。其次為軟體堆疊Astro AV，為機器人和無人機等裝置提供自動駕駛導航功能。再者是開發虛擬實驗室，這個實驗室讓機器人可以在其中學習如何執行任務，直到完全勝任為止，機器在這裡就和真實世界一樣，只是時間過得飛快。富士康與廣達等公司將採用NVIDIA HGX架構開發伺服器另外，黃仁勳也親口宣佈啟動包括富士康、英業達、廣達以及緯創等伺服器廠商的夥伴合作計畫。這些廠商將使用HGX架構開發伺服器。HGX架構是NVIDIA與微軟共同創造一個大規模的加速器名稱。為了因應超大規模雲端環境獨特的高效能、效率以及廣大規模的需求。HGX架構除了能基於作業負載需求靈活進行調整外，還能針對高效能運算、深度學習訓練和推論的需要，搭配各種GPU與CPU組合。在NVIDIA HGX合作夥伴計畫中，將提供ODM代工廠使用NVIDIA HGX架構、NVIDIA GPU 運算技術以及設計準則等資源。延伸閱讀OpenAI發表新演算法，讓機器人像人類般透過模仿學習新技能數字焦點10% 黃仁勳指出，摩爾定律在30年的進展後已經停止，如今就算電晶體持續以每年50%的速度成長，CPU效能僅能成長10%。
數說新語NextPedia圖形處理器GPU（Graphics Processing Unit） 「GPU」又稱顯示核心、視覺處理器、顯示晶片，是一種專門在個人電腦、遊戲機和一些行動裝置上執行繪圖運算工作的微處理器。圖形處理器是Nvidia在1999年首先提出的概念，在此之前，電腦中處理影像輸出的顯示晶片，通常很少被視為是一個獨立的運算單元。圖形處理器使顯示卡減少了對中央處理器（CPU）的依賴，並分擔了部分原本是由中央處理器所擔當的工作，尤其是在進行3D繪圖運算時，功效更加明顯。
(來源：
NVIDIA 、
維基百科 
)
#Computex#機器人#Nvidia#人工智慧#黃仁勳#hgx架構分享AmazonGoogle微軟阿裡巴巴人工智慧機器人5 在人工智慧與深度學習的商業市場上，NVIDIA 目前沒有敵手 
 
by
 James Huang
2017.05.31分享James Huang 攝影分享 創造GPU市場的 NVIDIA，以電競產業成功存活。無心插柳跨足 GPU 計算，卻因贊助比賽而開始全新商業領域。如今的 NVIDIA，正如黃仁勳那第三個剛跨過青春期的孩子，正往人工智慧市場野蠻生長。
讀者要如何理解 NVIDIA 這家公司？過去多數人知道這家公司多半來自於遊戲市場。只要要玩電腦遊戲，電腦多半需要配備一張可以算圖、加速的顯示卡。這張卡的品牌商眾多，但背後技術多半來自 ATi（目前已經被 AMD 合併）、NVIDIA，這是許多人認識 NVIDIA 的開始。NVIDIA 創辦人黃仁勳手持 ASUS ROG 筆電現場演示即時運算效能，黃仁勳笑稱這款電腦比那款稱之為 Pro 的電腦（意指 Mac Book Pro）效能還快 60%。James Huang 攝影遊戲市場不只是硬體重要，軟體平臺與內容的匹配也是關鍵。在過去一段時間裡，大廠此消彼長的不同策略牽動了整個產業的發展。任天堂、SONY與微軟都切入家用電視遊樂器的市場很長一段時間；有段時期甚至有評論家認為電腦遊戲這個名詞將會從市場上消逝。但如今不但看來沒有消逝，甚至電腦連網遊戲跟著整個電競遊戲相關內容領域的發展成為另一種產業趨勢。黃仁勳自己都強調，電競產業是一個仍在發展成長中，產值高達 1000 億美金的市場。如果不是與台灣廠商密切合作，NVIDIA 不可能把 GEFORCE 這個電競生態系創造得如此成功，讓世界上的每個玩家都可以很輕易的接觸到 NVIDIA 與電競的相關產品線。雖然電競是一個龐大的市場，但由於硬體開發週期的特性，硬體廠商永遠比軟體廠商需要找尋下一支成長引擎。一系列無心插柳的作為讓 NVIDIA 看到了未來。成功往往來自不經意，但堅定、積極的投資一個還在讀大學尋求實習的學生 Ian Buck 來到 NVIDIA，並在隨後就讀博士班期間繼續認真研究透過 GPU 協助加速平行運算的可能性。對工程師來說，CPU 每次能夠處理的執行緒是有限的，加速處理的方法之一是加速每一個單位時間內可以進行的執行緒，另一個更好的方法是開出更多處理的執行緒。大部分 CPU 進步的方法都是前者，偶爾開出後者。但 GPU 原本天生的屬性的是可以大量平行處理更多簡單數學運算（乘或加）的執行緒，多數程式設計師需要的是如何自動調配透過平行運算來加速原本所需完成的運算工作。CUDA 發明者，NVIDIA GPU 運算副總 Ian BuckJames Huang 攝影2004 年，讀完博士的 Ian Buck 最後落腳 NVIDIA，並最終將這個自動調配 GPU 做平行演算的函式庫 CUDA 在 2007 年開發出來。那時候使用 CUDA 來應用 GPU 根本不是整個產業的主流，甚至多數人都不知道可以透過調用 GPU 來加速自己需要完成的運算工作。CUDA 最終根本性地改變了 NVIDIA，在推出 CUDA 時想法是可以透過 GPU 加速科學需求的運算（GPUPU），卻因為支持 ImageNet 的比賽結果，讓加拿大多倫多大學的小子替 NVIDIA 上了一課。原來 GPUPU 的演算，可以如此加速機器學習與人工智慧中最重要的演算法運行，這個概念值不止千金，根本性地啟發了 NVIDIA 大力將 GPU 運算朝向人工智慧應用方面轉型。也才會有大力投資 30 億美金，重新開發 GPU 核心設計，將 Tensor Core 的概念設計引入 GPU 這樣特別的想法付諸實現。佈局完整，過去的經驗全都是今日成功的基礎NVidia 早已不是過去的小公司，從 2006 年全球約 2000 人，擴張至今日超過萬人，擴張的絕大多數都是軟體或程式工程師（這也是為什麼 NVIDIA 需要蓋新總部的真正原因）。如果認真觀察 NVIDIA 如今網頁所列出的產品線，你會發現已經有超過一半以上的產品其實是以軟體的形式出現，但這些軟體多半都連動著自己所帶有的硬體發展。對 NVIDIA 來說，不斷地投資在這些軟體身上，方便開發者或使用者很容易調用自己的產品，都將是使用者最終決定採用 NVIDIA 的關鍵。NVIDIA 創辦人黃仁勳展示 Volta 微架構晶片所成的 TESLA V100James Huang 攝影過去許多被市場認為的不成功，例如推出 NVIDIA 自己的主機板 mforce，卻都是今日成功的基礎，如果沒有過去做過這些硬體、驅動程式與各種需求的經驗，NVIDIA沒有辦法快速推出符合今日車用、城市應用等市場需求的合適產品。如今，NVIDIA 推出許多小型產品，如 Jetson 開發板系列，容許開發者在終端自行透過 SDK 設計對應的應用。當然，相較於更多免費、便宜、開源的開發板來說，Jetson 系列仍然昂貴，但 NVIDIA 正透過這些費用與特殊設計的機制（例如學生補助），篩選出合理、具有商業潛力的應用，並直接以公司資源投註在這些應用的合作開發上。NVIDIA Project Holodeck 展示 Gogoro 相關產品James Huang 攝影傳統上，NVIDIA 在算圖加速上的應用仍有演進，許多應用都與 VR、AR 有關，這些應用也將逐漸影響到未來產業。黃仁勳在 2017 GTC 與 Computex 都仍有一個小篇章在介紹 NVIDIA 在 VR 上的進展，著重在 VR 虛擬實境的工作環境演化上。這個稱之為 Holodeck 的專案，是一個以真實影像模型為基礎的互動共同工作實境，特別適用在設計類的專案情境上。在 Computex 2017 現場，黃仁勳甚至邀請了 Gogoro 的創辦人陸學森一起上臺感受多人遠端實際體驗 Holodeck 中的 Gogoro 一代與 Go Station 的設計。Gogoro 創辦人陸學森與 NVIDIA 創辦人黃仁勳於 Computex 2017 上演說James Huang 攝影相對 Apple、微軟、IBM 等不同廠商的發表會。NVIDIA 黃仁勳在 GTC 與 Computex 的演說都更像是一場個人秀（編按：無怪乎現在人稱人工智慧教父），但圍繞著人工智慧作為主題核心所要傳達的概念是很清晰明確的。NVIDIA 將挾現有 GPU 開發、平行運算作為根基，並可以 CUDA 作為輔助協助各家深度學習框架優化（並非僅 Google 的 TensorFlow、微軟的 Cognative Toolkit 自家的深度學習框架），除了讓需要深度學習或人工智慧計算的企業或組織能建構有足額的硬體資源，也讓所有希望接觸深度學習運算資源的人或組織可以透過雲計算等方式，快速學習、部署自己的深度學習運算環境。目標要讓 GPU 運算環境容易建置、轉移、無所不在。開源架構，透過生態系建構讓 NVIDIA 更強壯在大數據口號喊了許多年後，許多產業內公司都逐漸真的開始累積起穩定、龐大的數據量，除了已知、常見符合常理的推論，配合簡單統計與預測模型，可以協助公司找出龐大資料中所隱含的資訊外，資料科學家都在努力尋找出那些未知，但可以替公司自動化的資訊，能夠讓企業最終省下大把成本，自動化更多工作。最靠近 NVIDIA 應用與商業模式的 Google 率先跳出自己打造晶片 TPU，縱有 Alpha GO 的成功在前，加上 Google Translate 的應用在後證明 TPU 專案在內部具有足夠價值，但在雲端運算的市場上，是否能競逐商業效益仍未可知。但 NVIDIA 目前在軟、硬體、雲計算、端計算應用領域佈局完整，並相對得到生態系伙伴的信任。NVIDIA 創辦人黃仁勳宣佈將開源 DRIVE PX 下的 Xavier DLA，後圖為 DLA 硬體架構James Huang 攝影連曾被 Linux 之父 Linus Torvalds 調侃，對開放社群相對不友善的 NVIDIA，都甚至準備要透過開源硬體架構 DLA 來建造更廣大的生態系。加上幾乎所有車廠、科技廠在矽谷都以前所未有的砸錢速度投資在自動駕駛領域內，不難看到整個產業發展正在非常快速的前進，透過開源架構可能是能夠包容整個產業、加速產業運作的最好選項。仔細看看本屆 GTC 的贊助商們（AWS、IBM、CISCO、DELL & EMC、HPE、SUPERMICRO、inspur 浪潮與新聚思科技），就不難理解哪些企業將圍繞在接下來人工智慧、深度學習等需要資料、需要演算、能夠提供運算能量、提供運算框架、協助人才應用人工智慧達成前述應用的周遭趨勢。James Huang 攝影許多人都看好 NVIDIA 未來數年在人工智慧、深度學習上的進展。矽谷有許多華人圈的專屬討論群，在黃仁勳於 GTC 演講後股票大漲之際，議論紛紛 NVIDIA 的未來。同一時間微軟也在西雅圖舉辦自己年度的重要開發者會議 Build，但顯然得到的關註度不如 GTC。許多討論群人士紛紛表示，看起來這一仗黃仁勳打得非常漂亮。對 NVIDIA 來說，從原本的遊戲、電競領域舒適圈跳開本不是容易的事。54 歲的黃仁勳正值壯年，兩個孩子都大了以後，顯然把拼勁全力衝刺在事業上。看來 NVIDIA 正像他第三個剛過了青春期的孩子，正往人工智慧與深度學習的商業市場上野蠻生長。#Computex#Nvidia#人工智慧#GTC#深度學習分享


深度學習 - 維基百科，自由的百科全書































 







深度學習

維基百科，自由的百科全書


					前往：					導覽，					搜尋








機器學習與資料探勘





問題





分類
聚類
回歸
異常檢測
關聯規則
強化學習
結構預測（英語：Structured prediction）
特徵學習
線上學習（英語：Online machine learning）
半監督學習（英語：Semi-supervised learning）
語法歸納（英語：Grammar induction）






監督學習
(分類 · 回歸)






決策樹
表徵（裝袋, 提升，隨機森林）
k-NN
線性回歸
樸素貝葉斯
神經網路
邏輯回歸
感知器
支援向量機（SVM）
相關向量機（RVM）





聚類





BIRCH（英語：BIRCH）
層次（英語：Hierarchical clustering）
k平均
期望最大化（EM）

DBSCAN
OPTICS（英語：OPTICS）
均值飄移（英語：Mean shift）





降維





因子分析（英語：Factor analysis）
CCA
ICA
LDA
NMF（英語：Non-negative matrix factorization）
PCA
LASSO
t-SNE（英語：t-distributed stochastic neighbor embedding）





結構預測（英語：Structured prediction）





機率圖模型（貝葉斯網路，CRF, HMM）





異常檢測





k-NN
局部離群因子（英語：Local outlier factor）





神經網路





自編碼（英語：Autoencoder）
深度學習
多層感知機
RNN
受限玻爾茲曼機
SOM
CNN





理論





偏差/方差困境（英語：Bias–variance tradeoff）
計算學習理論（英語：Computational learning theory）
經驗風險最小化（英語：Empirical risk minimization）
PAC學習（英語：Probably approximately correct learning）
統計學習
VC理論








閱
論
編





深度學習（英語：deep learning）是機器學習拉出的分支，它試圖使用包含複雜結構或由多重非線性變換構成的多個處理層對資料進行高層抽象的演算法。[1][2][3][4][5]
深度學習是機器學習中一種基於對資料進行表徵學習的方法。觀測值（例如一幅圖像）可以使用多種方式來表示，如每個像素強度值的向量，或者更抽象地表示成一系列邊、特定形狀的區域等。而使用某些特定的表示方法更容易從例項中學習任務（例如，人臉識別或面部表情識別[6]）。深度學習的好處是用非監督式或半監督式（英語：Semi-supervised learning）的特徵學習和分層特徵提取高效演算法來替代手工取得特徵（英語：Feature (machine learning)）。[7]
表徵學習的目標是尋求更好的表示方法並建立更好的模型來從大規模未標記資料中學習這些表示方法。表達方式類似神經科學的進步，並鬆散地建立在類似神經系統中的資訊處理和通訊模式的理解上，如神經編碼，試圖定義拉動神經元的反應之間的關係以及大腦中的神經元的電活動之間的關係。[8]
至今已有數種深度學習框架，如深度神經網路、捲積神經網路和深度置信網路（英語：Deep belief network）和遞迴神經網路已被應用電腦視覺、語音識別、自然語言處理、音訊識別與生物資訊學等領域並取得了極好的效果。
另外，「深度學習」已成為類似術語，或者說是神經網路的品牌重塑。[9][10]



目錄


1 簡介
2 基本概念
3 人工神經網路下的深度學習
4 深度學習結構

4.1 深度神經網路
4.2 深度神經網路的問題
4.3 深度置信網路
4.4 捲積神經網路
4.5 捲積深度置信網路
4.6 結果

4.6.1 語音識別
4.6.2 圖像分類




5 深度學習與神經科學
6 公眾視野中的深度學習
7 批評
8 參見
9 參考資料
10 外部連結



簡介[編輯]
深度學習框架，尤其是基於人工神經網路的框架可以追溯到1980年福島邦彥提出的新認知機[11]，而人工神經網路的歷史更為久遠。1989年，揚·勒丘恩（Yann LeCun）等人開始將1974年提出的標準反向傳播演算法[12]應用於深度神經網路，這一網路被用於手寫郵政編碼識別。儘管演算法可以成功執行，但計算代價非常巨大，神經網路的訓練時間達到了3天，因而無法投入實際使用[13]。許多因素導致了這一緩慢的訓練過程，其中一種是由於爾根·施密德胡伯（英語：Jürgen Schmidhuber）的學生賽普·霍克賴特（Sepp Hochreiter（英語：Sepp Hochreiter））於1991年提出的梯度消失問題[14][15]。
最早的進行一般自然雜亂圖像中自然物體識別的深度學習網路是翁巨揚（Juyang Weng）等在1991和1992發表的生長網（Cresceptron）[16][17][18]。它也是第一個提出了後來很多實驗廣泛採用的一個方法：現在稱為最大匯集（max-pooling)以用於處理大物體的變形等問題。生長網不僅直接從雜亂自然場景中學習老師指定的一般物體，還用網路反向分析的方法把圖像內被識別了的物體從背景圖像中分割出來。
2007年前後，傑弗里·辛頓和魯斯蘭·薩拉赫丁諾夫（Ruslan Salakhutdinov）提出了一種在前饋神經網路中進行有效訓練的演算法。這一演算法將網路中的每一層視為無監督的受限玻爾茲曼機，再使用有監督的反向傳播演算法進行調優[19]。在此之前的1992年，在更為普遍的情形下，施密德胡伯也曾在遞迴神經網路上提出一種類似的訓練方法，並在實驗中證明這一訓練方法能夠有效提高有監督學習的執行速度[20][21].
自深度學習出現以來，它已成為很多領域，尤其是在電腦視覺和語音識別中，成為各種領先系統的一部分。在通用的用於檢驗的資料集，例如語音識別中的TIMIT和圖像識別中的ImageNet, Cifar10上的實驗證明，深度學習能夠提高識別的精度。與此同時，神經網路也受到了其他更加簡單歸類模型的挑戰，支援向量機等模型在20世紀90年代到21世紀初成為過流行的機器學習演算法。
硬體的進步也是深度學習重新獲得關註的重要因素。高效能圖形處理器的出現極大地提高了數值和矩陣運算的速度，使得機器學習演算法的執行時間得到了顯著的縮短[22][23]。
深度學習網路在2001年後正逐漸被更有潛力的基於腦模型的網路[24][25]所替代。腦科學的大量研究已表明人腦網路不是一個級聯的結構，大概是為了腦計算的必要吧。
基本概念[編輯]
深度學習的基礎是機器學習中的分散表示（distributed representation）。分散表示假定觀測值是由不同因子相互作用生成。在此基礎上，深度學習進一步假定這一相互作用的過程可分為多個層次，代表對觀測值的多層抽象。不同的層數和層的規模可用於不同程度的抽象[3]。
深度學習運用了這分層次抽象的思想，更高層次的概念從低層次的概念學習得到。這一分層結構常常使用貪婪演算法逐層構建而成，並從中選取有助於機器學習的更有效的特徵[3].
不少深度學習演算法都以無監督學習的形式出現，因而這些演算法能被應用於其他演算法無法企及的無標籤資料，這一類資料比有標籤資料更豐富，也更容易獲得。這一點也為深度學習贏得了重要的優勢[3]。
人工神經網路下的深度學習[編輯]
一部分最成功的深度學習方法涉及到對人工神經網路的運用。人工神經網路受到了1959年由諾貝爾獎得主大衛·休伯爾（David H. Hubel）和托斯坦·威澤爾（Torsten Wiesel）提出的理論啟發。休伯爾和威澤爾發現，在大腦的初級視覺皮層中存在兩種細胞：簡單細胞和複雜細胞，這兩種細胞承擔不同層次的視覺感知功能。受此啟發，許多神經網路模型也被設計為不同節點之間的分層模型[26]。
福島邦彥提出的新認知機引入了使用無監督學習訓練的捲積神經網路。燕樂存將有監督的反向傳播演算法應用於這一架構[27]。事實上，從反向傳播演算法自20世紀70年代提出以來，不少研究者都曾試圖將其應用於訓練有監督的深度神經網路，但最初的嘗試大都失敗。賽普·霍克賴特（英語：Sepp Hochreiter）在其博士論文中將失敗的原因歸結為梯度消失，這一現象同時在深度前饋神經網路和遞迴神經網路中出現，後者的訓練過程類似深度網路。在分層訓練的過程中，本應用於修正模型參數的誤差隨著層數的增加指數遞減，這導致了模型訓練的效率低下[28][29]。
為瞭解決這一問題，研究者們提出了一些不同的方法。於爾根·施密德胡伯（英語：Jürgen Schmidhuber）於1992年提出多層級網路，利用無監督學習訓練深度神經網路的每一層，再使用反向傳播演算法進行調優。在這一模型中，神經網路中的每一層都代表觀測變數的一種壓縮表示，這一表示也被傳遞到下一層網路[20]。
另一種方法是賽普·霍克賴特和於爾根·施密德胡伯提出的長短期記憶神經網路（英語：long short term memory），LSTM）[30]。2009年，在ICDAR 2009舉辦的連筆手寫識別競賽中，在沒有任何先驗知識的情況下，深度多維長短期記憶神經網路取得了其中三場比賽的勝利[31][32]。
斯文·貝克提出了在訓練時只依賴梯度符號的神經抽象金字塔模型，用以解決圖像重建和人臉定位的問題[33]。
其他方法同樣採用了無監督預訓練來構建神經網路，用以發現有效的特徵，此後再採用有監督的反向傳播以區分有標籤資料。辛頓等人於2006年提出的深度模型提出了使用多層隱變數學習高層表示的方法。這一方法使用斯摩棱斯基於1986年提出的受限玻爾茲曼機[34]對每一個包含高層特徵的層進行建模。模型保證了資料的對數似然下界隨著層數的提升而遞增。當足夠多的層數被學習完畢，這一深層結構成為一個生成模型，可以通過自上而下的採樣重構整個資料集[35]。辛頓聲稱這一模型在高維結構化資料上能夠有效地提取特徵[36]。
吳恩達和傑夫·迪恩領導的Google大腦團隊建立了一個僅通過YouTube影片學習高層概念（例如貓）的神經網路[37] [38]。
其他方法依賴了現代電子電腦的強大計算能力，尤其是GPU。2010年，在於爾根·施密德胡伯位於瑞士人工智慧實驗室IDSIA的研究組中，丹·奇雷尚（Dan Ciresan）和他的同事展示了利用GPU直接執行反向傳播演算法而忽視梯度消失問題的存在。這一方法在燕樂存等人給出的手寫識別MNIST資料集上戰勝了已有的其他方法[22]。
截止2011年，前饋神經網路深度學習中最新的方法是交替使用捲積層（convolutional layers）和最大值池化層（max-pooling layers）並加入單純的分類層作為頂端。訓練過程也無需引入無監督的預訓練[39][40]。從2011年起，這一方法的GPU實現[39]多次贏得了各類模式識別競賽的勝利，包括IJCNN 2011交通標誌識別競賽[41]和其他比賽。
這些深度學習演算法也是最先在某些識別任務上達到和人類表現具備同等競爭力的演算法[42]。
深度學習結構[編輯]
深度神經網路是一種具備至少一個隱層的神經網路。與淺層神經網路類似，深度神經網路也能夠為複雜非線性系統提供建模，但多出的層次為模型提供了更高的抽象層次，因而提高了模型的能力。深度神經網路通常都是前饋神經網路，但也有語言建模等方面的研究將其拓展到遞迴神經網路[43]。捲積深度神經網路（Convolutional Neuron Networks, CNN）在電腦視覺領域得到了成功的應用[44]。此後，捲積神經網路也作為聽覺模型被使用在自動語音識別領域，較以往的方法獲得了更優的結果[45]。
深度神經網路[編輯]
深度神經網路（Deep Neural Networks, DNN）是一種判別模型，可以使用反向傳播演算法進行訓練。權重更新可以使用下式進行隨機梯度下降法（英語：Stochastic gradient descent）求解：





Δ

w

i
j


(
t
+
1
)
=
Δ

w

i
j


(
t
)
+
η



∂
C


∂

w

i
j







{\displaystyle \Delta w_{ij}(t+1)=\Delta w_{ij}(t)+\eta {\frac {\partial C}{\partial w_{ij}}}}



其中，



η


{\displaystyle \eta }

為學習率，



C


{\displaystyle C}

為代價函式。這一函式的選擇與學習的類型（例如監督學習、無監督學習、增強學習）以及活化函數相關。例如，為了在一個多分類問題上進行監督學習，通常的選擇是使用ReLU作為活化函數，而使用交叉熵作為代價函式。Softmax函式定義為




p

j


=



exp
⁡
(

x

j


)



∑

k


exp
⁡
(

x

k


)





{\displaystyle p_{j}={\frac {\exp(x_{j})}{\sum _{k}\exp(x_{k})}}}

，其中




p

j




{\displaystyle p_{j}}

代表類別



j


{\displaystyle j}

的機率，而




x

j




{\displaystyle x_{j}}

和




x

k




{\displaystyle x_{k}}

分別代表對單元



j


{\displaystyle j}

和



k


{\displaystyle k}

的輸入。交叉熵定義為



C
=
−

∑

j



d

j


log
⁡
(

p

j


)


{\displaystyle C=-\sum _{j}d_{j}\log(p_{j})}

，其中




d

j




{\displaystyle d_{j}}

代表輸出單元



j


{\displaystyle j}

的目標機率，




p

j




{\displaystyle p_{j}}

代表應用了活化函數後對單元



j


{\displaystyle j}

的機率輸出[46]。
深度神經網路的問題[編輯]
與其他神經網路模型類似，如果僅僅是簡單地訓練，深度神經網路可能會存在很多問題。常見的兩類問題是過擬合和過長的運算時間。
深度神經網路很容易產生過擬合現象，因為增加的抽象層使得模型能夠對訓練資料中較為罕見的依賴關係進行建模。對此，權重遞減（




ℓ

2




{\displaystyle \ell _{2}}

正規化）或者稀疏（




ℓ

1




{\displaystyle \ell _{1}}

-正規化）等方法可以利用在訓練過程中以減小過擬合現象[47]。另一種較晚用於深度神經網路訓練的正規化方法是丟棄法（"dropout" regularization），即在訓練中隨機丟棄一部分隱層單元來避免對較為罕見的依賴進行建模[48]。
反向傳播演算法和梯度下降法由於其實現簡單，與其他方法相比能夠收斂到更好的局部最優值而成為神經網路訓練的通行方法。但是，這些方法的計算代價很高，尤其是在訓練深度神經網路時，因為深度神經網路的規模（即層數和每層的節點數）、學習率、初始權重等眾多參數都需要考慮。掃描所有參數由於時間代價的原因並不可行，因而小批次訓練（mini-batching），即將多個訓練樣本組合進行訓練而不是每次只使用一個樣本進行訓練，被用於加速模型訓練[49]。而最顯著地速度提升來自GPU，因為矩陣和向量計算非常適合使用GPU實現。但使用大規模集群進行深度神經網路訓練仍然存在困難，因而深度神經網路在訓練並列化方面仍有提升的空間。
深度置信網路[編輯]




一個包含完全連線可見層和隱層的受限玻爾茲曼機（RBM）。註意到可見層單元和隱層單元內部彼此不相連。


深度置信網路（deep belief networks，DBN）是一種包含多層隱單元的機率生成模型，可被視為多層簡單學習模型組合而成的複合模型[50]。
深度致信網路可以作為深度神經網路的預訓練部分，並為網路提供初始權重，再使用反向傳播或者其他判定演算法作為調優的手段。這在訓練資料較為缺乏時很有價值，因為不恰當的初始化權重會顯著影響最終模型的效能，而預訓練獲得的權重在權值空間中比隨機權重更接近最優的權重。這不僅提升了模型的效能，也加快了調優階段的收斂速度[51]。
深度置信網路中的每一層都是典型的受限玻爾茲曼機（restricted Boltzmann machine，RBM），可以使用高效的無監督逐層訓練方法進行訓練。受限玻爾茲曼機是一種無向的基於能量的生成模型，包含一個輸入層和一個隱層。圖中對的邊僅在輸入層和隱層之間存在，而輸入層節點內部和隱層節點內部則不存在邊。單層RBM的訓練方法最初由傑弗里·辛頓在訓練「專家乘積」中提出，被稱為對比分歧（contrast divergence, CD）。對比分歧提供了一種對最大似然的近似，被理想地用於學習受限玻爾茲曼機的權重[49]。當單層RBM被訓練完畢後，另一層RBM可被堆疊在已經訓練完成的RBM上，形成一個多層模型。每次堆疊時，原有的多層網路輸入層被初始化為訓練樣本，權重為先前訓練得到的權重，該網路的輸出作為新增RBM的輸入，新的RBM重複先前的單層訓練過程，整個過程可以持續進行，直到達到某個期望中的終止條件[2]。
儘管對比分歧對最大似然的近似十分粗略（對比分歧並不在任何函式的梯度方向上），但經驗結果證實該方法是訓練深度結構的一種有效的方法[49]。
捲積神經網路[編輯]
主條目：捲積神經網路
捲積神經網路（convolutional neuron networks，CNN）由一個或多個捲積層和頂端的全連通層（對應經典的神經網路）組成，同時也包括關聯權重和池化層（pooling layer）。這一結構使得捲積神經網路能夠利用輸入資料的二維結構。與其他深度學習結構相比，捲積神經網路在圖像和語音識別方面能夠給出更優的結果。這一模型也可以使用反向傳播演算法進行訓練。相比較其他深度、前饋神經網路，捲積神經網路需要估計的參數更少，使之成為一種頗具吸引力的深度學習結構[52]。
捲積深度置信網路[編輯]
捲積深度置信網路（convolutional deep belief networks，CDBN）是深度學習領域較新的分支。在結構上，捲積深度置信網路與捲積神經網路在結構上相似。因此，與捲積神經網路類似，捲積深度置信網路也具備利用圖像二維結構的能力，與此同時，捲積深度信念網路也擁有深度置信網路的預訓練優勢。捲積深度置信網路提供了一種能被用於訊號和圖像處理任務的通用結構，也能夠使用類似深度置信網路的訓練方法進行訓練[53]。
結果[編輯]
語音識別[編輯]
下表中的結果展示了深度學習在通行的TIMIT資料集上的結果。TIMIT包含630人的語音資料，這些人持八種常見的美式英語口音，每人閱讀10句話。這一資料在深度學習發展之初常被用於驗證深度學習結構[54]。TIMIT資料集較小，使得研究者可以在其上實驗不同的模型配置。


方法
聲音誤差率 (PER, %)


隨機初始化RNN
26.1


貝葉斯三音子GMM-HMM
25.6


單音子重複初始化DNN
23.4


單音子DBN-DNN
22.4


帶BMMI訓練的三音子GMM-HMM
21.7


共享池上的單音子DBN-DNN
20.7


捲積DNN
20.0


圖像分類[編輯]
圖像分類領域中一個公認的評判資料集是MNIST資料集。MNIST由手寫阿拉伯數字組成，包含60,000個訓練樣本和10,000個測試樣本。與TIMIT類似，它的資料規模較小，因而能夠很容易地在不同的模型配置下測試。Yann LeCun的網站給出了多種方法得到的實驗結果[55]。截至2012年，最好的判別結果由Ciresan等人在當年給出，這一結果的錯誤率達到了0.23%[56]。
深度學習與神經科學[編輯]
電腦領域中的深度學習與20世紀90年代由認知神經科學研究者提出的大腦發育理論（尤其是皮層發育理論）密切相關[57]。對這一理論最容易理解的是傑弗里·艾爾曼（英語：Jeffrey Elman）於1996年出版的專著《對天賦的再思考》（Rethinking Innateness（英語：Rethinking Innateness））[58]（參見斯拉格和詹森[59]以及奎茲和賽傑諾維斯基[60]的表述）。由於這些理論給出了實際的神經計算模型，因而它們是純計算驅動的深度學習模型的技術先驅。這些理論指出，大腦中的神經元組成了不同的層次，這些層次相互連線，形成一個過濾體系。在這些層次中，每層神經元在其所處的環境中取得一部分資訊，經過處理後向更深的層級傳遞。這與後來的單純與計算相關的深度神經網路模型相似。這一過程的結果是一個與環境相協調的自組織的堆疊式的轉換器。正如1995年在《紐約時報》上刊登的那樣，「……嬰兒的大腦似乎受到所謂『營養因素』的影響而進行著自我組織……大腦的不同區域依次相連，不同層次的腦組織依照一定的先後順序發育成熟，直至整個大腦發育成熟。」[61]
深度結構在人類認知演化和發展中的重要性也在認知神經學家的關註之中。發育時間的改變被認為是人類和其他靈長類動物之間智力發展差異的一個方面[62]。在靈長類中，人類的大腦在出生後的很長時間都具備可塑性，但其他靈長類動物的大腦則在出生時就幾乎完全定型。因而，人類在大腦發育最具可塑性的階段能夠接觸到更加複雜的外部場景，這可能幫助人類的大腦進行調節以適應快速變化的環境，而不是像其他動物的大腦那樣更多地受到遺傳結構的限制。這樣的發育時間差異也在大腦皮層的發育時間和大腦早期自組織中從刺激環境中取得資訊的改變得到體現。當然，伴隨著這一可塑性的是更長的兒童期，在此期間人需要依靠撫養者和社會群體的支援和訓練。因而這一理論也揭示了人類演化中文化和意識共同進化的現象[63]。
公眾視野中的深度學習[編輯]
深度學習常常被看作是通向真正人工智慧的重要一步[64]，因而許多機構對深度學習的實際應用抱有濃厚的興趣。2013年12月，Facebook宣佈雇用燕樂存為其新建的人工智慧實驗室的主管，這一實驗室將在加州、倫敦和紐約設立分支機構，幫助Facebook研究利用深度學習演算法進行類似自動標記相片中用戶姓名這樣的任務[65]。
2013年3月，傑弗里·辛頓和他的兩位研究生亞歷克斯·克裡澤夫斯基和伊利婭·蘇特斯科娃被Google公司雇用，以提升現有的機器學習產品並協助處理Google日益增長的資料。Google同時併購了辛頓創辦的公司DNNresearch[66]。
2016年3月，以深度學習開發的圍棋程式AlphaGo首度在比賽中擊敗人類頂尖對手，造成廣泛的討論。
批評[編輯]
對深度學習的主要批評是許多方法缺乏理論支撐。大多數深度結構僅僅是梯度下降的某些變式。儘管梯度下降已經被充分地研究，但理論涉及的其他演算法，例如對比分歧演算法，並沒有獲得充分的研究，其收斂性等問題仍不明確。深度學習方法常常被視為黑盒，大多數的結論確認都由經驗而非理論來確定。
也有學者認為，深度學習應當被視為通向真正人工智慧的一條途徑，而不是一種包羅萬象的解決方案。儘管深度學習的能力很強，但和真正的人工智慧相比，仍然缺乏諸多重要的能力。理論心理學家加里·馬庫斯（英語：Gary Marcus）指出：

就現實而言，深度學習只是建造智慧型機器這一更大挑戰中的一部分。這些技術缺乏表達因果關係的手段……缺乏進行邏輯推理的方法，而且遠沒有具備整合抽象知識，例如物品屬性、代表和典型用途的資訊。最為強大的人工智慧系統，例如IBM的人工智慧系統華生，僅僅把深度學習作為一個包含從貝葉斯推理和演繹推理等技術的複雜技術集合中的組成部分[67]。

參見[編輯]

圖模型
人工智慧的應用
吳恩達
人工智慧專案列表

深度學習庫

Torch（英語：Torch (machine learning)）
Theano（英語：Theano (software)）
Deeplearning4j
tensorflow
Caffe
Keras（英語：Keras）
Mxnet

參考資料[編輯]


^ Deng, L.; Yu, D. Deep Learning: Methods and Applications (PDF). Foundations and Trends in Signal Processing. 2014, 7: 3–4. 
^ 2.0 2.1 Bengio, Yoshua. Learning Deep Architectures for AI (PDF). Foundations and Trends in Machine Learning. 2009, 2 (1): 1–127. 
^ 3.0 3.1 3.2 3.3 Bengio, Y.; Courville, A.; Vincent, P. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2013, 35 (8): 1798–1828. arXiv:1206.5538. 
^ Schmidhuber, J. Deep Learning in Neural Networks: An Overview. Neural Networks. 2015, 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. 
^ Bengio, Yoshua; LeCun, Yann; Hinton, Geoffrey. Deep Learning. Nature. 2015, 521: 436–444. 
^ Glauner, P. Deep Convolutional Neural Networks for Smile Recognition (MSc Thesis). Imperial College London, Department of Computing. 2015. arXiv:1508.06535. 
^ Song, H.A.; Lee, S. Y. Hierarchical Representation Using NMF. Neural Information Processing. Lectures Notes in Computer Sciences 8226. Springer Berlin Heidelberg. 2013: 466–473. ISBN 978-3-642-42053-5. doi:10.1007/978-3-642-42054-2_58. 
^ Olshausen, B. A. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature. 1996, 381 (6583): 607–609. 
^ Collobert, R. Deep Learning for Efficient Discriminative Parsing. VideoLectures.net. April 2011. 事件發生在 7min 45s. 
^ Gomes, L. Machine-Learning Maestro Michael Jordan on the Delusions of Big Data and Other Huge Engineering Efforts. IEEE Spectrum. 20 October 2014. 
^ K. Fukushima., "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position," Biol. Cybern., 36, 193–202, 1980
^ P. Werbos., "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences," PhD thesis, Harvard University, 1974.
^ LeCun et al., "Backpropagation Applied to Handwritten Zip Code Recognition," Neural Computation, 1, pp. 541–551, 1989.
^ S. Hochreiter., "Untersuchungen zu dynamischen neuronalen Netzen," Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991.
^ S. Hochreiter et al., "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies," In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.
^ J. Weng, N. Ahuja and T. S. Huang, "Cresceptron: a self-organizing neural network which grows adaptively," Proc. International Joint Conference on Neural Networks, Baltimore, Maryland, vol I, pp. 576-581, June, 1992.
^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation of 3-D objects from 2-D images," Proc. 4th International Conf. Computer Vision, Berlin, Germany, pp. 121-128, May, 1993.
^ J. Weng, N. Ahuja and T. S. Huang, "Learning recognition and segmentation using the Cresceptron," International Journal of Computer Vision, vol. 25, no. 2, pp. 105-139, Nov. 1997.
^ G. E. Hinton., "Learning multiple layers of representation," Trends in Cognitive Sciences, 11, pp. 428–434, 2007.
^ 20.0 20.1 J. Schmidhuber., "Learning complex, extended sequences using the principle of history compression," Neural Computation, 4, pp. 234–242, 1992.
^ J. Schmidhuber., "My First Deep Learning System of 1991 + Deep Learning Timeline 1962–2013."
^ 22.0 22.1 D. C. Ciresan et al., "Deep Big Simple Neural Nets for Handwritten Digit Recognition," Neural Computation, 22, pp. 3207–3220, 2010.
^ R. Raina, A. Madhavan, A. Ng., "Large-scale Deep Unsupervised Learning using Graphics Processors," Proc. 26th Int. Conf. on Machine Learning, 2009.
^ J. Weng, J. McClelland, A. Pentland, O. Sporns, I. Stockman, M. Sur and E. Thelen, "Autonomous Mental Development by Robots and Animals," Science, vol. 291, no. 5504, pp. 599 - 600, Jan. 26, 2001.
^ J. Weng, "Brains as Naturally Emerging Turing Machines," in Proc. International Joint Conference on Neural Networks, Killarney, Ireland, 8 pages, July 12-17. 2015.
^ M Riesenhuber, T Poggio. Hierarchical models of object recognition in cortex. Nature neuroscience, 1999(11) 1019–1025.
^ Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1(4):541–551, 1989.
^ S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f. Informatik, Technische Univ. Munich, 1991. Advisor: J. Schmidhuber
^ S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001.
^ Hochreiter, Sepp; and Schmidhuber, Jürgen; Long Short-Term Memory, Neural Computation, 9(8):1735–1780, 1997
^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552
^ A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.
^ Sven Behnke. Hierarchical Neural Networks for Image Interpretation. (PDF). Lecture Notes in Computer Science 2766. Springer. 2003. 
^ Smolensky, P. Information processing in dynamical systems: Foundations of harmony theory.. In D. E. Rumelhart, J. L. McClelland, & the PDP Research Group, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. 1. 1986: 194–281. 
^ Hinton, G. E.; Osindero, S.; Teh, Y. A fast learning algorithm for deep belief nets (PDF). Neural Computation. 2006, 18 (7): 1527–1554. PMID 16764513. doi:10.1162/neco.2006.18.7.1527. 
^ Hinton, G. Deep belief networks. Scholarpedia. 2009, 4 (5): 5947. doi:10.4249/scholarpedia.5947.  編輯
^ John Markoff. How Many Computers to Identify a Cat? 16,000.. New York Times. 25 June 2012. 
^ Ng, Andrew; Dean, Jeff. Building High-level Features Using Large Scale Unsupervised Learning (PDF). 2012. 
^ 39.0 39.1 D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona), 2011.
^ Martines, H., Bengio, Y., & Yannakakis, G. N. (2013). Learning Deep Physiological Models of Affect. I EEE Computational Intelligence, 8(2), 20.
^ D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.
^ D. C. Ciresan, U. Meier, J. Schmidhuber. Multi-column Deep Neural Networks for Image Classification. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012.
^ T. Mikolov et al., "Recurrent neural network based language model," Interspeech, 2010.
^ Y. LeCun et al., "Gradient-based learning applied to document recognition," Proceedings of the IEEE, 86 (11), pp. 2278–2324.
^ T. Sainath et al., "Convolutional neural networks for LVCSR," ICASSP, 2013.
^ G. E. Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The shared views of four research groups," IEEE Signal Processing Magazine, pp. 82–97, November 2012.
^ Y. Bengio et al., "Advances in optimizing recurrent networks," ICASSP', 2013.
^ G. Dahl et al., "Improving DNNs for LVCSR using rectified linear units and dropout," ICASSP', 2013.
^ 49.0 49.1 49.2 G. E. Hinton., "A Practical Guide to Training Restricted Boltzmann Machines," Tech. Rep. UTML TR 2010-003, Dept. CS., Univ. of Toronto, 2010.
^ G.E. Hinton., "Deep belief networks," Scholarpedia, 4(5):5947.
^ H. Larochelle et al., "An empirical evaluation of deep architectures on problems with many factors of variation," in Proc. 24th Int. Conf. Machine Learning, pp. 473–480, 2007.
^ Convolutional Neural Network. [2014-09-16]. 
^ Honglak Lee; Roger Grosse; Rajesh Ranganath; Andrew Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. ICML '09. 2009: 609–616. 
^ TIMIT Acoustic-Phonetic Continuous Speech Corpus Linguistic Data Consortium, Philadelphia.
^ http://yann.lecun.com/exdb/mnist/.
^ D. Ciresan, U. Meier, J. Schmidhuber., "Multi-column Deep Neural Networks for Image Classification," Technical Report No. IDSIA-04-12', 2012.
^ P. E. Utgoff and D. J. Stracuzzi., "Many-layered learning," Neural Computation, 14, pp. 2497–2529, 2002.
^ J. Elman, et al., "Rethinking Innateness," 1996.
^ J. Shrager, MH Johnson., "Dynamic plasticity influences the emergence of function in a simple cortical array," Neural Networks, 9 (7), pp. 1119–1129, 1996
^ SR Quartz and TJ Sejnowski., "The neural basis of cognitive development: A constructivist manifesto," Behavioral and Brain Sciences, 20 (4), pp. 537–556, 1997.
^ S. Blakeslee., "In brain's early growth, timetable may be critical," The New York Times, Science Section, pp. B5–B6, 1995.
^ {BUFILL} E. Bufill, J. Agusti, R. Blesa., "Human neoteny revisited: The case of synaptic plasticity," American Journal of Human Biology, 23 (6), pp. 729–739, 2011.
^ J. Shrager and M. H. Johnson., "Timing in the development of cortical function: A computational approach," In B. Julesz and I. Kovacs (Eds.), Maturational windows and adult cortical plasticity, 1995.
^ D. Hernandez., "The Man Behind the Google Brain: Andrew Ng and the Quest for the New AI," Wired, 10 May 2013.
^ C. Metz., "Facebook's 'Deep Learning' Guru Reveals the Future of AI," Wired, 12 December 2013.
^ 谷歌收購DNNresearch，下一個帝國呼之欲出. CSDN. 2013-03-13 [2014-07-20]. 
^ G. Marcus., "Is "Deep Learning" a Revolution in Artificial Intelligence?" The New Yorker, 25 November 2012.


外部連結[編輯]

來自蒙特婁大學的深度學習資訊 [1]
傑弗里·辛頓的首頁 [2]
深度學習影片教程 [3]
燕樂存的首頁 [4]
麻省理工大學生物和計算學習中心 (CBCL) [5]
史丹福大學提供的無監督特徵學習和深度學習教程 [6]
GoogleDistBelief框架 [7]
Theano深度學習工具包（使用Python） [8]
Deeplearning4j開源深度學習工具包（使用Java） [9]
NIPS 2013會議（介紹深度學習相關資料） [10]





 
						取自 "https://zh.wikipedia.org/w/index.php?title=深度學習&oldid=44235700"					
2 個分類：機器學習神經網絡 



導覽選單


個人工具

沒有登入對話貢獻建立帳號登入 



命名空間

條目
討論




台灣正體



不轉換
簡體
繁體
大陸簡體
香港繁體
澳門繁體
馬新簡體
台灣正體






查看

閱讀
編輯
檢視歷史



更多







搜尋



 







導航


首頁分類索引特色內容新聞動態近期變更隨機條目 



說明


說明維基社群方針與指引互助客棧知識問答字詞轉換IRC即時聊天聯絡我們關於維基百科資助維基百科 



列印/匯出


下載成 PDF 



工具


連結至此的頁面相關變更上傳檔案特殊頁面可列印版靜態連結頁面資訊維基數據 項目引用此頁面 



其他語言


العربيةCatalàDeutschEnglishEspañolفارسیSuomiFrançaisBahasa IndonesiaItaliano日本語한국어PortuguêsРусскийSlovenščinaSvenskaไทยTürkçeУкраїнськаTiếng Việt 
編輯連結 





 本頁面最後修訂於2017年5月4日 (週四) 13:32。
本站的全部文字在創用CC 姓名標示-相同方式分享 3.0 協議之條款下提供，附加條款亦可能應用（請參閱使用條款）。
Wikipedia®和維基百科標誌是維基媒體基金會的註冊商標；維基™是維基媒體基金會的商標。
維基媒體基金會是在美國佛羅里達州登記的501(c)(3)免稅、非營利、慈善機構。


隱私政策
關於維基百科
免責聲明
開發人員
Cookie 聲明
手機版檢視



 

 











2016 暑期高效能運算課程-臺大計資中心 










2016 暑期高效能運算課程

High performance computing and Big data.










GPU 與CUDA 學習工作坊




 
基本資訊
 


【日期】 2016/8/22、8/23
【時間】  ■09:10~12:00  ■13:20~16:10
【地點】  計中 106 教室

【費用】 臺灣大學及國立臺灣大學系統 1000 元，其他 2000 元
【主辦單位】國立臺灣大學計算機及資訊網路中心（臺大計中）
【協辦單位】NVIDIA 及 兌全有限公司




 
師資
 


兌全有限公司 / 鄭羽熙 博士
兌全有限公司 總經理
		學歷：國立台灣大學 電機系博士 (1992)
		經歷：1. 中山科學研究院簡聘技正 2. 德碩半導體股份有限公司晶片設計經理 3. 麗臺科技股份有限公司副總工程師
		專長：1. Multi-core parallel processing and GPU massive processing  2. Video codec and algorithm 3. Embedded SOC system  4. FPGA and Chip design  5. ECG and HRV application
        




 
課程簡介
 


2016年3月於南韓首爾，AlphaGo（Google DeepMind所開發的電腦圍棋軟體）以四勝一負的戰績擊敗韓國職業九段棋士李世乭，此舉證實了使用深度學習（Deep Learning）來實現人工智慧（Artificial Intelligence）的實用性；也預言了深度學習技術在未來科技發展中將扮演著非常重要的角色。然而在深度學習的運算模型中，多節點及多層次的運算需求將直接挑戰處理器的平行計算能力，因此CPU處理器的計算資源將無法完全負擔深度學習眾多節點的平行運算需求。近年來，GPGPU（General-Purpose computing on Graphics Processing Units）的計算理念已然成熟，加上半導體技術的迅速成長，至今在單一圖形處理晶片中，處理器核心（processor core）的個數已增加到3,840；所提供每秒單精度浮點運算次數可達10.6 TFLOPS（Tera FLoating-point Operations Per Second)，搭配支援多平臺（Windows、Linux、MAC）的程式開發環境，例如：CUDATM SDK（Compute Unified Device Architecture Software Development Kit）；在眾多科學計算（scientific computing）領域中所發表的論文早已證明其加速運算的強大效能，因此運用GPGPU來實現深度學習在各領域的研究已成為目前及未來的研發趨勢。
為了引導學員能夠輕易的瞭解CUDA的平行理念，在短時間內就可以上手撰寫平行程式，本課程使用國立臺灣大學計算機及資訊網路中心配備的Tesla K80 GPUs作為訓練平臺，從CUDA的基礎平行計算模式開始介紹，針對在撰寫CUDA平行程式時，使用特殊記憶體該註意的事項、大量資料在平行處理時的各種切割方式、從多執行緒（multiple threads）到多GPU處理核心的分配方法、跨越多GPU晶片（multiple GPUs）的多串流（multiple streams）處理、甚至於整合CUDA到叢集系統（Cluster）運算的MPI（Message Passing Interface）處理等議題，都是本課程要探討的內容；此外，正值CUDA 8.0、Tesla P100及NVIDIA DGX-1 Deep Learning Supercomputer等新功能發表期間，本課程也將搶先介紹其支援的新架構及新技術。
為了加強學習的效果，本課程的安排特別著重於由淺而深，逐步引導學員從理解進而可以最快的速度上手CUDA平行程式的設計。搭配CUDA上機實習及許多個案研究，本課程的最大目標是引導CUDA初學者能夠在短時間內快速進入CUDA的平行世界。只要是具備有C程式語言的設計基礎者均可參加此課程，利用兩天的時間快速學會CUDA平行處理程式的設計技巧及目前最新平行理念。
誠摯的歡迎您來參加這場不能錯過的CUDA深度學習。




 
課程內容
 





第一天
第二天




CUDA SDK 8.0, Tesla P100圖形處理器架構及計算能力簡介
Thread, thread block, grid, warp 等平行計算模式概念說明
上機練習(一): 簡單的CUDA 平行程式開發
GPU Global, constant, shared 記憶體的階層概念及使用方法說明
						上機練習(二): 使用CUDA記憶體加速平行計算
CUDA 程式的效能評估方法
						上機練習(三): 在平行程式中加入計時器評估
個案研究 A : N Body 計算
						上機練習(四): 加強 N Body 平行計算
個案研究 B : Monte Carlo 及 Random Number Generation 計算
						上機練習(五): 加強 Monte Carlo 平行計算




NVIDIA DGX-1 Deep Learning Supercomputer 簡介
NVLink High Speed Interconnect 簡介
平行 streams及操控Multiple GPUs的使用方法 
上機練習(六): 使用 multiple streams 設計平行程
上機練習(七): Multiple GPUs 平行程式開發
CUDA Texture 及 Surface 記憶體的使用方式
上機練習(八): Image Sobel filtering and Rotation
個案研究 C : Image Denoising
GPUDirect Peer-to-peer Transfers 及 Unified Virtual Addressing
CUDA MPI









 
對象
 


具備基本C Programming知識，未來想從事高階軟體開發工程師、財務分析工程師、電子/電機相關工程師、測試/量測工程師、醫學界研發人員、生物科技研究人員，以及統計/數值分析人員等




 
備註
 



本課程使用階梯式教室，備有電源插座，建議學員自行攜帶筆記型電腦，以隨時配合講師操作實習。請預先安裝SSH client軟體（如putty）。
本課程提供午餐，限當日準時報到學員。
如因報名人數過多，本中心得更換教室。請學員務必於上課前三日，上網確認上課地點。










立即前往報名

















深度學習機構 | NVIDIA



















































 TWN - 台灣



驅動程式



GeForce 驅動程式




GeForce Experience




所有驅動程式






產品


處理器



GeForce




Quadro




Tegra




Tesla




NVIDIA GRID




NVS




過去世代的產品






技術



Advanced Rendering




CUDA




人工智慧和深度學習




G-SYNC




多 GPU 技術




Optimus




OptiX




PhysX




SLI




所有技術





NVIDIA DGX



NVIDIA DGX-1




分析專用的 NVIDIA DGX






NVIDIA GRID



虛擬桌面與應用程式




視覺運算裝置




雲端遊戲






Quadro VCA




NVIDIA DRIVE




3D Vision



平臺



桌上型




筆記型




智慧型手機




工作站




伺服器




高效能運算




汽車




嵌入式






SHIELD



Android TV




Tablet




Portable








深度學習與人工智慧



深度學習概觀



技術



人工智慧




機器學習




自然語言處理




影像辨識




自駕車





產品



深度學習應用軟體




DGX-1 深度學習系統




Jetson TX1 超級電腦模組




NVIDIA DRIVE PX 2




NVIDIA TITAN X




Tesla K80 加速器




Tesla M4 Hyperscale 加速器




Tesla M40 加速器




Tesla P100 資料中心加速器





教育



深度學習簡介




深度學習機構




線上課程





社群



深度學習部落格




DIGITS 使用者群組




人工智慧開創計畫








體驗區



GeForce.com 台灣




GeForce 台灣論壇




3D Vision Live




GTC (GPU 繪圖處理器技術大會)




NVIDIA 合作夥伴網絡




GPU 扶植創業專區




開發人員專區



CUDA Zone




DesignWorks 




嵌入式運算




GameWorks






PartnerForce




NVIDIA研究






酷玩意




關於 NVIDIA



公司資訊




新聞中心




NVIDIA Taiwan 官方部落格




投資者關係




企業公民




人工智慧運算




購買資訊




相關活動




NVIDIA 台灣官方粉絲團











深度學習機構




NVIDIA Home>產品>深度學習機構 

讓世界瞭解如何使用深度學習解決問題







  
 


什麼是深度學習機構
深度學習機構提供學習設計、訓練以及在應用程式中部署神經網路的最新技術。您可以探索廣為使用的開放原始碼架構和 NVIDIA 最新的 GPU 加速深度學習平臺。
NVIDIA 深度學習機構 (DLI) 與例如 Udacity 和 Coursera 等著名的線上教學供應商以及 Microsoft 等領先業界的雲端服務供應商合作，在全球各地提供線上和麵授訓練課程。

 











深度學習線上課程
從新創初期到躋身全美 500 強企業，公司發展的各個階段皆會使用到深度學習和人工智慧。深度學習是人工智慧發展最快的技術領域，推動所有新興市場的重要發展，未來也將運用在我們意想不到的地方。


NVIDIA 深度學習實驗室
自駕車線上培訓微課程 [由    Udacity 提供]




深度學習工作坊
NVIDIA 深度學習機構為產品團隊、會議和其他組織提供工作坊。深度學習機構教師可以與您合作，量身打造課程內容，滿足特定專案和群眾的需求。



 


 
NVIDIA 官方授權深度學習實作坊開課時刻表
加入NVIDIA深度學習官方認證講師授課實作坊，享有與全球同步互動的參與性。


NVIDIA 深度學習實作坊 | 新竹市 | 2017.8.11 國家實驗研究院國家高速網路與計算中心
NVIDIA 深度學習實作坊 | 臺中市 | 2017.8.18 國立中興大學計算機及資訊網路中心 

 



對深度學習訓練有任何疑問嗎？ 聯絡我們.





 


GPU運算解決方案
總覽
什麼是 GPU 運算?
GPU 應用
個案研究
為什麼選用 TESLA?
伺服器與工作站
購買資訊


軟硬體
Tesla 產品資料
NVLINK 高速互連
Tesla軟體功能
軟體開發工具
CUDA 開發套件
顧問與訓練服務
雲端運算
OpenACC GPU 指令
GPU Starter 開發套件


新聞和信息
新聞和文章
實證考驗
Webinars
NVIDIA研究
追求更美好的科學
請註冊訂閱Tesla電子報
聯絡我們


線上搜尋NVIDIA
NVIDIA Taiwan 官方部落格FacebookFlickrYoutube


&nbsp;

 



車用 | 繪圖卡 | GRID | 高效能運算 | 視覺化方案 | CUDA | Tegra



活動 | 開發人員 | 人才招募 | 購買資訊 | 訂閱 RSS | 電子報 | 聯絡我們 | 產品安全



                
                版權所有 
                
                ©
                
                 2017 NVIDIA 公司 
                
                法律資訊 | 隱私權政策

















通向未來人工智慧的三條賽道：高效能計算、神經形態計算和量子計算 - 閱讀屋



















 










閱讀屋



健康
養生
時尚
娛樂
家居
汽車
科技
科學
財經
體育
旅遊
社會
時政
人文
熱門













通向未來人工智慧的三條賽道：高效能計算、神經形態計算和量子計算


科技 
2017-06-26




編者按：本文由微信公眾號”機器之心“（ID：almosthuman2014）編譯，選自datasciencecentral，作者：William Vorhies，參與：黃小天、蔣思源；36氪經授權釋出。
有三種技術，可以帶來更快、更簡單、更廉價、更聰明的人工智慧。今天，高效能計算，以及後來出現的量子計算機和神經形態計算已觸手可及；並且，後兩者正變革著人工智慧和方興未艾的深度學習。
人工智慧與深度學習的三個問題
時間：訓練一個 CNN或 RNN通常需要數週的時間。這還不算上為了達到所需的效能表現，花在定義問題以及程式設計深度網路時迭代成敗上的數週甚至數月的時間。
成本：數百塊 GPU連續數週的計算成本高昂。從亞馬遜雲端計算服務中租用 800塊 GPU一週的時間花費在 120,000美元。這還沒開始算上人力成本。完成一個 AI項目往往需要佔用最優秀人才數月、一年甚或更多的時間。ADVERTISEMENT
資料：由於缺乏足夠數量的標註資料而使項目無法展開的情況比比皆是。由於無法以合理的價格獲取訓練資料，很多好創意被迫放棄。
因此，取得較好商業表現的多是影象處理、文字和語音識別，並且那些借力谷歌、IBM、微軟和其他巨頭的初創企業成果更多。通向未來人工智慧的三條賽道
如果你關註這一領域就會發現，我們已經使用 CNN和 RNN做了一些應用，但是超越這些應用的進展才剛開始。下一波的進展來自生成對抗網路和強化學習，並獲得了問答機器（比如沃森）的一些幫助。我們最近的一篇文章對此作了很好的總結（詳見：人工智慧的三個階段：我們正從統計學習走向語境順應）。
這是一個有關如何推動人工智慧發展的最常見版本。這是日益複雜的深度神經網路，它與現在的 CNN和 RNN有著不同的架構。僅僅是讓它們執行更快。
實際上未來可能相當不同。現在展現在我們面前的是基於完全不同技術的通向未來人工智慧的三條賽道。它們是：ADVERTISEMENT
1.高效能計算（HPC）
2.神經形態計算（NC）
3.量子計算（QC）
其中，高效能計算是本篇文章關註的焦點。晶片製造商以及以及谷歌等巨頭正在開展競爭，爭相打造深度學習專用晶片。
另外兩個，神經形態計算（也被稱為脈衝神經網路）和量子計算看起來似乎還需要幾年。但事實是商用神經形態晶片和商用量子計算機已投入應用於機器學習之中。不管兩者是冷是熱，神經形態計算和量子計算都會使人工智慧的未來之路變得更撲所迷離，但這是一件好事。高效能計算（HPC）
高效能計算關註度最高。它使用我們已知的深度神經網路架構，並使其更快更容易被獲取。通常這意味著兩件事：更好的通用環境，比如 TensorFlow；更多地利用更大資料中心中的 GPU和 FPGA，也許不久之後會出現更專業化的晶片。
現如今人工智慧的新商業模式是「開源」。2016年上半年，人工智慧的每一個主要玩家都開源了其 AI平臺。這些競爭者在資料中心、雲服務、人工智慧 IP上進行了大量投資。開源背後的策略很簡單：平臺使用者最多者獲勝。ADVERTISEMENT
同時，英特爾、英偉達及其他傳統晶片製造商也正積極滿足使用者對於 GPU的新需求，其他巨頭如谷歌和微軟則自己開發了全新的專屬晶片，從而使其深度學習平臺更快，更具吸引力。連同新近推出的 TPU，谷歌鐵定了要把 TensorFlow作為其主打的通用型解決方案。微軟則在兜售非專屬晶片 FPGA的使用，並釋出了 CNTK 2.0完整版；它提供了 Java API可直接整合 Spark，同時支援 Keras程式碼。據稱 CNTK比 TensorFlow更快更精確，且也提供 Python API。
整合 Spark將持續成為一個重要的推動力。雅虎已實現了 TensorFlow與 Spark的整合。Spark的主要商業供應商 Databricks現在有其自己的開源工具包，以把深度學習與 Spark相整合。這裡的關鍵驅動力將至少解決三個障礙中的兩個。這些進展將會使程式設計更快更簡單，從而帶來更可靠的結果，尤其是更快速的晶片將會使機器計算的原始時間變的更短。
問題是這些提升將會幫助我們到那一步（這和摩爾定律的侷限性很像），是否可用於 GAN和強化學習；答案很可能是，至少在今天我們知道如何使用這些深度學習架構。神經形態計算（NC）或脈衝神經網路（SNN）
神經生態計算或脈衝神經網路是通向強人工智慧的一條路徑，它基於一些大腦執行的原理而設計，與深度神經網路的結構和原理有顯著的不同。
神經形態計算最開始是由研究者發現大腦神經元並不是每一次都全部啟用而啟發。神經元將選擇性訊號沿著突觸傳播，並且資料實際上是以訊號的潛在脈衝方式傳播。實際上，這些訊號是由一系列脈衝組成，所以研究者對資訊是否編碼在一系列脈衝的振幅、頻率或延遲中做進一步探討。
在現有的深度神經網路中，神經元根據相對簡單的啟用函數（如 Sigmod或 ReLU等）每一次都會全部啟用。
神經形態計算相對於深度神經網路已經展示了一些巨大的提升：

因為並不會每一次都啟用所有的神經元，所以大
早期案例展示了脈衝神經網路可以僅使用無監督技術（無標註）從環境中學習，而少量的樣本可以令它們學習非常迅速。
神經形態計算可以從學習一個環境泛化到另一個環境，它能夠記憶並且泛化，這真的是一個突破性的能力。
因為神經形態計算能效非常高，所以可以進行小型化。

所以轉變這種基礎架構能夠解決深度學習如今面臨的三個基礎問題。更重要的是，如今我們能夠購買和使用神經形態脈衝神經網路系統。這並不是遙遠未來的一個技術。
BrainChip Holdings (Aliso Viejo, CA)已經在拉斯維加斯最大的賭場應用了商業安防監控系統，並且它宣稱還有一些其他應用也已經交付。在拉斯維加斯，該系統的功能就是通過標準攝像頭的視訊流自動監控 dealer的錯誤。該系統完全通過觀察學習遊戲規則。BrainChip表明它的 SNN有 IP 專利保護，並藉此推出了一系列賭博監控產品。
現如今有很多科技進步，但 SNN是開發 AI商業系統很有競爭力的選擇。量子計算
可能讀者對量子計算並沒有如下認識：

量子計算如今是可用的，Lockheed Martin從 2010開始就已經從事相關的商業運作。還有其他幾家公司都在推出基於 D-Wave量子計算機的商業應用，D-Wave的量子計算機是第一個發展的商業市場。
今年五月，IBM聲稱他們的量子計算機 IBM Q現在已經可以投入商業中。在這是一種基於雲端的訂閱服務，它無疑將大大簡化對這些昂貴且複雜的機器的訪問。IBM表示截止到目前，使用者已經在 IBM Q機器上進行了 30萬次實驗。
谷歌和微軟計劃在兩三年內釋出他們的商業化量子機器，並整個作為獨立的研究學術機構。
D-Wave和其他一些獨立的研究者已經引進了量子計算機的開源程式語言，他們希望可以對量子計算機程式設計更加容易。
量子計算機擅長於解決現有所有類型的優化問題，包括整套基於隨機梯度下降的各類演算法。量子計算機也很容易模擬受限玻爾茲曼機，它是很多深度神經網路架構中的一個，並且還可以用於深度學習結構中以像 CNN那樣解決影象分類問題。因為基礎架構不一樣，所以我們稱其為量子神經網路（QNN）。
根據谷歌基準 2015年的研究報告，D-Wave量子計算機相對於傳統計算機效能要優秀 108倍，也即快 1億倍。谷歌工程主任 Hartmut Nevan說：「D-Wave在 1秒中所做的，傳統計算機需要花 1萬年計算」。

所以量子表徵仍然是第三條通向強人工智慧的道路，它同樣剋服了速度與成本問題。三條道路
事實是神經形態計算和量子計算都是很有潛力的方向，它們都有可能令深度學習甚至是新型人工智慧更快地執行。
首先是時間線。高效能計算如今正在持續發展，並在接下來幾年都基於前面介紹的新型晶片而得到效能上的持續發展。然而，隨後幾年很大一部分實驗室和資料中心都會由更先進的量子計算機和神經形態計算所替代。
像谷歌 TensorFlow和微軟 Cognitive Toolkit（CNTK）那樣的深度學習平臺正在發展，而其他競爭對手也在努力構建平臺並獲得使用者。因此隨著量子計算和神經形態計算的能力得到傳播，這些平臺都會適應它們。
神經形態脈衝神經網路（SNN）和量子計算現在僅僅在商業上出現，但它們都會賦予人工智慧非凡的能力。
SNN有希望成為強大的自學習者。通過更小的非標註訓練集以及不同領域之間的知識遷移能力，極大地提升了效率。
量子計算機將徹底消除時間障礙，成本障礙最終也將降低，基於時間的解決方案從數月縮短至數分鐘。重要的是當前使用的學習風格被稱作增強型量子計算，因為它是基於當前的深度學習演算法，並提升了其效能。然而將來會出現基於完全不同能力（為這些機器所獨有）的全新類型的機器學習。
我的個人感覺是在量子計算和神經形態計算上，我們現在的處境和 2007年很像，那一年穀歌的 Big Table（譯者註：谷歌設計的分散式資料儲存系統，用來處理海量資料的一種非關係型的資料庫）變成了開源的 Hadoop。一開始我們確實不知道怎麼對待它，但是三年之後 Hadoop幾乎主導了整個資料科學。我認為從今天開始，下一個三年也同樣令人驚奇。



科技 

 » 36氪
 喜歡這篇文章嗎？立刻分享出去讓更多人知道～







相關文章
2017-02-09未來；人工智慧的天下 
2016-12-01從谷歌出發，來看人工智慧的投資賽道 
2017-04-04你即將失業！未來人工智慧將同時勝任體力和腦力勞動 
2017-03-27未來人工智慧會發展成啥樣？博鰲論壇做了份命題作文 
2017-01-09『120秒』未來人工智慧推動下的五大媒體變革 
2017-01-07IBM說這5項科技5年後將改變世界：未來，人工智慧可預測精神疾病 
2016-12-10智慧投顧是理財行業的未來! 人工智慧是必然趨勢 
2016-12-08未來，人工智慧將帶動金融服務如何發展？ 
2016-11-17移動網際網路時代結束，未來“人工智慧”當道？ 









您可能感興趣




《悟空傳》片場趣事大揭祕 眾主創“集體飆車” 


白酒股午後持續攀升 貴州茅臺五糧液股價均創新高 


沙特等四國所提十三點要求 連美國都認為卡達難以滿足 




46歲閆妮身材性感似少女，PK宋茜美腿！ 




張歆藝幫華晨宇套圈贏飲料 為林志玲安利羊肉吃法 




江西豐城一村莊被洪水圍困 25名被困村民緊急轉移 




南方多地發生洪災:數百萬人受災 多省列車停運 




萬科榮成蘭喬聖菲王樂樂:認真服務每一位購房者 




中蒙關係友好？蒙古國大選候選人帶頭炒作中國威脅論 


滁州市地稅局原黨組書記、局長徐春雷接受組織審查 


















© 2017 閱讀屋 ReadHouse.Net 聯絡我們






GTC Taiwan 大會Presented ByGTC Around the WorldGTCGTCx AUSTRALIAGTC CHINAGTC EUROPEGTCx INDIAGTC JAPANGTCx KOREAGTC TAIWANGTCx WASHINGTON DCGTCGTCx AUSTRALIAGTC CHINAGTC EUROPEGTCx INDIAGTC JAPANGTCx KOREAGTC TAIWANGTCx WASHINGTON DCTaipei   Sep. 21-22, 2016參加精彩內容大會主題現場競賽與海報展示議程合作夥伴ECSECSUSTREAM (EN) TWITCH (中文)Taipei   Sep. 21-22, 2016MenuPresented by議程Day 1Day 2GTC TAIWAN 2016 主題議程時間議程07:30 - 09:30大會報到09:30 - 11:00主題演說 NVIDIA 共同創辦人、總裁兼執行長黃仁勳11:00 - 13:00午餐時間深度學習與人工智慧13:00 - 13:30A.I. – A New Style of Computing NVIDIAMarc Hamilton13:30 - 14:00以深度學習加速語音及影像辨識應用發展 中華電信研究院楊蕣仿14:00 - 14:30Towards Machine Comprehension of Spoken Content 台灣大學資訊網路與多媒體研究所李宏毅14:30 - 15:00Learning from Dashcam Videos 國立清華大學電機工程學系孫民15:00 - 15:30休息時間15:30 - 16:00專為 Machine Learning 設計的運算平臺 Hewlett Packard Enterprise沈仲傑16:00 - 16:20圖形處理器於腦部核磁共振影像處理應用臺中榮民總醫院陳享民靜宜大學洪哲倫16:20 - 16:40基於深層網路之影像檢索特徵學習 中央研究院資訊科學研究所陳祝嵩16:40 - 17:00NVIDIA DGX-1 超級電腦與人工智慧及深度學習研究發展 NVIDIA康勝閔17:00閉幕暨問捲回饋專業級視覺化繪圖13:00 - 13:30文物電腦斷層掃描系統建置及影像重建處理分析 國立故宮博物院陳東和、黃千奇13:30 - 14:00NVIDIA系列產品應用於NextLab即時動畫專案之分享 NextLab石千泓14:00 - 14:30麗明營造使用NVIDIA成效分享 麗明營造 BIM中心朱建璘14:30 - 15:00【樓下的房客】以數位特效技術打造寫實近代台灣風格街景 WeFX studio左志中15:00 - 15:30休息時間15:30 - 16:00Lenovo 桌面虛擬化最佳實踐 Lenovo楊學斌16:00 - 16:20東海大學使用 NVIDIA Quadro & GRID 技術在教育雲端創新服務的經驗分享 東海大學 電子計算機中心楊朝棟16:20 - 16:40ASUS GPU Server 華碩電腦劉禮璿16:40 - 17:00工欲善其事，必先利其器 如何尋找適合您的行動工作站 MSI林品潔17:00閉幕暨問捲回饋高效能運算與虛擬桌面技術13:00 - 13:30見證以CUDA編程鋒面法以加速不可壓縮納維爾史托克方程的有限元素求解 國立台灣大學工程科學吳政道利用多GPU卡模擬以高強度聚焦超音波來燒灼肝腫瘤的過程 台灣大學工程科學Maxim Solovchuk13:30 - 14:00見証利用OpenAcc來加速三維不可壓縮納維爾史托克方程的計算 國立台灣大學工程科學高仕超利用GPU來加速特定病人氣管的成像 國立台灣大學工程科學張育維14:00 - 14:30高效益 , 設計專利保護 如何雙贏? Dell CCC蘇建龍14:30 - 15:00OpenPOWER Foundation 概貌介紹 IBM, Mary Coucher15:00 - 15:30休息時間15:30 - 16:00High Density / Optimized GPU Solution for HPC and GRID Applications SuperMicroBenedict Khoo16:00 - 16:20藉由NVIDIA GRID 2.0 強大運算能力，使研發生產力最大化 凱柏精密機械 電腦中心劉華正16:20 - 16:40全面保護企業的關鍵智慧資產 Vmware羅元佃16:40 - 17:00啟動嵌入式系統的智慧，為生活帶來美好連結 DT42陳伯符17:00閉幕暨問捲回饋嵌入式運算平臺與虛擬實境應用13:00 - 13:30工研院智慧影像分析在自動駕駛服務與機器人自動化系統之應用 工研院機械與機電系統研究所胡竹生13:30 - 14:00深度學習應用於健康照護機器人 三個爸爸吳昊14:00 - 14:30機器人解決方案&開放式平臺新概念&機器人視覺及深度學習 金寶科技 新產品事業開發處林傳凱14:30 - 15:00NVIDIA TK1/TX1於醫學影像處理 靜宜大學洪哲倫15:00 - 15:30休息時間15:30 - 16:00透過 NVIDIA VRWorks 加速虛擬實境應用 NVIDIA林耀南16:00 - 16:20NVIDIA VR Funhouse 開放原始碼 NVIDIAVictoria Rege16:20 - 16:40嵌入式和高可靠度的機器視覺 立普思劉凌偉16:40 - 17:00智慧多軸飛行器在TX1平臺的應用 Aeroprobing Co.高丈淵17:00閉幕暨問捲回饋新創公司創意發表會 / GPU海報暨智慧機器人競賽成果發表14:00 - 16:40新創公司創意發表會16:40 - 18:30GPU海報競賽成果發表 ( ~ 17:00)嵌入式智慧機器人競賽成果發表 ( ~ 18:30)現場活動13:00 - 17:00NVIDIA 技術展區NVIDIA 夥伴展區NVIDIA VR 體驗區GPU海報展示嵌入式智慧機器人決賽17:00閉幕暨問捲回饋今年 GTC Taiwan 為強化所有產業夥伴的研究動能，首度加開「深度學習實作營」 (採線上報名制，付費課程) ， NVIDIA 將以領先全球業界的人工智慧應用技術，針對台灣當地產業發展特性與應用面規劃專屬訓練課程。精彩內容包括：在 NVIDIA DIGITS 系統訓練物件偵測模型、適用於自動駕駛車的 TensorFlow 圖像分割應用、於 MXNet 平臺進行醫學影像分析以及在嵌入式 Jetson TX1平臺上佈署 Caffe模型，從深度學習的入門到進階，讓台灣開發者能藉此機會深入體驗與學習 GPU 加速運算的效能所帶來的驚奇。GTC Taiwan 2016 深度學習實作營 課程資訊時段課程08:00 - 09:00報到09:00 - 10:30Getting Started with Deep Learning (latest DIGITS)Instructor-Led Lab深度學習入門 (帶領使用NVIDIA DIGITS 系統最新版)深度學習機構講師指導10:30 - 10:45課間休息10:45 - 12:15Deep Learning for Object Detection (latest DIGITS)Instructor-Led Lab在NVIDIA DIGITS 系統上訓練物件偵測模型深度學習機構講師指導12:15 - 13:00午餐13:00 - 14:30Deep Learning for Image Segmentation (TensorFlow)Instructor-Led Lab深度學習圖像分割應用於TensorFlow深度學習機構講師指導14:30 - 14:45課間休息14:45 - 16:00Deep Learning for Medical Analysis (MXNet)Instructor-Led Lab深度學習應用在醫學影像分析於MXNet深度學習機構講師指導16:00 - 16:30課間休息16:30 - 18:00Deep Learning Deployment*Instructor-Led Lab深度學習佈署在 Jetson TX1平臺深度學習機構講師指導 *請自備Jetson TX1 開發者套件18:00活動結束「深度學習實作營」註意事項請參加者務必攜帶個人筆記型電腦。若您有個人的 Jetson TX1開發套件可攜帶至會場。現場備有螢幕，需與其他學員共用。如因個人因素不克出席，退費申請截止時間為 2016/9/11 23:59客服信箱：NVIDIA.GTC@erapr.com.tw客服專線：(02)2578-8225
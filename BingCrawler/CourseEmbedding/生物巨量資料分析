





課程規劃表 - 國立交通大學 統計學研究所

















您的瀏覽器不支援Javascript，部分功能將無法呈現。


:::
迴首頁
|

網站導覽
|
註冊
|
友善列印
|
Login
|
English
國立交通大學 統計學研究所





:::巨量資料分析學分學程最新消息簡介課程規劃表巨量資料分析實務課程相關連結


:::
課程規劃表
學分學程課程規劃表
一、 學程名稱：巨量資料分析
二、 課程名稱及開課系所  (總學分必須修滿18學分) ：

A. 先修課程：
先修課程：學生須於進入學程時繳交成績單，由本學程所規定之委員會審查該生之「程式設計」、「統計概論」二科是否修過大學部水準之課程。若有不足者，必須補修通過大學部等同課程或選修通過研究所相關課程，始得發給學程修畢證明。


B. 基礎核心課程 (至少修滿 9 學分)：　











課程名稱
開課系所
學分
備註


資料探勘
資工系
3
　


統計方法
統計所
3
或可修讀「統計應用方法」(生物資訊及系統生物研究所)　


數據科學概論與軟體實務
資工系
3
　


巨量資料分析技術與應用
資工系
3
 


數據科學專題
資工系
3
　













C. 進階選修課程 (至少修滿 6 學分)：　











課程名稱
開課系所
學分
備註


機器學習
資工系
3
　或可修讀「機器學習」(應用數學系)


平行程式設計
資工系
3
　


資料視覺化與視覺分析
資工系
3
　


迴歸分析
統計所
3
　


統計計算
統計所
3
　


多變量分析
統計所
3
　


統計學習
統計所
3
　








三、 召集人姓名 ：彭文志   單位：資工系
                                             黃冠華   單位：統計所
四、 聯絡人姓名 ：曹雅珽   單位：資工系

 

▲Top








:::


標準版RSs 2.x套裝最新消息系所公告演講/學術活動公告招生訊息職涯座談會學生獎助/就業資訊本所簡介系所簡介系所組織章程活動花絮位置圖聯絡我們本所成員師資行政技術人員學生招生入學未來學生專區碩士班博士班國際生學術研究各領域研究說明學術著作研究計畫技術報告課程暨修業當年度課程所有課程碩士班修業規章博士班修業規章巨量資料分析學分學程統計諮詢校友專區畢業生發展校友名單


Login
:::

帳號:
密碼:
















本站
 站外 

















本網站著作權屬於國立交通大學 統計學研究所，請詳見使用規則。
電話：886-3-5712121 分機號碼：56801　傳真：886-3-5728745
地址：30010新竹市大學路1001號綜合一館4樓　 位置圖　
聯絡我們
E-Mail：statmail@stat.nctu.edu.tw
建議使用Mozilla Firefox 3.6+或Google瀏覽器，並將螢幕解析度設定為1024*768，以獲得最佳瀏覽效果




	
1419988























	台灣富士通與成功大學簽署「農業與生物多樣性巨量資料合作備忘錄」 - 台灣富士通













GTM-MGX65ZP








Skip to main content















Taiwan
變更國家 A 到 B AlgeriaAnguillaArgentinaArubaAustraliaAustriaBahamasBarbadosBelgiumBrazilBulgariaC 到 ECanada (English)Canada (French)Cayman IslandsChileChinaColombiaCzech RepublicDenmarkDominicaEgyptEstoniaF 到 IFinlandFranceFrench GuianaGermanyGreeceGuadeloupeHong KongHungaryIndiaIndonesiaIrelandIsraelItalyJ 到 MJamaicaJapanKazakhstanKoreaLuxembourgMacedoniaMalaysiaMartiniqueMexicoMontserratMoroccoN 到 RNetherlandsNetherlands AntillesNew ZealandNorwayPhilippinesPolandPortugalRomaniaRussiaSSaint Vincent and GrenadinesSerbiaSingaporeSlovakiaSloveniaSouth AfricaSpainSwedenSwitzerland (French)Switzerland (German)TTaiwanThailand (English)Thailand (Thai)Trinidad and TobagoTurkeyU 到 VUkraineUnited Arab EmiratesUnited KingdomUnited StatesUzbekistanVietnam (English)Vietnam (Vietnamese)Virgin Islands BritishVirgin Islands U.S. Worldwide




 服務 數據中心服務 災難復原服務(DRS) e化教室服務(eClassRoom) 綠色機房建置服務(GDC) 維護服務 資訊委外服務(Outsourcing) 整合式威脅管控服務(UTM) 服務中心 技術與維修服務 Lifecycle Management Services Other 消費者服務 網路與通訊 無線 產品電腦及週邊產品 全系列伺服器 PRIMERGY標準工業伺服器 UNIX新一代 SPARC/Solaris 標準伺服器 ServerView伺服器管理軟體 PRIMEQUEST關鍵任務伺服器 ETERNUS儲存設備 ETERNUS產品 儲存管理軟體 磁帶系統 磁碟儲存系統 資料保護系統 全快閃儲存系統 用戶端電腦產品 筆記型電腦平板電腦 桌上型電腦工作站 精簡型電腦 Microsoft 印表機 DL 3750+ 點陣印表機 DL 3850+ 點陣印表機 印表機授權經銷商一覽表 掃描器 fi 系列 ScanSnap系列 ScanPartner系列 SP 系列半導體製造服務產品 半導體 存儲器産品 晶圓代工服務流通資訊設備 行動式解決方案 TeamPad700 收銀機解決方案 TeamPoS 7000 A -Series TeamPoS 7000 M-Series TeamPoS A10 解決方案 基礎解決方案 富士通整合系統 PRIMEFLEX® for SAP HANA解決方案 產業解決方案 公共部門 政府機關解決方案 流通業 專賣及餐飲店管理系統 人潮計數系統 醫療業 護理系統 遠距照護支援系統 系統解決方案 掌靜脈辦識系統 智能社會解決方案 RFID IT Policy N@vi 高可用性/災難復原解決方案 主機容錯解決方案—everRun 支援與下載 產品 印表機 Dotmatrix Manual 掃描器 用戶端電腦 FIND 半導體技術雜誌 存檔 按產品分類 集團概要 企業理念 公司願景 公司價值觀 原則 行為準則 經營方針 品牌承諾 公司管理 富士通在台灣 台灣富士通  台灣富士通科技服務 台灣富士通將軍 香港商富士通電子(台灣) 台灣富士電化 富晶通科技 人才招募 企業簡介 富士通一覽 役員一覽 組織架構 富士通的歷史 富士通歷史1923-1949年 富士通歷史1950-1958年 富士通歷史1959-1969年 富士通歷史1970-1979年 富士通歷史1980-1991年 富士通歷史1992年至今 產品大事記 公司位置 全球位置 企業責任 社長致辭 特徵：ICT之力 專題：保持和超越可持續發展的ICT之力 特徵: 六人講述：如何靈活運用各種各樣的企業經營資本 專題：營造一個可持續發展的社會 富士通集團的CSR 富士通集團的CSR 富士通集團的CSR基本方針 CSR活動的目標和成就 活動的目標和成就 社會負責投資 使用ISO 26000開展企業社會責任活動 人權聲明 社會貢獻 富士通獎學金 社會貢獻活動的方式 員工志願者活動支援系統 促進學習、教育、文化和贊助活動 通過體育為社會做貢獻 國際支援和賑災活動 2014財年的示範活動 透過ICT提供機會和安全性 ict 靈活應對城市問題 靈活應對醫療問題 靈活應對環境問題 靈活應對自然災害 包容多元化，發展人力資源 多樣化和包容性 努力促進對人權的尊重 創造良好的工作環境 職業安全、衛生及健康管理 人力資源與職業發展 與利益關係者溝通和合作 利益關係者對話 與我們的客戶 品質措施 與我們的供應商 與我們的股東和投資商 管理系統 企業管理 風險管理 企業合規 資訊安全 品牌管理 創新管理 更多訊息 成功案例 公司新聞 新聞存檔 訊息公告 公司聯繫方法 台灣富士通科技服務 台灣富士通將軍 香港商富士通電子(台灣) 富晶通科技 公司活動 2007年8FX競賽 2010年MCU競賽 採購單條款 富士通集團資訊安全報告 富士通科學與科技雜誌 CIO全球智慧 富士通願景 社長致辭 人類開創的未來 互通互聯的價值 數位化轉型 新的模式 以人為本的智能社會 通向未來的里程圖 以人為本的創新 賦予人力量 事業的變革 橫跨不同行業的價值創造 富士通能為客戶貢獻的力量 企業IT的課題 數位化商業平臺 富士通提供的技術與服務 產品系列 整合資源創造價值 移動與賦力 資訊的新價值 資料安全 無所不能的雲端服務 整合計算 廣域網路優化






首頁
>


 集團概要
>


 更多訊息
>


 公司新聞
>


 新聞存檔
>
  2014年>台灣富士通與成功大學簽署「農業與生物多樣性巨量資料合作備忘錄」





 集團概要




 企業理念


 富士通在台灣


 企業簡介


 企業責任


 更多訊息


 成功案例


 公司新聞


 新聞存檔


 2017年


 2016年


 2015年


 2014年


 2013年


 2012年


 2011年


 2010年


 2009年


 2008年


 2007年


 2006年


 2005年


 2004年


 2003年


 2002年


 FEP




 訊息公告




 公司聯繫方法


 公司活動


 採購單條款


 富士通集團資訊安全報告


 富士通科學與科技雜誌


 CIO全球智慧




 媒體連絡


 富士通品牌


 社長致辭


 環境


 事業方針









台灣富士通與成功大學簽署「農業與生物多樣性巨量資料合作備忘錄」
Fujitsu Taiwan Limited

臺北, 2014-08-04



成功大學校長黃煌煇與台灣富士通董事長池上一郎共同簽署MOU


富士通近年來積極佈局雲端服務市場，2014年5月將雲端科技及巨量資料做為集團發展的核心，在全球ICT解決方案市場上站穩日本第一的位置。其中，農業生物更是富士通發展的強項之一。國立成功大學致力於生物資訊研究已多年，校長黃煌煇表示為將農業與生物基因體研究和巨量資料分析、雲端運算結合，，將借重台灣富士通處理巨量資料的經驗，將巨量資料的運算應用於各個領域，雙方已於7月底，由成功大學黃煌煇校長及台灣富士通董事長池上一郎代表簽署「農業與生物多樣性巨量資料合作備忘錄」完成。
台灣富士通在2012年就與成功大學簽署基因體研究平臺備忘錄。奠基在已有的基因體研究平臺此良好基礎上，台灣富士通提供於巨量資料及雲端應用之先進ICT技術，結合成功大學領先於台灣學界之農業及生物多樣性研究，雙方的合作將可突破農業與生物巨量資料分析上的瓶頸，加速生物基因體研究成果，有助於提升成大成為全球基因體巨量資料分析的頂尖大學。
此舉為產學合作的雙贏契機。透過建教合作方式，共同建置「巨量資料研究平臺」，並藉由舉辦研討會和創辦期刊等方式，培訓生物資訊人才和人員交流。
台灣富士通董事長池上一郎讚揚成大在農業方面研究的成果，他認為以成大豐富的研究經歷再與日本的經驗相乘，將可讓日本與台灣的合作可以提升到更高層次。

關於富士通 (Fujitsu)
富士通是提供全系列科技產品、解決方案及資訊服務的日系資、通訊科技(ICT)領導品牌。超過十七萬名富士通人在全球逾百個國家傾力協助客戶。憑藉著自身的經驗及整合ICT的能力，我們和客戶攜手塑造社會的未來。富士通集團(東京證券交易所上市代碼：6702)截至2014年3月31日為止，年度財務報告合併營收為4.8兆日圓(460億美金)。如需更多資訊，請瀏覽：www.fujitsu.com關於台灣富士通 (Fujitsu Taiwan)
台灣富士通為富士通集團台灣子公司，提供客戶整合先進科技、解決方案與服務的IT專業產品與技術。主要事業群包括系統整合、電腦週邊產品與國際採購中心。系統整合事業群提供伺服器平臺產品、商務資訊設備、系統解決方案、先進科技解決方案、以及IT資訊服務。電腦週邊產品則涵蓋筆記型電腦、印表機以及掃瞄器等產品。如需更多資訊，請瀏覽：www.fujitsu.com/tw






林鈺文 Ami Lin

 電話號碼: (02)2311-2255 ext.5814
傳真號碼: (02)2311-2277
 E-mail: ami.lin@tw.fujitsu.com
Company:台灣富士通股份有限公司





Press Release ID: 2014-08-04
Date: 2014-08-04
City: 臺北
Company:
	
	台灣富士通股份有限公司


    

分享此頁面






首頁




聯絡我們











產品

全系列伺服器ETERNUS儲存設備流通資訊設備筆記型電腦點矩陣印表機掃描器



支援與下載

LIFEBOOK掃描器印表機



富士通集團

歷史沿革富士通在台灣經營理念



Country Selector
Taiwan

變更國家












使用規範
保密聲明
聯絡我們
網站地圖




Copyright 1995 - 2017 FUJITSU








巨量資料 - 維基百科，自由的百科全書































 







巨量資料

維基百科，自由的百科全書


					前往：					導覽，					搜尋















本條目可能包含原創研究或未查證內容。（2014年8月8日） 
請協助添加參考資料以改善這篇條目。詳細情況請參見討論頁。 


巨量資料（英語：Big data[1][2][3]），指的是傳統資料處理應用軟體不足以處理它們的大或複雜的資料集的術語[4][5]。在總資料量相同的情況下，與個別分析獨立的小型資料集（Data set）相比，將各個小型資料集合併後進行分析可得出許多額外的資訊和資料關聯性，可用來察覺商業趨勢、判定研究品質、避免疾病擴散、打擊犯罪或測定即時交通路況等；這樣的用途正是大型資料集盛行的原因[6][7][8]。
截至2012年 (2012-Missing required parameter 1=month!)[update]，技術上可在合理時間內分析處理的資料集大小單位為艾位元組（exabytes）[9]。在許多領域，由於資料集過度龐大，科學家經常在分析處理上遭遇限制和阻礙；這些領域包括氣象學、基因組學[10]、神經網路體學、複雜的物理模擬[11]，以及生物和環境研究[12]。這樣的限制也對網路搜尋、金融與經濟資訊學造成影響。資料集大小增長的部分原因來自於資訊持續從各種來源被廣泛收集，這些來源包括搭載感測裝置的行動裝置、高空感測科技（遙感）、軟體記錄、相機、麥克風、無線射頻辨識（RFID）和無線感測網路。自1980年代起，現代科技可儲存資料的容量每40個月即增加一倍[13]；截至2012年 (2012-Missing required parameter 1=month!)[update]，全世界每天產生2.5艾位元組（2.5×1018位元組）的資料[14]。
巨量資料幾乎無法使用大多數的資料庫管理系統處理，而必須使用「在數十、數百甚至數千台伺服器上同時平行運行的軟體」（電腦集群是其中一種常用方式）[15]。巨量資料的定義取決於持有資料組的機構之能力，以及其平常用來處理分析資料的軟體之能力。「對某些組織來說，第一次面對數百GB的資料集可能讓他們需要重新思考資料管理的選項。對於其他組織來說，資料集可能需要達到數十或數百TB才會對他們造成困擾。」[16]
隨著大資料被越來越多的提及，有些人驚呼大資料時代已經到來了，2012年《紐約時報》的一篇專欄中寫到，「大資料」時代已經降臨，在商業、經濟及其他領域中，決策將日益基於資料和分析而作出，而並非基於經驗和直覺。但是並不是所有人都對大資料感興趣，有些人甚至認為這是商學院或諮詢公司用來譁眾取寵的buzzword，看起來很新穎，但只是把傳統重新包裝，之前在學術研究或者政策決策中也有海量資料的支撐，巨量資料並不是一件新興事物。
巨量資料時代的來臨帶來無數的機遇，但是與此同時個人或機構的隱私權也極有可能受到衝擊，大資料包含各種個人資訊資料，現有的隱私保護法律或政策無力解決這些新出現的問題。有人提出，巨量資料時代，個人是否擁有「被遺忘權」，被遺忘權即是否有權利要求資料商不保留自己的某些資訊，巨量資料時代資訊為某些網際網路巨頭所控制，但是資料商收集任何資料未必都獲得用戶的許可，其對資料的控制權不具有合法性。2014年5月13日歐盟法院就「被遺忘權」（right to be forgotten）一案作出裁定，判決Google應根據用戶請求刪除不完整的、無關緊要的、不相關的資料以保證資料不出現在搜尋結果中。這說明在巨量資料時代，加強對用戶個人權利的尊重才是時勢所趨的潮流。




IBM對維基百科的編輯紀錄資料進行視覺化的呈現。維基百科上總計數兆位元組的文字和圖片正是大資料的例子之一






全球資訊儲存容量成長圖





目錄


1 定義
2 應用範例

2.1 巨大科學
2.2 科學研究
2.3 衛生學
2.4 公共部門
2.5 民間部門
2.6 社會學


3 市場
4 相關條目
5 註釋
6 參考文獻
7 延伸閱讀
8 外部連結



定義[編輯]
巨量資料由巨型資料集（英語：Data set）組成，這些資料集大小常超出人類在可接受時間下的收集（英語：data acquisition）、庋用（英語：data curation）、管理和處理能力[17]。巨量資料的大小經常改變，截至2012年 (2012-Missing required parameter 1=month!)[update]，單一資料集的大小從數太位元組（TB）至數十兆億位元組（PB）不等。
在一份2001年的研究與相關的演講中[18]，麥塔集團（META Group，現為高德納）分析員道格·萊尼（Doug Laney）指出資料增長的挑戰和機遇有三個方向：量（Volume，資料大小）、速（Velocity，資料輸入輸出的速度）與多變（Variety，多樣性），合稱「3V」或「3Vs」。高德納與現在大部份巨量資料產業中的公司，都繼續使用3V來描述大資料[19]。高德納於2012年修改對大資料的定義：「巨量資料是大量、高速、及/或多變的資訊資產，它需要新型的處理方式去促成更強的決策能力、洞察力與最佳化處理[原文 1][20]。」另外，有機構在3V之外定義第4個V：真實性（Veracity）為第四特點[21]。
巨量資料必須藉由計算機對資料進行統計、比對、解析方能得出客觀結果。美國在2012年就開始著手大資料，歐巴馬更在同年投入2億美金在大資料的開發中，更強調巨量資料會是之後的未來石油。
資料探勘（data mining）則是在探討用以解析巨量資料的方法。
應用範例[編輯]
巨量資料的應用範例包括大科學、RFID、感測裝置網路、天文學、大氣學、交通運輸、基因組學、生物學、大社會資料分析[22]、網際網路檔案處理、製作網際網路搜尋引擎索引、通訊記錄明細、軍事偵查、社群網路、通勤時間預測、醫療記錄、相片圖像和影像封存、大規模的電子商務等[23]。




應用於運動界


巨大科學[編輯]
大型強子對撞機中有1億5000萬個感測器，每秒傳送4000萬次的資料。實驗中每秒產生將近6億次的對撞，在過濾去除99.999%的撞擊資料後，得到約100次的有用撞擊資料[24][25][26]。
將撞擊結果資料過濾處理後僅記錄0.001%的有用資料，全部四個對撞機的資料量複製前每年產生25拍位元組（PB），複製後為200拍位元組。
如果將所有實驗中的資料在不過濾的情況下全部記錄，資料量將會變得過度龐大且極難處理。每年資料量在複製前將會達到1.5億拍位元組，等於每天有近500艾位元組（EB）的資料量。這個數字代表每天實驗將產生相當於500垓（5×1020）位元組的資料，是全世界所有資料來源總和的200倍。
科學研究[編輯]
衛生學[編輯]
國際衛生學教授漢斯·羅斯林使用「Trendalyzer」工具軟體呈現兩百多年以來全球人類的人口統計資料，跟其他資料交叉比對，例如收入、宗教、能源使用量等。
公共部門[編輯]
目前，已開發國家的政府部門開始推廣巨量資料的應用。2012年歐巴馬政府投資近兩億美元開始推行《巨量資料的研究與發展計劃》，本計劃涉及美國國防部、美國衛生與公共服務部門等多個聯邦部門和機構，意在通過提高從大型複雜的的資料中提取知識的能力，進而加快科學和工程的開發，保障國家安全。
民間部門[編輯]

亞馬遜，在2005年的時點，這間公司是世界上最大的以LINUX為基礎的三大資料庫之一[27]。
沃爾瑪可以在1小時內處理百萬以上顧客的消費處理。相當於美國議會圖書館所藏的書籍之167倍的情報量[6]。
Facebook，處理500億枚的使用者相片[28]。
全世界商業資料的數量，統計全部的企業全體、推計每1.2年會倍増[29]。
西雅圖文德米爾不動產（英語：Windermere Real Estate）分析約1億匿名GPS信號，提供購入新房子的客戶從該地點使用交通工具(汽車、腳踏車等)至公司等地的通勤時間估計值[30]。
軟銀，每個月約處理10億件（2014年3月現在）的手機LOG情報，並用其改善手機訊號的訊號強度[31]。
企業對巨量資料技能需求大，吸引了許多大學諸如伯克利大學開專門提供受過巨量資料訓練的畢業者的大學部門。矽谷紐約為主《The Data Incubator》公司,2012年成立，焦點是資料科學與巨量資料企業培訓，提供國際巨量資料培訓服務。

社會學[編輯]
大資料產生的背景離不開Facebook、微博等社群網路的興起，人們每天通過這種自媒體傳播資訊或者溝通交流，由此產生的資訊被網路記錄下來，社會學家可以在這些資料的基礎上分析人類的行為模式、交往方式等。美國的塗爾幹計劃就是依據個人在社群網路上的資料分析其自殺傾向，該計劃從美軍退役士兵中揀選受試者，透過Facebook的行動app收集資料，並將用戶的活動資料傳送到一個醫療資料庫。收集完成的資料會接受人工智慧系統分析，接著利用預測程式來即時監視受測者是否出現一般認為具傷害性的行為。
市場[編輯]
巨量資料的出現提升了對資訊管理專家的需求，Software AG、甲骨文、IBM、微軟、SAP、易安信、惠普和戴爾已在多間資料管理分析專門公司上花費超過150億美元。在2010年，資料管理分析產業市值超過1,000億美元，並以每年將近10%的速度成長，是整個軟體產業成長速度的兩倍[6]。
經濟的開發成長促進了密集資料科技的使用。全世界共有約46億的行動電話用戶，並有10至20億人連結網際網路[6]。自1990年起至2005年間，全世界有超過10億人進入中產階級，收入的增加造成了識字率的提升，更進而帶動資訊量的成長。全世界透過電信網路交換資訊的容量在1986年為281兆億位元組（PB），1993年為471兆億位元組，2000年時增長為2.2艾位元組（EB），在2007年則為65艾位元組[13]。根據預測，在2013年網際網路每年的資訊流量將會達到667艾位元組[6]。
相關條目[編輯]


資訊科技主題




資料探勘
資料庫
物件資料庫
關聯式資料庫
統計學
商務智慧型
分散式計算、分散式資料庫、分散式檔案系統、分散式運算環境
超級電腦
運籌學
MapReduce
合成作戰中心


註釋[編輯]


^ 原文：Big data are high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization.


參考文獻[編輯]


^ White, Tom. Hadoop: The Definitive Guide. O'Reilly Media. 2012-05-10: 3. ISBN 978-1-4493-3877-0. 
^ MIKE2.0, Big Data Definition. 
^ 巨量資料與進階分析解決方案.  已忽略文字「 Microsoft Azure 」 (幫助)
^ Kusnetzky, Dan. What is "Big Data?". ZDNet. （原始內容存檔於2010-02-21）. 
^ Vance, Ashley. Start-Up Goes After Big Data With Hadoop Helper. New York Times Blog. 2010-04-22. 
^ 6.0 6.1 6.2 6.3 6.4 Data, data everywhere. The Economist. 2010-02-25 [2012-12-09]. 
^ E-Discovery Special Report: The Rising Tide of Nonlinear Review. Hudson Global. [1 July 2012]. （原始內容存檔於3 七月 2012）.  請檢查|archive-date=中的日期值 (幫助) by Cat Casey and Alejandra Perez
^ What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology — Re-Humanizing Technology-Assisted Review. Forbes. [1 July 2012]. （原始內容存檔於18 六月 2012）.  請檢查|archive-date=中的日期值 (幫助)
^ Francis, Matthew. Future telescope array drives development of exabyte processing. 2012-04-02 [2012-10-24]. 
^ Community cleverness required. Nature. 4 September 2008, 455 (7209): 1. doi:10.1038/455001a. 
^ Sandia sees data management challenges spiral. HPC Projects. 2009-08-04. （原始內容存檔於2011-05-11）. 
^ Reichman, O.J.; Jones, M.B.; Schildhauer, M.P. Challenges and Opportunities of Open Data in Ecology. Science. 2011, 331 (6018): 703–5. doi:10.1126/science.1197962. 
^ 13.0 13.1 Hilbert & López 2011
^ IBM What is big data? — Bringing big data to the enterprise. www.ibm.com. [2013-08-26]. 
^ Jacobs, A. The Pathologies of Big Data. ACMQueue. 6 July 2009. 
^ Magoulas, Roger; Lorica, Ben. Introduction to Big Data. Release 2.0 (Sebastopol CA: O'Reilly Media). 2009-02, (11). 
^ Snijders, C., Matzat, U., & Reips, U.-D. (2012). ‘Big Data’: Big gaps of knowledge in the field of Internet science. International Journal of Internet Science, 7, 1-5. http://www.ijis.net/ijis7_1/ijis7_1_editorial.html
^ Douglas, Laney. 3D Data Management: Controlling Data Volume, Velocity and Variety (PDF). Gartner. [2001-02-06]. 
^ Beyer, Mark. Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data. Gartner. [2011-07-13]. （原始內容存檔於2011-07-10）. 
^ Douglas, Laney. The Importance of 'Big Data': A Definition. Gartner. [21 June 2012]. 
^ What is Big Data?. Villanova University. 
^ Erik Cambria; Dheeraj Rajagopal, Daniel Olsher, and Dipankar Das. 13. Big social data analysis. Taylor & Francis. 2013.  引文使用過時參數coauthors (幫助)
^ Hogan, M. What is Big Data. 3 March 2013 [2013-06-20]. 
^ LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public.. CERN-Brochure-2010-006-Eng. LHC Brochure, English version. CERN. [20 January 2013]. 
^ LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers.. CERN-Brochure-2008-001-Eng. LHC Guide, English version. CERN. [20 January 2013]. 
^ Brumfiel, Geoff. High-energy physics: Down the petabyte highway. Nature 469. 19 January 2011: 282–83. doi:10.1038/469282a. 
^ Layton, Julia. Amazon Technology. Money.howstuffworks.com. [2013-03-05]. 
^ Scaling Facebook to 500 Million Users and Beyond. Facebook.com. [2013-07-21]. 
^ eBay Study: How to Build Trust and Improve the Shopping Experience. Knowwpcarey.com. 2012-05-08 [2013-03-05]. （原始內容存檔於2012-06-19）. 
^ Wingfield, Nick. Predicting Commutes More Accurately for Would-Be Home Buyers - NYTimes.com. Bits.blogs.nytimes.com. 2013-03-12 [2013-07-21]. 
^ 柴山和久. ビッグデータを利益に変える方法. 幻冬舎. 2014. ISBN 978-4344952393 （日語）. 


延伸閱讀[編輯]

Big Data for Good (PDF). ODBMS.org. 2012-06-05 [2013-11-12]. 
Hilbert, Martin; López, Priscila. The World's Technological Capacity to Store, Communicate, and Compute Information. Science. 2011, 332 (6025): 60–65. PMID 21310967. doi:10.1126/science.1200970. 
The Rise of Industrial Big Data. GE Intelligent Platforms. [2013-11-12]. 
ISBN 978-986-320-191-5 《大數據》
ISBN 978-986-241-673-0 《雲端時代的殺手級應用：Big Data巨量資料分析》
IEEE Big Data Service. ODBMS.org. 2014-09-07 [2014-09-07]. 

外部連結[編輯]



維基共享資源中相關的多媒體資源：大資料





於維基詞典中查詢big data。



巨量資料的相關報導文章 （《Wired》中文網站）
處理巨量資料的挑戰（美國麻省理工學院線上課程）










閱
論
編


電腦科學






數學基礎

數理邏輯 · 集合論 · 數論 · 圖論 · 類型論 · 範疇論 · 數值分析 · 資訊理論






計算理論

自動機 · 可計算性理論 · 計算複雜性理論 · 量子計算 · 數值計算方法






演算法和資料結構

演算法分析 · 演算法設計 · 計算幾何






程式語言和編譯器

語法分析器 · 直譯器 · 編程典範（程序化編程 · 物件導向程式編程 · 函數語言程式設計 · 邏輯編程等）






並行、並列和分散式系統

多處理器 · 網格計算 · 並行控制






軟體工程

需求分析 · 軟體設計 · 程式設計 · 形式化方法 · 軟體測試 · 軟體開發過程






系統架構

電腦系統架構 · 微處理器體系結構 · 作業系統






電信與網路

路由 · 網路拓撲 · 密碼學






資料庫

資料庫管理系統 · 關聯式資料庫 · 結構化查詢語言 · NoSQL · 事務處理 · 資料庫索引 · 資料探勘






人工智慧

自動推理 · 計算語言學 · 電腦視覺 · 進化計算 · 專家系統 · 機器學習 · 自然語言處理 · 機器人學






電腦圖形學

視覺化 · 電腦動畫 · 圖像處理






人機互動

電腦輔助功能 · 使用者介面 · 可穿戴電腦 · 普適計算 · 虛擬現實 · 聊天機器人






科學計算

人工生命 · 生物資訊學 · 認知科學 · 計算化學 · 計算神經科學 · 計算物理學 · 數值演算法 · 符號計算







註：電腦科學領域也可根據ACM-2012分類系統進行分類。










 
						取自 "https://zh.wikipedia.org/w/index.php?title=大數據&oldid=44895418"					
7 個分類：資訊科學資料庫數據挖掘電腦數據電腦架構計算機科學信息革命隱藏分類：含有未命名參數的引用的頁面引文格式1錯誤：日期含有過時參數的引用的頁面CS1日語來源 (ja)自2014年8月可能帶有原創研究的條目拒絕當選首頁新條目推薦欄目的條目含有英語的條目使用ISBN魔術連結的頁面 



導覽選單


個人工具

沒有登入對話貢獻建立帳號登入 



命名空間

條目
討論




台灣正體



不轉換
簡體
繁體
大陸簡體
香港繁體
澳門繁體
馬新簡體
台灣正體






查看

閱讀
編輯
檢視歷史



更多







搜尋



 







導航


首頁分類索引特色內容新聞動態近期變更隨機條目 



說明


說明維基社群方針與指引互助客棧知識問答字詞轉換IRC即時聊天聯絡我們關於維基百科資助維基百科 



其他專案


維基共享資源 



列印/匯出


下載成 PDF 



工具


連結至此的頁面相關變更上傳檔案特殊頁面可列印版靜態連結頁面資訊維基數據 項目引用此頁面 



其他語言


العربيةAzərbaycancaБеларускаяBosanskiCatalàکوردیČeštinaDanskDeutschEnglishEspañolEuskaraفارسیSuomiFrançaisעבריתहिन्दीMagyarBahasa IndonesiaÍslenskaItaliano日本語한국어LietuviųLatviešuNederlandsNorsk bokmålPolskiPortuguêsRomânăРусскийසිංහලSimple EnglishСрпски / srpskiSvenskaதமிழ்తెలుగుไทยTürkçeТатарча/tatarçaУкраїнськаOʻzbekcha/ўзбекчаTiếng Việt 
編輯連結 





 本頁面最後修訂於2017年6月23日 (週五) 15:12。
本站的全部文字在創用CC 姓名標示-相同方式分享 3.0 協議之條款下提供，附加條款亦可能應用（請參閱使用條款）。
Wikipedia®和維基百科標誌是維基媒體基金會的註冊商標；維基™是維基媒體基金會的商標。
維基媒體基金會是在美國佛羅里達州登記的501(c)(3)免稅、非營利、慈善機構。


隱私政策
關於維基百科
免責聲明
開發人員
Cookie 聲明
手機版檢視



 

 






﻿
















雲端與巨量資料分析成熟度調查，台灣為亞太地區優等生 | TechNews 科技新報






























































搜尋：




EN
 簡
TW















        在粉絲團上追蹤我們










 
 





















雲端與巨量資料分析成熟度調查，台灣為亞太地區優等生




作者
MoneyDJ | 
                            發布日期
2014 年 12 月 11 日 19:01  | 
                                
                        分類

Big Data
																														, 										雲端

 | edit












































根據 IDC 與資策會創新應用服務研究所共同執行的「2014 年台灣雲端運算與巨量資料分析成熟度調查研究」發現，台灣在雲端與巨量資料分析的成熟度表現較其他亞太地區突出。從調查結果來看，交叉比較同時採用雲端運算與巨量資料分析的企業在巨量資料分析應用的成熟度為佳，其中，又以金融服務產業尤甚如此，這與金融服務企業對於客戶資料分析的需求較其他產業為高有關。 IDC 台灣市場分析師蔡宜秀表示，IDC 依照企業對新科技的應用成熟度與創造的商業價值將之區分做試驗（Ad Hoc）、專案（Opportunistic）、可重複（Repeatable）、可管理（Managed）與最佳化（Optimized）五個階段，根據本次調查結果，有超過七成的台灣企業在雲端運算與巨量資料分析的成熟度已從「試驗」階段進入「專案」階段，相較於亞太國家平均約四成企業處於「專案」階段來得突出。 展望未來，蔡宜秀表示，在國內外業者先後投入台灣市場後，IDC 預估台灣企業在雲端運算與巨量資料分析的成熟度將從「專案」階段邁向「可重複」階段，相關發展將更進一步。   在雲端運算成熟度這塊，根據調查結果顯示，有 76% 的台灣企業處於「專案」階段，較亞太國家平均僅 37% 企業進入「專案」階段的表現為佳。其中，政府與教育、流通服務、製造以及醫療產業有少數已建立明確的策略和發展藍圖而進入「可重複」階段。 IDC 台灣資深市場分析師吳乃沛表示，從企業工作負載來看，絕大多數的企業仍處於測試與評估有哪些工作負載適合轉移到雲端環境階段，預估在 2016 年前，企業以就地佈署（On-premises）方式佈署資訊基礎建設的比例將從 2014 年的 50.4% 降低為 42.6%，將有 7.8% 的工作負載轉移至雲端環境。 另外，在巨量資料分析成熟度方面，根據調查結果顯示，有 80.5% 的企業已進入「專案階段」，較亞洲國家平均僅 49.6% 企業進入專案階段為高。蔡宜秀指出，台灣企業在成熟度的表現之所以較亞洲地區（平均值）來得領先，主要是在「流程」與「人員」這兩個指標的表現較突出。 整體而言，IDC 表示，根據 2014 年台灣雲端運算與巨量資料分析成熟度調查研究，願意在雲端環境執行行銷活動、電子郵件、整合通訊與顧客關係管理等工作負載的企業較願意透過蒐集與分析內外部數據資料的方式萃取出能夠輔助決策下達的各種資訊，例如製程分析、精準行銷與客戶回饋等，亦即在雲端運算成熟度表現較佳的企業，在巨量資料分析成熟度的表現將呈現正向增長。 （本文由 MoneyDJ新聞 授權轉載；首圖來源：Flickr/FutUndBeidl BY CC2.0） 你可能有興趣的文章: 蘋果 Mac 美國市場成長或是衰退，調研機構不同調      為企業洞察先機！社群資料商 DataSift 推時事分析服務      甲骨文全面性雲端解決方案，助力台灣醫藥產業創新發展        阿裡巴巴進軍太陽能光伏產業，推出智慧型光伏雲        「怪我囉？」臉書研究撇清演算法造成言論濾泡挨轟        Google 雲端儲存服務再掀價格戰，搶 Amazon 用戶送 100PB 空間   prev next

關鍵字: big data , IDC , 巨量資料 , 雲端









發表迴響 取消回覆抱歉，你必須要登入才能發表迴響喔！ 
  







						我們偵測
						到您有啟用
AD Block


請您暫停使用AD Block，以支持我們持續能提供更多新聞資訊與優質的閱讀環境。



 



















贊助專欄
活動專區
研討會































 






本週熱門【一圖弄懂半導體】台積電與英特爾在追趕的奈米製程是什麼？ 





智慧手機正走向死亡，兇手就是科技巨頭自己 你家電視盒常看的影集都找不到了嗎？中國 A 站、B 站影片資源全被下架，日劇日影內容幾乎全滅 三大問題搞不定，傳 iPhone 團隊陷恐慌，出貨量恐下修 地球若毀滅，「牠」命超硬竟能活下來 攝影師因「自拍猴子」版權訴訟而面臨破產 鳳梨酥模式：後蘋果時代，台灣科技業最後一搏 （更新）HTC 預設輸入法出現推播廣告，引發怒火 

 






編輯精選

[ 專題 ]【一圖弄懂半導體】台積電與英特爾在追趕的奈米製程是什麼？
[ 專欄 ]【豐雲】玩具不死只會重生，孩之寶如何構建數位時代王圖霸業
[ 專題 ]【經濟科普】何謂產業轉型？從製造業迷思看臺灣經濟現象
[ 推廣 ]支付堵塞、監管曖昧不明，臺灣 P2P 網貸機會在哪？
[ 專欄 ]落實程式設計教育，需跳脫傳統考試評量、更不為單純培育軟體人才
[ 專題 ]Office 軟體到底該用哪個？各家格式標準為何？
[ 專欄 ]【豐雲】當亞馬遜揮軍進攻最終堡壘，通路末日戰役號角吹響
[ 專題 ]【金融史】貨幣寬鬆造就貧富差距外，還在 1720 年引起金融危機






FB 粉絲團 




其它 
登入
文章 RSS 訂閱
迴響 RSS 訂閱
WordPress 台灣正體中文 


 
1














							請您暫停使用AD Block，以支持我們持續能提供更多新聞資訊與優質的閱讀環境。
						








詹宏志：全世界第三方支付都以網路業者為主，台灣則偏重銀行業跟進 Intel 腳步，Samsung 14nm 製程開始投產




































巨量資料來襲 | iThome





















移至主內容















































 










技術文章


 

巨量資料來襲
 eBAY每天資料增加50TB、中華電信每月累積4TB紀錄、義大醫院醫療資料7年成長60倍，越來越多企業面臨PB級規模的巨量資料挑戰，你的企業準備好了嗎？

 






 
按讚加入iThome粉絲團









 




 
文/iThome
|
2011-06-24發表
  

 









 撰文☉王宏仁、辜雅蕾 攝影☉辜雅蕾巨量資料的超級挑戰來臨了eBay的資料庫每天增加50TB、中華電信每個月累積3～4TB記錄、義大醫院醫療資料7年成長60倍，越來越多企業面臨PB級規模的巨量資料挑戰分析師：複雜又巨量的資料為資料倉儲系統帶來新挑戰Gartner認為，資料倉儲的角色在2011年將面臨非常明顯的改變，來因應巨量、複雜、更多不同資料形式資料的挑戰，未來邏輯資料庫的概念也會逐漸浮現eBay如何分析巨量資料 用Hadoop分析巨量非結構性資料eBay在4年前就關註半結構性的資料分析，並打造專屬軟硬體平臺，藉由壓縮技術與Hadoop技術分析巨量的半結構資料中華電信如何分析巨量資料 組合168臺伺服器建立巨量資料運算平臺中華電信研究所去年1月完成名為「大資料運算平臺」的建置後，利用平臺測試大量非結構性資料如訊務資料與MOD收視率分析，進行加值應用Teradata的巨量資料對策：Aster Data 用結合SQL和MapReduce平臺處理Teradata以具有SQL-MapReduce技術的Aster Data平臺，處理非結構性資料。同時藉由含SSD的資料倉儲硬體，加快回應速度IBM的巨量資料對策：InfoSphere Big Insights 以多層次架構分析巨量非結構資料IBM透過另建平臺搜尋檔案系統資料，將檔案轉為資料庫格式後，由資料倉儲系統分析Oracle的巨量資料對策：Exadata v2 用儲存層工具過濾資料減少存取瓶頸Exadata v2在儲存硬碟系統中安裝儲存層工具軟體，預先過濾資料欄位，減少資料存取數EMC的巨量資料對策：Greenplum Greenplum可同時支援兩種技術Greenplum的資料庫引擎同時支援SQL與MapReduce，可直接讀取非結構性資料，將其當作外部資料表來進行分析巨量資料的超級挑戰來臨了
在eBay上，每天有上百萬件商品在線上交易，8,800萬名使用者平均每天查詢商品數百萬次，eBay資料庫系統每天會增加上1.5兆筆新記錄，合計每天增加的資料量超過50TB。為了提供各種交易媒合以及使用者行為分析，eBay的系統還得每天處理超過50PB的資料量，來進行5千多項商業分析，這正是eBay分析平臺高級總監Oliver Ratzesberger所面臨的挑戰，一個巨量資料（Big Data）的超級挑戰。巨量資料的挑戰並非只有規模如eBay般的企業才會面臨的問題。在臺灣，中華電信每個月保留的資料如用戶網頁瀏覽記錄等，大約有3～4TB的資料量，若要分析這些原始資料，過程中必須對資料進行多種複製和轉換，系統要處理的資料量還會再增加2倍以上，但中華電信現有資料倉儲設備僅能負荷6～9月所保存的資料量，若直接使用原始資料和傳統資料庫技術，難以進行長期或深入性的分析，來找出關鍵的用戶特性進行客製化行銷。如何分析龐大資料，是中華電信越來越嚴峻的挑戰。不只員工人數上萬人的中華電信，就連人數不到20人的臺灣社群遊戲廠商力可科技，也遇到資料暴增的難題，力可科技今年新推出的Facebook社群遊戲越來越受歡迎，記錄遊戲資訊的資料庫容量也快速暴增。數千名使用者在一天內產生的遊戲資訊高達2GB，1個月就會增加60GB的資料量，按照這個增長速度，不用幾個月，就會用完資料庫伺服器上的硬碟，力可科技總經理馮彥文既興奮又擔心。像社交遊戲這類的社交型網站，資料成長速度飛快，你可以想像假設一個社交網站隨時上線的使用者有1萬人，若這些人每天和10位朋友打招呼，一天就會產生10萬筆資料。若是Facebook，每天有5百萬名臺灣用戶上線，光是臺灣地區產生的資料量，每天至少有5千萬筆。就連醫院也有資料暴增的隱憂，例如義大醫院2004年4月成立後採取全面無紙化的策略，所有醫療影像，包括X光照片、斷層掃描等資料，甚至在導入電子病歷和心血管系統後，7年資料量增加了60倍，從2TB暴增到120TB，義大醫院預估，再過5年，資料量甚至會達到PB等級。 全球資料量2010年成長70億 GB麥肯錫全球研究中心今年5月發表了一份全球巨量資料研究報告，報告中指出，全球資料量光是在2010年就增加了70億GB，相當於4千座美國國會圖書館典藏資料的總和。巨量資料並非是新的議題，過去大多只有科學研究時，會遇到這類巨量資料的挑戰，例如高能物理分析、氣象預測、基因分析、生物研究、財務或商業資訊分析等，學者為了進行複雜現象的模擬和預測，經常利用平行運算或分散式運算技術來處理這樣大量的資料分析工作。 但現在，面臨資料暴增挑戰的企業陸續出現，也有越來越多的企業，擔心自己不久之後會面臨同樣的情況。現在很多提供全球性服務的網路業者如Google的搜尋服務、Flickr的圖片服務、Facebook的各種社交服務等，也都用巨量資料來描述他們所遭遇到的挑戰，再加上行動裝置普及後，光是2010年全球使用中的手機就有50億隻，許多新型態的資料蒐集工具也產生了大量資料，例如相機、麥克風、RFID讀取裝置、無線網路感應等，使用這些裝置提供服務的業者也同樣面臨了巨量資料的處理挑戰。這樣數10TB甚至是PB等級的資料量無法一次儲存在單一硬碟上，必須分散儲存，一般程式常用的關聯式資料庫架構處理這樣龐大資料時的效率不佳，遠超過傳統常見的資料庫管理工具所能處理的資料量。因為資料規模龐大也連到造成了後續進行資料擷取、保存、使用、分享以及分析時的處理難度。目前常見的巨量資料例如像網站Log記錄、RFID資訊、感測器網路資訊、社交網路的互動資料、網際網路上的文件、影音圖片、網路搜尋索引、客服中心呼叫記錄、天文資料、大氣科學分析、基因資料、生化分析、複雜或跨學科的科學分析、醫學記錄，甚至是生產線機臺設備產生的Log記錄。這些大多是非結構性資料，不容易以傳統關聯式資料庫的作法，透過固定資料欄位架構，將資料儲存到關聯式資料庫中來進行處理。除了巨量的挑戰以外，還有資料結構複雜化的挑戰。巨量資料衝擊資料倉儲技術升級這些巨量資料的挑戰帶來IT技術和商用軟體的變革，首當其衝的當然是資料庫和資料倉儲業者，Gartner副總裁Donald Feinberg表示，資料倉儲的角色在2011年時將面臨非常明顯的改變，來因應巨量、複雜、更多不同資料形式的挑戰。Donald Feinberg認為，資料倉儲的角色之所以會有所轉變，其中一項因素就是資料不僅在數量上變多，而且日益複雜，對於5到10年前所設計出來的資料倉儲系統來說，還必須能處理資訊的多樣性、複雜性、巨大的容量而且系統反應速度要即時等特性。而他認為，雖然巨量資料對不同產業都有不同的意義，但基本上，巨量資料代表的就是大量、複雜和非結構化的資料。巨量資料影響層面之廣，IDC軟體市場分析師鍾翠玲表示，巨量資料對於各方廠商都是新的戰場，但她認為，資料倉儲會是因應巨量資料的主力。一直以來，資料倉儲系統大多運用在OLAP（On-Line Analytical Processing）領域，著重在資料採礦、深度分析的功能。在過去，深度資料採礦與建模的工作並沒有立即性的需求，因此，相對於以交易為基礎的OLTP（On-Line Transaction Processing）領域，對時間與立即性的要求沒有那麼高。不過，隨著巨量資料時代來臨，資料量不僅大幅成長，資料的種類與態樣也越來越複雜，企業對於大量訊息快速分析後要立即給予反應的要求也越來越高。這些對專營OLAP資料倉儲的廠商來說，就帶來了效能與分析多樣性資料的能力都必須提升的挑戰。於是，舉凡壓縮技術、SSD硬碟等都紛紛出籠。而各家廠商面對巨量資料的因應策略也有所不同。Donald Feinberg表示，現今大多數廠商都在談巨量資料，並試著將此納入資料倉儲管理的環境中。他分析的幾個資料倉儲廠商分別是甲骨文、微軟、IBM、SAP/Sybase、Teradata、EMC的Greenplum等。他表示，甲骨文與微軟都處於剛開始談論巨量資料的階段，IBM則是靠著Big Insights進入巨量資料領域，Teradata收購的Aster Data被Donald Feinberg歸納為處理多樣性、複雜性資料的領導廠商，因此，他表示，該併購也讓Teradata一舉成為處理巨量資料的領導廠商之一。另外，儲存廠商EMC所併購的Greenplum也將MapReduce納入資料倉儲中，成為另一個處理巨量資料領導廠商。事實上，隨著資料量增加，已有OLTP領域的廠商推出軟硬體整合的產品跨足進資料倉儲的世界。3年多前甲骨文將自己最擅長的OLTP資料庫搭上昇陽的硬體，推出了一款資料倉儲產品Exadata，並且在第二版的儲存硬碟上加裝了儲存伺服器軟體，讓資料採礦計分的功能直接在儲存硬碟上進行，降低資料庫引擎的負擔。面對巨量資料的挑戰，甲骨文大中華區臺灣技術諮詢部資深諮詢經理黃久安表示，原先資料庫軟體其實就能處理許多複雜的資料來源，包括文本檔案，甲骨文推出軟硬體搭配的資料倉儲產品，就是因為企業資料量逐漸增加，必須藉由硬體去剋服為了分析所帶來的巨量資料。不過，雖然甲骨文的資料庫引擎可以做到半結構性資料的分析，並宣稱自己的資料倉儲系統可以同時提供OLAP與OLTP的功能，但是，這代表所有的資料都必須儲存在資料倉儲系統中才能分析，若以資料量計價的話，恐怕沒有很經濟。　
  

     ▲中華電信資訊處第四科科長楊秀一點出巨量資料對企業的真正挑戰是「最大的問題是沒有便宜的儲存方式。」所以，中華電信用Hadoop預先處理來減量資料，降低資料倉儲的負擔。　
中華電信資訊處第四科科長楊秀一就指出巨量資料對企業真正的難題是，「最大的問題在於沒有便宜的儲存方式。」他說。這使得Teradata與IBM Netezza都用另一個平臺先處理大量非結構或是半結構性的資料來達到減量效果。Teradata專門用於儲存分析巨量資料的平臺是併購來的Aster Data，IBM則是採用自家開發的InfoSphere來儲存與分析巨量資料。資料倉儲結合Hadoop運算技術 
 Hadoop是目前最受歡迎的高擴充雲端運算技術，不論企業或IT廠商都使用Hadoop來處理巨量資料，圖為Hadoop提供的網頁管理介面，可以監控每一個運算節點的執行情形。
這些平臺的特色都是採用了Hadoop開源雲端運算平臺的MapReduce處理技術，來解決關連式資料庫無法分析非結構性資料的困境。像是Aster Data就能同時支援使用者透過標準SQL來進行MapReduce的處理，可先透過MapReduce找出非結構性資料中的有用的結構性資料，減少資料量後，再由傳統資料倉儲接手分析。MapReduce是Google用來分析龐大網路服務資料的關鍵技術，在2005年釋出後，開源社群改用Java實作出MapReduce技術的開發平臺Hadoop。MapReduce的基本概念其實不難懂，用一個真實的數錢幣故事來解釋。有位企業主為了刁難銀行，用50元硬幣和10元硬幣償還316萬元的貸款，數萬枚硬幣重達1公噸，還得找來弔車才能送到銀行，幾位行員七手八腳花了好幾個小時才清點完畢。銀行只要不斷加派人手，就能縮短清點時間，例如能立即找到100個人手，10分鐘內就能完成，不會影響到正常銀行運作。就像這個不斷加派人手來清點錢幣的做法一樣，MapReduce可以不斷增加更多伺服器來提高運算能力，增加可承載的運算量。透過Map程式將資料切割成不相關的區塊，分配給大量電腦處理，再透過Reduce程式將結果彙整，輸出開發者需要的結果。資料倉儲業者正是利用Hadoop的高擴充分散式運算技術，來解決巨大資料的難題。不過，另外建置一個平臺，除了增加建置成本之外，當然也就多一道資料存取的工作，資料的正確性也就成了另一個風險。以單純軟體起家的Greenplum雖可同時支援SQL與MapReduce，直接讀取企業存取資料的儲存設備中的資料，並且在計價時只計算保留在資料倉儲空間中的資料量。過去Greenplum一直都是以販售軟體為主，讓企業自行搭配硬體。去年第三季他們也推出了軟硬體整合的產品。就目前而言，各家廠商認為臺灣企業對於分析結構性資料的需求仍舊大於非結構性資料，而龐大資料的產生，則是正在發生中的事情，多數企業開始面對巨量資料的問題，都是在為了未來而打算，而不是現在就面臨的困難。低成本的NoSQL資料庫技術倒是網路業者更快面臨了巨量資料，例如先前提到的力可科技，或像eBay、Facebook、Google等，他們為瞭解決PB等級的資料儲存和擴充問題，也開始研發建置成本較低的分散式開源資料庫，也就是所謂的NoSQL資料庫。這類技術可使用個人電腦等級的伺服器，以較低的成本，但大量的設備來撐起巨量資料需要的儲存規模和分析處理能力。像是Google自行研發的BigTable就是最好的例子。其他如Amazon、Yahoo、Facebook、Twitter也都投入這類NoSQL資料庫的研發。甚至連微軟Azure雲端平臺也使用了NoSQL技術來存取資料。而像Facebook也開發了一套NoSQL資料庫Cassandra，在600多個運算核心的叢集系統上，儲存了超過120TB的站內郵件資料。 目前主要有4種NoSQL資料，包括Key-Value資料庫，記憶體資料庫（In-memory Database）、圖學資料庫（Graph Database）以及文件資料庫（Document Database）。Key-Value資料庫是NoSQL資料庫中最大宗的類型，這類資料最大的特色就是採用Key-Value資料架構，最簡單的Key分析師：複雜又巨量的資料為資料倉儲系統帶來新挑戰
分析機構Gartner副總裁Donald Feinberg表示，資料倉儲的角色之所以會有所轉變，其中一項因素就是資料不僅在數量上變多，而且日益複雜，對於5到10年前所設計出來的資料倉儲系統來說，就必須要能處理資訊的多樣性、複雜性、巨大的容量而且系統反應速度要即時等特性。而他認為，雖然巨量資料對不同產業都有不同的意義，但基本上，巨量資料代表的就是大量、複雜和非結構化的資料。但是，對於擅長處理結構化資料的關聯式資料庫管理系統來說，是很難去處理巨量資料的。因此，Donald Feinberg表示，目前大部分的企業會特別打造一個空間或是平臺來存放非結構化資料或是巨量資料。因應巨量資料的來臨，Donald Feinberg表示，未來邏輯資料倉儲的概念將會浮現，也就是將不同性質的資料存放在不同的資料庫中，就可以用適當的工具來獲取正確的資訊，同時，邏輯資料倉儲會利用適當的後設資料連結所有資料倉儲系統中不同的資料。Donald Feinberg也表示，有許多工具都是用來處理巨量而非結構化的資料，但是，將有很多應用程式是使用MapReduce技術開發。資料倉儲廠商面臨必須快速回應查詢的挑戰巨量資料影響層面之廣，IDC軟體市場分析師鍾翠玲表示，巨量資料對於各方廠商都是新的戰場，其中也包含了儲存廠商，像是EMC買下資料倉儲軟體業者Greenplum就是一例。原因正是，她認為，資料倉儲的確是可因應巨量資料的主力。不過，對資料倉儲廠商來說，還是有不少挑戰存在，首當其衝的是，他們必須要強化關聯式資料庫的效能，增加資料管理和資料壓縮的功能。因為過往關聯性資料庫產品處理大量資料時的運作速度都不快，需要引進新技術來加速資料查詢的功能。另外，資料倉儲的廠商也開始嘗試不只採用傳統硬碟來儲存資料，像是使用快閃記憶體的資料庫、記憶體式資料庫等，都逐漸產生。另一個挑戰就是傳統關聯性資料庫無法分析非結構化資料，因此，併購具有分析非結構化資料的廠商以及資料管理廠商，是目前資料倉儲大廠擴增實力的方向。資料管理的影響主要是資訊安全的考量。IDC軟體市場分析師吳乃沛表示，巨量資料對於儲存技術與資訊安全也都會產生衝擊。首先，快照、重覆資料刪除等技術在巨量資料時代都很重要，就衍生了資料擷取權限的管理。舉例來說，現在企業後端與前端所看到的資料模式並不一樣，當企業要處理非結構化資料時，就必須界定出是IT部門還是業務單位才是資料管理者。而吳乃沛表示，由於這牽涉的不僅是技術問題，還有公司政策的制定，因此界定出資料管理者是臺灣企業目前最頭痛的問題。文⊙辜雅蕾eBay如何分析巨量資料 用Hadoop分析巨量非結構性資料
對於臺灣企業來說，巨量資料的分析與應用大多採取較為保守的態度，即便看到巨量資料的來臨，還是著重於結構性資料的應用。這可能是受臺灣的市場規模所限，相對來說，國外的許多大型企業則是已經看到不少巨量資料的挑戰，並且試著用更好的效能或是管理來解決。比如以麥當勞來說，他們因為近年來在世界各地越來越多產品品項，每一個品項在各地的銷售狀況，都會再傳回美國母公司的資料庫進行分析，為了優化資料庫的效能，藉由管理工作負載的軟體來改善資料回應的速度。但對擁有全世界最大資料倉儲系統的eBay來說，挑戰不僅於此。eBay近年來數據成長的腳步相當驚人。他們現在每天有50PB的新增資料，使用者平均每天對5萬種商品進行數百萬次的網上查詢。除此之外，eBay還有7,000多個商業用戶和分析人員，為了分析，每8秒鐘會產生1TB的資料量。這些加起來，eBay每天要處理的資料量高達100PB。巨量資料最大的挑戰是要同時處理結構與非結構性資料這麼驚人的數字對eBay 來說並不只是量的增加而已。eBay分析平臺高級總監Oliver Ratzesberger認為，近年來在分析資料領域最大的挑戰就是要同時處理結構化與非結構化的資料。eBay的非結構化資料主要是來自行為分析的數據、以及網站點擊率的分析。Ratzesberger表示，這些資料都比過去還要複雜、多變許多。就拿追蹤點擊率來說，近年來大部分的網頁都是動態網頁，過去只要透過網址就能知道使用者正在看什麼網頁，但是現在一個網頁上的內容變多了，而且每一秒都在變化，這對於分析使用者行為來說，難度也就更高。Ratzesberger認為，目前關聯式資料庫已經能夠將結構性資料處理的非常好，不過，在半結構性資料或非結構性資料的處理能力還不行，而這些點擊率資訊就是這一類的資料。運用Hadoop技術將大塊資料打散，可讓資料模型更小因此，eBay在4年多前另外建立了一個軟硬體整合的平臺Singularity，在這個平臺上他們開始開發壓縮技術與解決結構、半結構資料的技術。2年多前這個平臺上加入了Hadoop的技術，專門處理半結構與非結構性的資料。Ratzesberger表示，現在eBay在這個平臺上處理的結構與非結構性資料已經達到40PB。這個平臺除了可以儲存大量的結構與非結構性資料外，藉由Hadoop 的技術處理大塊的非結構性資料，當資料庫引擎出現大量的請求與查詢時，就不需要將整個資料放到資料模型中，因此讓資料模型變得較小。在eBay的作法上，他們先為點擊率中的共同特性設定欄位，像是IP位址、時間、URL、使用者所使用的瀏覽器等，再進行交叉分析。這些欄位仍舊保留，其他較複雜的非結構化訊息則集中在另一個欄位中，一旦出現查詢需求時，就可以經過相對應數值的匹配，找到符合的概念與資料，再應用到資料模型中。而這些資料都會放在同一個資料表中。eBay充分運用所能蒐集到的資料加以分析，並不是IT技術上的變革而已，這些工作，也確實對eBay產生了不少正面的影響。比如說，頂級賣家的銷售額占所有賣家銷售額的百分比從2009年10月的22％，到2010年4月時成長為32％，一共上升了10個百分點。Ratzesberger表示，從分析使用者行為中得出來的介面設計經驗，也優化了網頁結構，提高銷售額。應用虛擬化的資料超市來減少重覆複製資料其實分析本身就是資料量不斷增加的一個重要因素。每進行一次分析，用於分析的資料就會再被複製一次，使得資料量不斷成長。但是，企業也必須使用資料倉儲進行深度分析，惟有深度分析才能創造出與其他企業區隔的獨特之處。為了減少資料不斷複製的情況，Ratzesberger就提出了分析作為服務（Analytics as a Service）的觀念。作法是提供虛擬化的資料架構，也就是將資料超市（Data Mart）虛擬化，這些資料超市就像是一種資料次集合。當eBay旗下各單位有資料分析需求時，可以用自助服務申請專屬的資料次集合。Ratzesberger表示，過去當業務單位需要分析資料時，這些資料可能來自於不同的資料庫，而這些資料庫也不能共享，資料產出的分析會存在一個資料庫中，當另一個單位需要分析時，就要將類似的資料再複製一次，導致許多資料為了不同的分析目的而被一再地複製。資料架構虛擬化之後所有的資料都儲存在統一的資料倉儲中，當任何單位有需要分析服務時，這些資料超市都是共享同一個資料倉儲中的資料，而不需要複製，只有特別具有保存價值的分析結果資料，才另外進行儲存。這樣做也可以解決資料庫使用效率不彰的問題。Ratzesberger表示，過去不同部門的資料超市都各自儲存，使得有些資料庫的效能浪費掉，有的資料庫效能又不敷使用。「在資料架構虛擬化之前，資料庫有一半以上的性能都浪費掉了。」他說。舉例來說，在早上9點時，是行銷與IT部門的使用率最高，10點時，又變成行銷、財務、搜尋的使用率最高，到了下午1點，詐欺跟客戶服務的使用率變得最高。利用資料架構虛擬化的作法，就能在不同時間彈性調配不同的資料庫運算資源給不同單位使用。現在eBay還讓企業內部的員工可以自助服務，申請自己需要的資料超市。這個虛擬的資料超市可以隨時調配需求，也具有簡單的管理功能。未來，eBay也關註於社群網站的言論分析，配合行動裝置銷售版圖的擴展進行更多行動應用的優化，像是藉由行動裝置拍攝逛街看到的衣服照片，就可以立刻搜尋拍賣網站是否有相同物件等功能。因此，從今年開始，eBay還要進一步把Hadoop的技術整合進資料倉儲中，協助分析大量的資料，以因應未來持續增加、越來越複雜的資料量，以及更即時的回應。文⊙辜雅蕾 
     eBay將資料庫中的資料架構虛擬化，讓資料庫資源可彈性配置，也減少因分析造成的大量資料複製的資料量，各單位也可以透過自助服務申請所需要使用的資料超市（Data     Mart）。
中華電信如何分析巨量資料 組合168臺伺服器建立巨量資料運算平臺
雖然對很多臺灣企業來說，巨量資料仍被視為未來才會逐漸發生的事情，但實際上，臺灣電信業者確實已開始發展處理巨量資料的方法。企業分析巨量資料的目的，多半是為了分析客戶行為，並針對這些行為給予主動行銷。以過去常運用資料倉儲分析資料的金融業來說，現在有更多即時性的資料出現，若能結合歷史性跟即時性的資料進行綜合分析，就能讓應用資料的效益更高。現在臺灣也有電信業將巨量資料的分析結果應用在預測駭客攻擊的領域。由於一般駭客在正式攻擊之前，會先嘗試攻擊不同的伺服器，這些攻擊都會在系統的Log中留下軌跡。在Log會有一段敘述，用來描述錯誤訊息，電信業者就藉由可同時處理結構與非結構性資料的資料倉儲系統來處理這些半結構性資料，利用這些錯誤訊息建立模式，進而預測駭客攻擊提早加以預防。顯見得巨量資料在應用領域上已經逐漸多元化。為瞭解決這些巨量又大型的非結構性資料，各個企業作法並不同，但Gartner副總裁Donald Feinber表示，大部分的企業會特別打造一個空間或是平臺來存放並分析這些非結構化資料或是巨量資料。中華電信的作法也是如此。中華電信研究所為了配合中華電信的需求，在2010年1月建置了一個以Hadoop技術為核心的平臺，稱為「大資料運算平臺」，用來分析一些訊務資料、MOD每日收視率分析等。中華電信研究所寬網室研究員蕭毅表示，目前這個平臺仍在研發階段，會針對中華電信內部的需求來開發平臺功能，接下來，將進一步對內實際提供加值應用，未來也打算將技術包裝後，對外提供巨量資料平臺運算的服務。開發巨量資料元件庫，即使不會Hadoop技術也能使用平臺資料
中華電信「大資料運算平臺」組成要素
1. 168臺伺服器叢集，容量600TB。    2. Hadoop雲端運算技術為核心。    3. 平臺使用度模組化工具，如分散式資料庫系統、工作排程、流程管理、資料庫介接工具。    4. 平臺維護效率模組化工具，如效能監測、告警通知、組態管理等。    5. 「大資料元件庫」提供API，簡化開發。
這個「大資料運算平臺」目前是由168臺伺服器所組成，資料容量是600TB，以Hadoop雲端運算技術為核心架構。在底層核心架構上，中華電信研究所再利用開放原始碼各種技術開發了其他模組，像是分散式資料庫系統、工作排程、流程管理、資料庫介接工具等來提高平臺的可使用度。同時，為了提升平臺的維運效率，中華電信研究所開發了管理模組、效能監測、告警通報、組態管理等一般網管所具有的機制。另外，中華電信研究所也開發了一個「大資料元件庫」。蕭毅表示，元件庫就如同中介軟體，藉由這些元件庫的API，開發人員不一定要學會用Hadoop技術來寫程式，就能使用平臺上的資料分析功能。目前這個元件庫底下約有6種元件庫，包括MOD元件庫、影音搜尋元件庫、影像辨識元件庫等。蕭毅表示，當巨量資料平臺所群組起來的伺服器越多時，就表示要處理資料量越大，相對來說，管理能力就一定要增強，而目前中華電信研究所開發的「大資料運算平臺」程式功能已具有管理200臺伺服器的能力，未來若要商用化，這些能力都會持續擴增，讓可運算的資料量達到PB等級。開發介面程式可相互轉換結構與非結構性資料，讓資料的分析更具有彈性中華電信研究所會想要開發這個巨量資料運算平臺，有一個原因就是想要利用Hadoop來處理過去關聯性資料庫不容易處理的大量非結構化資料。非結構化資料通常都呈現非常大型的狀態，就好像是一整篇文章，而不是經過篩選的資料欄位，蕭毅稱為「一尾式」的資料。蕭毅表示，這種「一尾式」的資料，如果要變成結構性資料，第一個問題是由於資料太過龐大，讓資料庫的費用大幅增加。第二個問題是，在資料量太大的時候，傳統資料庫難以連接不同的資料表。因此，「一尾型的資料因為傳統資料庫無法運算，就改放入Hadoop平臺運算，若原本是簡化過的結構性資料，才放進原來的資料庫分析。」他說。像之前中華電信研究所曾經利用傳統資料庫來計算MOD每日收視率分析，結果由於資料量過大而無法分析，建構了「大資料運算平臺」以後，運用Hadoop 技術只需1個小時就能計算出結果。蕭毅認為，如果硬體的數量再增加，分析所需要的時間甚至可以在幾分鐘內完成。雖然巨量資料運算平臺可以解決關聯性資料庫無法分析非結構性資料的難題，但蕭毅認為，只要是200TB以內的資料量都還是可以靠傳統資料庫分析，但超過PB等級資料量的話，傳統的資料庫恐怕就無法負荷。因此，中華電信研究所也在這個平臺上開發了介面程式來轉換結構與非結構性資料，可以將非結構性資料轉成結構性資料分析，也可以把原先結構性資料打散變成非結構性資料，再讓Hadoop技術做倒立式的搜尋。建立培訓機制協助技術人員學習Hadoop技術雖然開發這個平臺為中華電信研究所帶來許多嶄新的測試經驗，但在開發過程中，他們遇到最大的困難就是嚴重的技術瓶頸。蕭毅表示，最大的挑戰在於NoSQL與Hadoop等巨量資料技術與傳統資料的設計觀念完全不一樣，必須訓練資訊人員接受新方法，而難題就在於說服資深的技術人員學習新技術。「一旦採用Hadoop技術，原始程式和應用系統功能都要修改，要讓技術人員重新學習，這就是我們的困難。」他說。中華電信要自己開發元件庫也是為瞭解決這個難題。不過，在這個元件庫下還是得開發不同專業領域元件庫，這時候需要該領域的專家以及能用Hadoop技術開發程式的技術人員共同合作才能完成。就算可以使用元件庫來簡化開發，對於技術人員來說，要從本來傳統資料庫的元件庫轉換，也會出現轉換障礙。而且，蕭毅表示，即使可以利用元件庫，技術人員還是直接用Java寫MapReduce的分析程式，在運作效能與速度都會比較好，技術人員還是得熟悉Hadoop技術。因此，中華電信研究所建立了巨量資料技術的專長培訓機制，來協助技術人員的轉換技能，也為了訓練各應用領域的系統設計人員，建立新的巨量資料架構觀念擁有自行評估設計、規畫、開發的技術能力。文⊙辜雅蕾
Hadoop技術的優缺點比較
    中華電信研究所從2010年1月建置好「大資料運算平臺」後，陸陸續續在這個平臺上測試了不少工作，包含訊務的分析以及MOD收視率分析等。    綜合這1年半來運用Hadoop技術的經驗，中華電信研究所認為，Hadoop技術各有幾項優缺點。    第一個優點是，運用Hadoop技術可以節省成本。中華電信研究所寬網室研究員蕭毅表示，開發巨量資料運算平臺的硬體很便宜，軟體也是免費的，如果要計算一樣的資料量來說，只需要支付用傳統資料庫計算時價格的10分之1。        第二個優點是，減輕程式維護人力。蕭毅表示，過去企業要維護一個傳統資料庫，需要大量的專業人力，尤其是要計算PB等級的資料量時，資料庫會變得更加複雜，加上備份的機制，對於程式維護來說是很大的負擔。但利用Hadoop技術，由於每一筆資料體積龐大，只要利用類似使用搜尋引擎的功能就可以找到資料，而且Hadoop也會自動備份3次，大大減低程式維護的人力。        另外，中華電信研究所也看好Hadoop技術將成為Java程式語言主流架構，可以同時支援開發單機版，或是多機版。    同時，他們也提出幾項目前採用Hadoop技術的缺點，其中有一些也是正在解決的問題。        第一個缺點是，安全性不足的問題。蕭毅解釋，中華電信研究所未來想要讓這個「大資料運算平臺」上擁有多租戶的功能，但是如果要將這些用戶隔開，就必須自行開發程式或使用不同的硬體來區分不同的用戶，不過，他認為，這對於單一企業應用此技術開發單一平臺來說，就不會有這個問題。        第二個缺點是程式開發人員必須要學習MapReduce架構才能在Hadoop平臺開發程式，而且沒有針對各行各業需求打造的元件庫。    第三個缺點則是各版本功能的差異較大，容易造成應用程式相容性的問題。蕭毅表示，為瞭解決這個問題，在開發的過程中就必須加進許多管控機制才能讓程式運作更順暢。        第四個缺點則是管理機制大多為指令介面，缺乏友善的圖像管理介面。事實上，蕭毅認為，對專業的開發人員來說，指令介面並沒有什麼問題，不過對於一般的人來說，容易產生距離感。文⊙辜雅蕾
Teradata的巨量資料對策：Aster Data 用結合SQL和MapReduce平臺處理
專註在OLAP領域的資料倉儲廠商Teradata，一直以來都是以MPP（Massive Parallel Processing）技術分析結構性資料，但巨量資料中混合存在的結構與非結構資料，使得資料的管理分析困難度便高，難以用原先的技術進行分析。為了藉由第三方工具協助，他們在去年12月買下行銷軟體公司Aprimo，用來分析及管理來自網站、客服中心、郵寄廣告或社交網路等不同管道的資料，強化行銷業務領域的分析能力，又在今年買下了Aster Data強化分析非結構性資料的能力。面對巨量資料中的非結構性資料，由於Teradata的資料庫引擎仍沒有處理非結構性資料的能力，在Teradata買下Aster Data以後，資料處理的架構則是將結構性資料與非結構性資料先儲存在Aster Data的平臺上，再藉由Aster Data來支援關連性資料庫SQL語法，以及可處理非結構性資料的MapReduce函式庫，將非結構性資料中可利用的訊息，轉變成結構性資料，進一步分析並建立資料模組，再交由Teradata資料倉儲系統進行關聯式分析。Teradata大中華區專業服務副總經理張錦滄表示，目前Aster Data已預先提供了80個分析模組，針對多種資料源，包括圖形檔、臉書上的文字和感應器收集來的資料，企業不需要自行開發全部的分析模組。同時，該平臺也與Teradata資料庫完成初步整合，藉由開放API，Teradata資料庫可直接用SQL指令呼叫Aster Data平臺分析所產生的資料集（Dataset），並進行分析；在Aster Data平臺上也可取出Teradata資料庫的資料進行分析整合。另外在資料處理的效能強化上，Teradata也在今年4月底推出全新Active EDW 6 系列的SSD與硬碟混合型的資料倉儲來提高資料處理的速度，解決巨量資料需要更即時反應查詢。這種混合型的資料倉儲系統，主要是藉由Teradata的虛擬存儲軟體（TVS）依照企業內部資料使用的頻率來自動分配，讓較常用的的資料置於SSD中處理，較少用的資料則放在一般硬碟中處理，讓平常越常使用資料，能獲得更快的資料處理效能。Active EDW 6680搭配300GB到600GB的硬碟，加上300GB的SSD，資料量可從7TB擴充到36PB。Active EDW 6650則是搭配300GB到450GB的硬碟，企業可自行選擇是否升級到SSD，資料量可從7.5TB擴充到92PB。文⊙辜雅蕾IBM的巨量資料對策：InfoSphere Big Insights 以多層次架構分析巨量非結構資料
面對巨量資料，IBM是以去年併購的資料倉儲廠商Netezza的資料倉儲產品TwinFin應戰。IBM軟體事業處資訊工程顧問莊惟欽表示，專責OLAP的資料倉儲與專責OLTP的資料庫，其應用與強項各有不同，但近年來，單純記錄交易資料已經無法區隔出企業的競爭力，因此，資料倉儲就成為企業發揮獨特優勢的工具。IBM針對巨量資料共有3種產品線，第一種是針對傳統關聯性有結構的資料，像是企業內部一般的ERP、CRM、採購等所產生的資料，第二種則是非傳統、非關聯的資料源，比如智能電網、股票交易等，都是在短時間內產生大量數據的系統，其中也包含了影像。第三種則是巨量資料，這種資料來源通常是非結構性以及半結構性，資料的格式也很多元，像是個人部落格、文件檔案等。在資料處理的架構上，第一種資料則是藉由資料庫或是資料倉儲系統處理。第二種資料則是會藉由InfoSphere Streams處理感應器接收的資料，並不以儲存為目的，而是要快速回應，盡快提供分析結果。第三種資料，則是透過InfoSphere Big Insights平臺當作搜尋引擎，搜尋分析時所需要的文本資料。InfoSphere Big Insights平臺可作為資料的存放空間，運用Hadoop中MapReduce的技術，搜尋存放在該平臺內部的檔案資料，但該平臺僅能進行簡單分析，例如文字敘述中的偏好等，企業若要進一步與歷史資料比對交叉分析的話，還是要靠底層資料庫架構將資料轉換成資料庫格式，再放到資料倉儲中運算。莊惟欽表示，MapReduce可以搜尋影音、文件、語音等檔案，但是在應用上，仍還是以文件為主。TwinFin為了快速回應查詢，搭配了FPGA硬體分析伺服器，來負責大量資料解壓縮、欄位縮減過濾等動作，如同在硬碟與主機資料庫中加裝了加速卡，解決巨量資料從磁碟取出放入主機伺服器記憶體中分析的效能瓶頸，也讓資料存取的速度變快。另外，莊維欽還強調TwinFin可將進階分析軟體如SAS、SPSS直接內建在資料倉儲的系統中，並支援平行處理進階分析軟體。文⊙辜雅蕾Oracle的巨量資料對策：Exadata v2 用儲存層工具過濾資料減少存取瓶頸
甲骨文Exadata v2是整合了昇陽的硬體與甲骨文資料庫的資料倉儲系統。Exadata v2的特色是在儲存層就安裝了可過濾資料的儲存伺服器軟體（Oracle Exadata Storage Server Software），讓儲存伺服器可以先處理部分資料，壓縮後傳給資料庫伺服器，減少往來傳輸的資料量，降低處理器的負擔。另外，壓縮技術使得系統可儲存的資料量更大，加上以快閃記憶體做為快取（cache），提高了系統的效能。甲骨文大中華區臺灣技術諮詢部資深諮詢經理黃久安表示，不同於其他資料倉儲是將全部的資料都傳到資料庫引擎中運算，甲骨文的資料倉儲系統的特色就是在儲存系統中加裝了可過濾資料的儲存伺服器軟體，而這套軟體最大的功能就是過濾資料欄位，減少資料存取時的瓶頸。簡單來說，就是Exadata v2運用儲存系統上的軟體先過濾出部分資料，產生一部份的資料超市（Data Mart）後，資料庫引擎就只需要進行統合的工作，也大大減少了資料庫主機伺服器的工作。另外，藉由Exadata v2採用自家資料庫軟體來處理半結構性資料或是非結構性資料，直接利用甲骨文資料庫中的XML DB產品，把文件檔案當作一種XML網頁的格式，只要在文件內容貼上索引用的標籤，資料庫就可以全文檢索，並能傳到資料庫進行分析。黃久安表示，傳統資料庫軟體比起原本資料倉儲系統更擅長處理半結構性資料，不需要多層次處理。不過，若要處理的資料來源是企業外部的資料，就得利用甲骨文的企業搜尋軟體Secure Enterprise Search來進行搜尋。但只要是企業想分析的資料都得放進資料倉儲的儲存空間中，才能進行分析，成本難免增加。黃久安表示，目前臺灣已有4家企業採用Exadata資料倉儲系統，其中一家高科技製造業已經上線，而他認為，甲骨文的資料倉儲系統可同時執行OLTP與OLAP，當巨大資料不斷產生時，許多資料其實是混合型產生的，比較符合企業的實際狀況。舉例來說，臺灣的高科技製造業導入Exadata資料倉儲系統，主要是用來分析生產線上的狀況，然而，生產線上的資訊大多屬於OLTP，快速運算之後，再經由OLAP分析處理，一旦發現任何問題，如良率下降時，就能立即停止線上的生產，再進行調整。文⊙辜雅蕾EMC的巨量資料對策：Greenplum Greenplum可同時支援兩種技術
資料倉儲軟體Greenplum去年被儲存大廠EMC併購，原先僅有軟體產品，併購後旋即推出軟硬體整合的機櫃式產品，其中的DCA-1000，每個全櫃機櫃內含2臺主機伺服器，16個節點，搭配壓縮技術可達144TB的容量，最大可擴充到24個機櫃。同時，資料計價方式也從過去按照使用人數計算軟體授權費用，改由以資料倉儲系統中的資料量計費。EMC業務協理楊傑儂表示，Greenplum最大的優勢在於它是一個可以同時支援SQL跟MapReduce語法的資料庫引擎。因此，在處理非結構性資料時，Greenplum並不需要再另外建一個分析平臺，而是可以直接讀取存在企業內部儲存系統中的非結構資料，並進行分析。大量的非結構性資料無論儲存在企業的SAN架構或是NAS架構，Greenplum會將這些文件當作是外部資料表（External Table），利用在資料倉儲系統上開發的MapReduce程式分析這些資料表。楊傑儂表示，不過，僅需進一步利用的非結構性資料才儲存在資料倉儲系統中，計價時，也只計算這部分的資料量，能解決儲存非結構性資料成本高昂的問題。為了強化系統效能，Greenplum採用MPP（Massive Parallel Processing）架構，由2臺Master主機，和最多可擴充到上千臺的Segment主機，中間以網路連接。Master主機負責建立用戶端的連接和管理、SQL的解析與形成執行計畫、分發Segment執行計畫與收集結果等工作。資料的儲存、存取和SQL查詢的執行則由各個Segment負責。每個Segment都使用一般PC等級伺服器，上面會搭載Greenplum的資料庫軟體，各自擁有獨立的儲存硬碟、記憶體、CPU等。當執行查詢工作時，則是切分給各個Segment去執行，提高效能。楊傑儂認為，隨著企業有越來越多分析需求，資料查詢的工作越來越複雜，資料存取的速度就很重要。因此，Greenplum將用來選擇、萃取資料的ETL（Extract-Transform-Load）伺服器獨立出來，連接內部網路，這個ETL伺服器會自動將資料分散到讓每一個Segment來進行分析，加快資料存取的速度。文⊙辜雅蕾
 


































 




 







 Advertisement


 

 

更多 iThome相關內容


  
 新一代主流2路伺服器規格大更新 

 Intel推出首款採用Broadwell架構的Xeon處理器，內建10Gb網路晶片 

 活用混合雲策略，重新定義IT服務 

 當代企業IT新顯學：混合雲應用 

 新世代入門級NAS進軍企業應用需求 

 Avago展出100GbE、PCIe I/O整合技術，引領資料中心網路新方向 
 
 






 







 

熱門新聞






 


英國Wi-Fi業者使壞，讓2.2萬名用戶無意中同意去掃流動廁所

2017-07-18
 
 






 


Google 兩步驟驗證將以手機提示取代簡訊

2017-07-17
 
 






 


【AI關鍵技術】三大熱門深度學習框架新進展

2017-07-19
 
 






 


硬體不相容，部份英特爾Atom裝置無法升級Windows 10 Creators Update

2017-07-18
 
 






 


災難！CoinDash首度發行貨幣當天就被駭，損失近700萬美元

2017-07-18
 
 






 


微軟亞洲研究院院長洪小文：越複雜越不管用，AI最適合封閉型的高重複性任務

2017-07-17
 
 






 


安撫使用者不滿情緒，Skype聯絡人狀態顯示功能回來了!

2017-07-17
 
 






 


南韓代管業者Nayana遭勒索軟體攻擊，與駭客達成協議將支付110萬美元贖金

2017-06-21
 
 






 


2017年AI開始普及化，技術競賽白熱化

2017-07-15
 
 






 


國際警方聯手關閉暗網市集AlphaBay

2017-07-17
 
 



 

專題報導




企業行動化管理解決方案採購大特輯 


AI 100（上） 


公有雲儲存服務大盤點 


富士通AI新戰略 


企業身分驗證雲端服務採購特輯 

更多專題報導
 

 











臺灣醫學會
























































































































































 





 


專題討論12：海量資料與雲端在醫療的運用與發展
 
程　序　表




S12-3
                                                        海量資料與基因庫之研究：基因體醫學中的巨量資料分析
莊曜宇
國立臺灣大學生醫電資所




 　　巨量資料分析是一個新興的科技潮流，目前已被廣泛的應用於各類學科及領域上，例如工業界、市場調查以及科學研究等等。在生物醫學研究上，高通量(high-throughput)的實驗技術已逐漸發展並趨向成熟，其中最主要的兩個技術為微陣列(microarray)及次世代定序(next-generation  sequencing, NGS)可協助研究人員在短暫的時間中，一次性的檢測數以百萬的基因特徵。舉例來說，人類基因體定序計畫(Human Genome Project)共耗費了38億美金及超過十年的時間才正式完成，但次世代定序技術可以在十天內針對單一個體提供30億組的序列資料。因此，如何針對如此巨量的基因體資料進行有效率且準確的分析，已對生物學家及醫師產生巨大的困難與挑戰。為瞭解決這個困難，我的研究室目前正在開發一套雲端的次世代定序資料線上分析系統，其主要架構將建構於平行計算上以減少分析所需時間，另外我們希望透過這套系統提供一個簡單易懂的介面，讓非資訊背景的使用者能夠直觀的操作系統進行數據處理。除了這套雲端次世代定序資料分析系統外，我們已經利用高通量的基因體實驗技術及生物資訊方法研究了許多生物醫學上重要的領域及課題，例如癌症的基因體變化、放射基因體學及心血管疾病等等。我們的研究團隊已經開發出許多演算法及線上資料庫不僅可用於分析高通量基因體資料及癌症基因組改變情形，亦可協助研究人員闡述及解讀資料的生物及臨床醫學意義。在今天的演講中，我將利用我們過去的研究經驗作為例子來說明結合基因體技術與生物資訊方法將能夠加快癌症醫學研究的速度。此外，我亦將分享我們過去在食道癌上的轉譯醫學研究成果，我們可以透過兩個單核?酸位點來預測病患的治療效果，此研究成果已經獲得美國及台灣的專利。總結來說，在本次演講中，我將說明巨量資料分析在基因體醫學研究上的優勢與展望，以及要如何將基因體的巨量資料轉為可應用性的生物標記，以協助並改善疾病的診斷與治療。
  


 





 














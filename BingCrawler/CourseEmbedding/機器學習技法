



NTUMOOC















































Machine Learning Techniques (機器學習技法) - YouTube












































 
 






 

 
Skip navigation
  
 

 
Sign inSearch
 



 Hsuan-Tien Lin
 Videos
 Playlists
 Channels
 Discussion
 About
 

 












            Home
          










            Trending
          










            History
          










            Get YouTube Red
          










            Get YouTube TV
          











      Best of YouTube
    




 










            Music
          







 










            Sports
          







 










            Gaming
          







 










            Movies
          







 










            TV Shows
          







 










            News
          







 










            Live
          







 










            Spotlight
          







 










            360° Video
          

















            Browse channels
          










      Sign in now to see your channels and recommendations!
    

Sign in
























































 





Watch QueueQueueWatch QueueQueue 
Remove allDisconnect







The next video is startingstop 


Loading...
    

 



 

Watch Queue
    
Queue

__count__/__total__
    






































Hsuan-Tien Lin




SubscribeSubscribedUnsubscribe8,283 












Loading...
    









Loading...
    













        Working...
    




















Home


Videos


Playlists


Channels


Discussion


About













► Play all 
    Machine Learning Techniques (機器學習技法)
  
Hsuan-Tien Lin65 videos137,754 viewsLast updated on Feb 13, 2016 Play all


Share






Loading...
    







Save


Sign in to YouTube


Sign in








 







 
Play nextPlay now



 

      Linear Support Vector Machine (SVM) :: Course Introduction @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 4:08

 







 
Play nextPlay now



 

      Linear SVM :: Large-Margin Separating Hyperplane @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 14:18

 







 
Play nextPlay now



 

      Linear SVM :: Standard Large-Margin Problem @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 19:17

 







 
Play nextPlay now



 

      Linear SVM :: Support Vector Machine @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 15:34

 







 
Play nextPlay now



 

      Linear SVM :: Reasons behind Large-Margin Hyperplane @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:32

 







 
Play nextPlay now



 

      Dual Support Vector Machine :: Motivation of Dual SVM @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 15:55

 







 
Play nextPlay now



 

      Dual Support Vector Machine :: Largange Dual SVM @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 18:51

 







 
Play nextPlay now



 

      Dual Support Vector Machine :: Solving Dual SVM @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 14:20

 







 
Play nextPlay now



 

      Dual Support Vector Machine :: Messages behind Dual SVM @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 11:19

 







 
Play nextPlay now



 

      Kernel Support Vector Machine :: Kernel Trick @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 20:24

 







 
Play nextPlay now



 

      Kernel Support Vector Machine :: Polynomial Kernel @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 12:17

 







 
Play nextPlay now



 

      Kernel Support Vector Machine :: Gaussian Kernel @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 14:44

 







 
Play nextPlay now



 

      Kernel Support Vector Machine :: Comparison of Kernels @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:36

 







 
Play nextPlay now



 

      Soft-Margin Support Vector Machine :: Motivation and Primal @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 14:28

 







 
Play nextPlay now



 

      Soft-Margin Support Vector Machine :: Dual Problem @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 7:39

 







 
Play nextPlay now



 

      Soft-Margin Support Vector Machine :: Messages @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:45

 







 
Play nextPlay now



 

      Soft-Margin Support Vector Machine :: Model Selection @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 9:58

 







 
Play nextPlay now



 

      Kernel Logistic Regression :: Soft-Margin SVM as Regularized @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:41

 







 
Play nextPlay now



 

      Kernel Logistic Regression :: SVM versus Logistic Regression @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 10:19

 







 
Play nextPlay now



 

      Kernel Logistic Regression :: SVM for Soft Binary @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 9:37

 







 
Play nextPlay now



 

      Kernel Logistic Regression :: Kernel Logistic Regression @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 16:23

 







 
Play nextPlay now



 

      Support Vector Regression :: Kernel Ridge Regression @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 17:18

 







 
Play nextPlay now



 

      Support Vector Regression :: Support Vector Regression Primal @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 18:45

 







 
Play nextPlay now



 

      Support Vector Regression :: Support Vector Regression Dual @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:06

 







 
Play nextPlay now



 

      Support Vector Regression :: Summary of Kernel Models @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 9:07

 







 
Play nextPlay now



 

      Blending and Bagging :: Motivation of Aggregation @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 18:55

 







 
Play nextPlay now



 

      Blending and Bagging :: Uniform Blending @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 20:32

 







 
Play nextPlay now



 

      Blending and Bagging :: Linear and Any Blending @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 16:49

 







 
Play nextPlay now



 

      Blending and Bagging :: Bagging (Bootstrap Aggregation) @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 11:49

 







 
Play nextPlay now



 

      Adaptive Boosting :: Motivation of Boosting @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 12:48

 







 
Play nextPlay now



 

      Adaptive Boosting :: Diversity by Re-weighting @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 14:29

 







 
Play nextPlay now



 

      Adaptive Boosting :: Adaptive Boosting Algorithm @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:35

 







 
Play nextPlay now



 

      Adaptive Boosting :: Adaptive Boosting in Action @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 11:05

 







 
Play nextPlay now



 

      Decision Tree :: Decision Tree Hypothesis @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 17:29

 







 
Play nextPlay now



 

      Decision Tree :: Decision Tree Algorithm @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 15:21

 







 
Play nextPlay now



 

      Decision Tree :: Decision Tree Heuristics in C&RT @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:22

 







 
Play nextPlay now



 

      Decision Tree :: Decision Tree in Action @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 8:45

 







 
Play nextPlay now



 

      Random Forest :: Random Forest Algorithm @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:07

 







 
Play nextPlay now



 

      Random Forest :: Out-of-bag Estimate @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 12:33

 







 
Play nextPlay now



 

      Random Forest :: Feature Selection @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 19:28

 







 
Play nextPlay now



 

      Random Forest :: Random Forest in Action @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:29

 







 
Play nextPlay now



 

      Gradient Boosted Decision Tree :: AdaBoost Decision Tree @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 15:06

 







 
Play nextPlay now



 

      Gradient Boosted Decision Tree :: Optimization of AdaBoost @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 27:26

 







 
Play nextPlay now



 

      Gradient Boosted Decision Tree :: Gradient Boosting @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 18:21

 







 
Play nextPlay now



 

      Gradient Boosted Decision Tree :: Summary of Aggregation @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 11:20

 







 
Play nextPlay now



 

      Neural Network :: Motivation @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 20:37

 







 
Play nextPlay now



 

      Neural Network :: Neural Network Hypothesis @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 18:02

 







 
Play nextPlay now



 

      Neural Network :: Neural Network Learning @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 22:27

 







 
Play nextPlay now



 

      Neural Network :: Optimization and Regularization @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 17:30

 







 
Play nextPlay now



 

      Deep Learning :: Deep Neural Network @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 21:31

 







 
Play nextPlay now



 

      Deep Learning ::Autoencoder @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 15:18

 







 
Play nextPlay now



 

      Deep Learning ::Denoising Autoencoder @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 8:32

 







 
Play nextPlay now



 

      Deep Learning :: Principal Component Analysis @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 31:21

 







 
Play nextPlay now



 

      Radial Basis Function Network :: RBF Network Hypothesis @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 12:56

 







 
Play nextPlay now



 

      Radial Basis Function Network :: RBF Network Learning @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 20:09

 







 
Play nextPlay now



 

      Radial Basis Function Network :: k-Means Algorithm @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 16:20

 







 
Play nextPlay now



 

      Radial Basis Function Network :: k-Means and RBFNet in Action @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 9:47

 







 
Play nextPlay now



 

      Matrix Factorization :: Linear Network Hypothesis @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 20:17

 







 
Play nextPlay now



 

      Matrix Factorization :: Basic Matrix Factorization @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 16:33

 







 
Play nextPlay now



 

      Matrix Factorization :: Stochastic Gradient Descent @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 12:23

 







 
Play nextPlay now



 

      Matrix Factorization :: Summary of Extraction Models @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 9:13

 







 
Play nextPlay now



 

      Finale :: Feature Exploitation Techniques @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 16:12

 







 
Play nextPlay now



 

      Finale :: Error Optimization Techniques @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 8:41

 







 
Play nextPlay now



 

      Finale :: Overfitting Elimination Techniques @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 6:45

 







 
Play nextPlay now



 

      Finale :: Machine Learning in Practice @ Machine Learning Techniques (機器學習技法)
    

by Hsuan-Tien Lin




 13:00












  

 
Language:
  
  English



 
Content location:
  
  United States



 
Restricted Mode:
  
Off



History Help





Loading...
    






Loading...
    






Loading...
    


 About
Press
Copyright
Creators
Advertise
Developers
+YouTube
 Terms
Privacy

Policy & Safety
  
Send feedback

Test new features













Loading...
    














        Working...
    









 
Sign in to add this to Watch Later

    
 

Add to
    





        Loading playlists...
    


 









Hsuan-Tien Lin > MOOCs




Hsuan-Tien Lin


Home
|

MOOCs
|

Courses
|

Research Group
|

Awards
|

Publications
|

Presentations
|

Programs



I am fortunate to be among the very first NTU EECS professors to offer two Mandarin-teaching MOOCs (massive open online courses) on NTU@Coursera. The two MOOCs are
Machine Learning Foundations and Machine Learning Techniques and are based on the textbook Learning from Data: A Short Course that I co-authored. The book is consistently among the best sellers in Machine Learning on Amazon. 

The slides of the MOOCs below are available as is with no explicit or implied warranties. 
  The slides themselves are shared by CC-BY-NC 3.0, but the copyright of all materials (figures in particular) remain with the original copyright holder (in almost all cases the authors of the Learning from Data: A Short Course book).

Here are some quick links for each MOOC. The erratas are here.
  
Machine Learning Foundations

MOOC
all handout slides
free youtube videos

Machine Learning Techniques

MOOC
all handout slides
free youtube videos


Detailed outlines for each MOOC, along with the presentation sldies, are listed below.

  Machine Learning Foundations


When can machines learn?


Lecture 1
the learning problem: 

Course Introduction
What is Machine Learning
Applications of Machine Learning
Components of Machine Learning
Machine Learning and Other Fields



handout slides; presentation slides



Lecture 2
learning to answer yes/no:

Perceptron Hypothesis Set
Perceptron Learning Algorithm (PLA)
Guarantee of PLA
Non-Separable Data



handout slides; presentation slides



Lecture 3
types of learning:

Learning with Different Output Space
Learning with Different Data Label
Learning with Different Protocol
Learning with Different Input Space



handout slides; presentation slides



Lecture 4
feasibility of learning:

Learning is Impossible?
Probability to the Rescue
Connection to Learning
Connection to Real Learning



handout slides; presentation slides



Why can machines learn?


Lecture 5
training versus testing:

Recap and Preview
Effective Number of Lines
Effective Number of Hypotheses
Break Point



handout slides; presentation slides



Lecture 6
theory of generalization:

Restriction of Break Point
Bounding Function: Basic Cases
Bounding Function: Inductive Cases
A Pictorial Proof



handout slides; presentation slides



Lecture 7
the VC dimension:

Definition of VC Dimension
VC Dimension of Perceptrons
Physical Intuition of VC Dimension
Interpreting VC Dimension



handout slides; presentation slides



Lecture 8
noise and error:

Noise and Probabilistic Target
Error Measure
Algorithmic Error Measure
Weighted Classification



handout slides; presentation slides



How can machines learn?


Lecture 9
linear regression:

Linear Regression Problem
Linear Regression Algorithm
Generalization Issue
Linear Regression for Binary Classification



handout slides; presentation slides



Lecture 10
logistic regression:

Logistic Regression Problem
Logistic Regression Error
Gradient of Logistic Regression Error
Gradient Descent



handout slides; presentation slides



Lecture 11
linear models for classification:

Linear Models for Binary Classification
Stochastic Gradient Descent
Multiclass via Logistic Regression
Multiclass via Binary Classification



handout slides; presentation slides



Lecture 12
nonlinear transformation:

Quadratic Hypotheses
Nonlinear Transform
Price of Nonlinear Transform
Structured Hypothesis Sets


handout slides; presentation slides



How can machines learn better?


Lecture 13
hazard of overfitting:

What is Overfitting?
The Role of Noise and Data Size
Deterministic Noise
Dealing with Overfitting


handout slides; presentation slides



Lecture 14
regularization:

Regularized Hypothesis Set
Weight Decay Regularization
Regularization and VC Theory
General Regularizers


handout slides; presentation slides



Lecture 15
validation:

Model Selection Problem
Validation
Leave-One-Out Cross Validation
V-Fold Cross Validation


handout slides; presentation slides



Lecture 16
three learning principles:

Occam's Razor
Sampling Bias
Data Snooping
Power of Three


handout slides; presentation slides



Machine Learning Techniques


embedding numerous features



Lecture 1

linear support vector machine:

Course Introduction
Large-Margin Separating Hyperplane
Standard Large-Margin Problem
Support Vector Machine
Reasons behind Large-Margin Hyperplane



handout slides; presentation slides



Lecture 2
dual support vector machine:

Motivation of Dual SVM
Lagrange Dual SVM
Solving Dual SVM
Messages behind Dual SVM


handout slides; presentation slides



Lecture 3
kernel support vector machine:

Kernel Trick
Polynomial Kernel
Gaussian Kernel
Comparison of Kernels


handout slides; presentation slides



Lecture 4
soft-margin support vector machine:

Motivation and Primal Problem
Dual Problem
Messages behind Soft-Margin SVM
Model Selection


handout slides; presentation slides



Lecture 5
kernel logistic regression:

Soft-Margin SVM as Regularized Model
SVM versus Logistic Regression
SVM for Soft Binary Classification
Kernel Logistic Regression


handout slides; presentation slides



Lecture 6
support vector regression:

Kernel Ridge Regression
Support Vector Regression Primal
Support Vector Regression Dual
Summary of Kernel Models


handout slides; presentation slides



combining predictive features


Lecture 7
blending and bagging:

Motivation of Aggregation
Uniform Blending
Linear and Any Blending
Bagging (Bootstrap Aggregation)


handout slides; presentation slides



Lecture 8
adaptive boosting:

Motivation of Boosting
Diversity by Re-weighting
Adaptive Boosting Algorithm
Adaptive Boosting in Action

handout slides; presentation slides



Lecture 9
decision tree:

Decision Tree Hypothesis
Decision Tree Algorithm
Decision Tree Heuristics in C&RT
Decision Tree in Action


handout slides; presentation slides



Lecture 10
random forest:

Random Forest Algorithm
Out-Of-Bag Estimate
Feature Selection
Random Forest in Action


handout slides; presentation slides



Lecture 11
gradient boosted decision tree:

Adaptive Boosted Decision Tree
Optimization View of AdaBoost
Gradient Boosting
Summary of Aggregation Models


handout slides; presentation slides



distilling hidden features


Lecture 12
neural network:

Motivation
Neural Network Hypothesis
Neural Network Learning
Optimization and Regularization


handout slides; presentation slides



Lecture 13
deep learning:

Deep Neural Network
Autoencoder
Denoising Autoencoder
Principal Component Analysis


handout slides; presentation slides



Lecture 14
radial basis function network:

RBF Network Hypothesis
RBF Network Learning
k-Means Algorithm
k-Means and RBF Network in Action


handout slides; presentation slides



Lecture 15
matrix factorization:

Linear Network Hypothesis
Basic Matrix Factorization
Stochastic Gradient Descent
Summary of Extraction Models


handout slides; presentation slides



happy learning!


Lecture 16
finale:

Feature Exploitation Techniques
Error Optimization Techniques
Overfitting Elimination Techniques
Machine Learning in Practice


handout slides; presentation slides







Last updated at CST 15:20, December 26, 2016
Please feel free to contact me: 













機器學習課程


















機器學習
(Machine 
		Learning, ML)


3學分        
                          
        3小時
(上課        
         3小時/週)







Reference Books:
1.



		Introduction to Machine 
        Learning, E. Alpaydin, The MIT Press, 2009.



2.




類神經網路與模糊控制理論-入門與應用,
			

王進德,
			

全華, 
			2007.




3.

機器學習-類神經網路、模糊系統以及基因演算法,
				

				蘇木春&張孝德,
				

				全華, 
				2006. 



4.




研究分析方法，邱裕鈞&馮正民，建都文化，2004.




5.




資料探勘，施雅月&賴錦慧 
			譯，培生，2007.




6.





類神經網路模式-應用與實作，葉怡成，儒林，2009.




Instructor：

陳士傑




















1.


				歡迎大家自行上線學習，並歡迎各界免費使用。若需用於公開教學、著作…等，請適當引用原著作人資訊。請勿使用於販售…等有價侵權行為。 
				本人所學有限，故講義內容若有誤謬或引用不全之處，也煩請不吝指正。由於本人事務繁重，故僅就課程教學上為大家釋疑。請勿將您在它處取得的作業或相關考題直接寄給本人要求詳解 
				(若僅是討論問題的少數個疑惑點則不在此限)。



2.


				若影音檔中有 "音質" 以外之其它可藉由修剪影片改善的問題 
				(如：兩段話前後空白過久，重覆講述同一句話過多次…等)，皆歡迎告知本人以做修正。僅需告知本人哪一章的哪一個影音檔宜再修剪即可。



3.



					希臘數學符號羅馬拼音(pdf)



4.



				本課程所有影音檔是與本繫系網放置同一空間。若您在連結時連不上，可先連結至敝繫系網(國立聯合大學資管系)，若系網掛點，則是伺服器的問題，敝系助理會於上班期間處理。若非此因素，則請連絡本人，謝謝您。



5.



				若影音檔下載開啟時有所延遲，表示目前觀看人數較多，敬請耐心等候。



6.


建議參考課程：(皆有影音檔，本門課修習完畢後、欲更深入探討者適用)


　





					台灣大學林軒田教授機器學習基石與機器學習技法(上課去)







					台灣大學李宏毅教授機器學習與深度學習(上課去)







					史丹佛大學機器學習課程







Course

Slide




課程簡介


　

人工智慧、機器學習與計算智慧概念

　
　
　
　
　
　
　
　
　
　










導論(Introduction)


　


	最佳化問題(Optimization Problem)

　
　
　
　
　
　
　
　
　
　










變化型啟發式搜尋(Meta-heuristic Search)


　




基因演算法基礎(Foundations of Genetic Algorithm)


(1)

(2)

(3)

(4)

(5)

						　

						　

						　

						　

						　

						　









螞蟻演算法基礎 
	(Foundations of Ant Algorithm)


(1)

(2)

(3)

(4)

(5)
　
　
　
　
　
　









粒子群演算法基礎 (Foundations 
	of Particle Swarm Optimization; PSO)--【第一片下載速度較慢】


(1)
　
　
　
　
　
　
　
　
　
　









禁制搜尋法基礎 (Foundations 
	of Tabu Search)


(1)

(2)

(3)
　
　
　
　
　
　
　
　











模擬退火法基礎 (Foundations 
		of 

Simulated Annealing)


(1)

(2)

(3)
　
　
　
　
　
　
　
　











分類(Classification)


　

分類概念

　

F
課程內容請見本人資料探勘數位課程之"分類"一章




      　



決策樹學習 (Decision 
	Tree Learning)

　
　
　
　
　
　
　
　
　
　









最近鄰居分類 
	(Nearest-Neighbor Classification)

　
　
　
　
　
　
　
　
　
　
　









貝氏分類 (Bayes 
	Classification)

　
　
　
　
　
　
　
　
　
　
　









支持向量機 (Support 
	Vector Machine; SVM)


(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)





(12)

(13)
　
　
　
　
　
　
　
　
　










增強式學習(Reinforcement 
	Learning)


　

增強式學習概念

　
　
　
　
　
　
　
　
　
　










類神經網路基礎(Foundations 
	of Neural Networks)


　

類神經網路導論

　
　
　
　
　
　
　
　
　
　
　











監督式學習網路





感知機 
		(Perceptron)

　
　
　
　
　
　
　
　
　
　
　












倒傳遞網路 (Back-Propagation 
		Network; BPN)

　
　
　
　
　
　
　
　
　
　
　










徑向基底函數網路 
	(Radial Basis Function; RBF)

　
　
　
　
　
　
　
　
　
　
　









反傳遞神經網路 
	(Counter Propagation Network; CPN)

　
　
　
　
　
　
　
　
　
　
　









機率神經網路 
	(Probabilistic Neural Network; PNN)

　
　
　
　
　
　
　
　
　
　
　









學習向量量化網路 (Learning 
	Vector Quantization; LVQ)

　
　
　
　
　
　
　
　
　
　
　











無監督式學習網路



自適應共振理論網路 (Adaptive 
	Resonance Theory Network; ART)


						　

						　
　
　
　
　
　
　
　
　
　









自組織映射圖網路 (Self-Organizing 
	Map Network; SOM)


						　

						　
　
　
　
　
　
　
　
　
　











聯想式學習網路



霍普菲爾網路 (Hopfield Neural Network; 
	HNN)


(1)

(2)
　
　
　
　
　
　
　
　
　









雙向聯想記憶網路 (Bi-directional 
	Associative Memory; BAM)


						　

						　
　
　
　
　
　
　
　
　
　










深度學習基礎(Foundations 
	of Deep 
	Learning)


　



深度學習概念


(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)
　
　









捲積神經網路 
	(Convolution Neural Network, CNN)


(1)

(2)

(3)

(4)
　
　
　
　
　
　
　









循環神經網路 
	(Recurrent Neural Network; RNN)


(1)

(2)
　
　
　
　
　
　
　
　
　

















機器學習 - 維基百科，自由的百科全書































 







機器學習

維基百科，自由的百科全書


					前往：					導覽，					搜尋








機器學習與資料探勘





問題





分類
聚類
回歸
異常檢測
關聯規則
強化學習
結構預測（英語：Structured prediction）
特徵學習
線上學習（英語：Online machine learning）
半監督學習（英語：Semi-supervised learning）
語法歸納（英語：Grammar induction）






監督學習
(分類 · 回歸)






決策樹
表徵（裝袋, 提升，隨機森林）
k-NN
線性回歸
樸素貝葉斯
神經網路
邏輯回歸
感知器
支援向量機（SVM）
相關向量機（RVM）





聚類





BIRCH（英語：BIRCH）
層次（英語：Hierarchical clustering）
k平均
期望最大化（EM）

DBSCAN
OPTICS（英語：OPTICS）
均值飄移（英語：Mean shift）





降維





因子分析（英語：Factor analysis）
CCA
ICA
LDA
NMF（英語：Non-negative matrix factorization）
PCA
LASSO
t-SNE（英語：t-distributed stochastic neighbor embedding）





結構預測（英語：Structured prediction）





機率圖模型（貝葉斯網路，CRF, HMM）





異常檢測





k-NN
局部離群因子（英語：Local outlier factor）





神經網路





自編碼（英語：Autoencoder）
深度學習
多層感知機
RNN
受限玻爾茲曼機
SOM
CNN





理論





偏差/方差困境（英語：Bias–variance tradeoff）
計算學習理論（英語：Computational learning theory）
經驗風險最小化（英語：Empirical risk minimization）
PAC學習（英語：Probably approximately correct learning）
統計學習
VC理論








閱
論
編





機器學習是人工智慧的一個分支。人工智慧的研究是從以「推理」為重點到以「知識」為重點，再到以「學習」為重點，一條自然、清晰的脈絡。顯然，機器學習是實現人工智慧的一個途徑，即以機器學習為手段解決人工智慧中的問題。機器學習在近30多年已發展為一門多領域交叉學科，涉及機率論、統計學、逼近論、凸分析、計算複雜性理論等多門學科。機器學習理論主要是設計和分析一些讓電腦可以自動「學習」的演算法。機器學習演算法是一類從資料中自動分析獲得規律，並利用規律對未知資料進行預測的演算法。因為學習演算法中涉及了大量的統計學理論，機器學習與推斷統計學聯繫尤為密切，也被稱為統計學習理論。演算法設計方面，機器學習理論關註可以實現的，行之有效的學習演算法。很多推論問題屬於無程式可循難度，所以部分的機器學習研究是開發容易處理的近似演算法。
機器學習已廣泛應用於資料探勘、電腦視覺、自然語言處理、生物特徵識別、搜尋引擎、醫學診斷、檢測信用卡欺詐、證券市場分析、DNA序列測序、語音和手寫識別、戰略遊戲和機器人等領域。



目錄


1 定義
2 分類
3 演算法
4 參考文獻

4.1 參照
4.2 書目


5 外部連結
6 參見



定義[編輯]
機器學習有下麵幾種定義：

機器學習是一門人工智慧的科學，該領域的主要研究物件是人工智慧，特別是如何在經驗學習中改善具體演算法的效能。
機器學習是對能通過經驗自動改進的電腦演算法的研究。
機器學習是用資料或以往的經驗，以此最佳化電腦程式的效能標準。

一種經常參照的英文定義是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
分類[編輯]
機器學習可以分成下麵幾種類別：

監督學習從給定的訓練資料集中學習出一個函式，當新的資料到來時，可以根據這個函式預測結果。監督學習的訓練集要求是包括輸入和輸出，也可以說是特徵和目標。訓練集中的目標是由人標註的。常見的監督學習演算法包括回歸分析和統計分類。
無監督學習與監督學習相比，訓練集沒有人為標註的結果。常見的無監督學習演算法有聚類。
半監督學習介於監督學習與無監督學習之間。
增強學習通過觀察來學習做成如何的動作。每個動作都會對環境有所影響，學習物件根據觀察到的周圍環境的反饋來做出判斷。

演算法[編輯]
具體的機器學習演算法有：

構造間隔理論分佈：聚類分析和模式識別

人工神經網路
決策樹
感知器
支援向量機
整合學習AdaBoost
降維與度量學習
聚類
貝葉斯分類器


構造條件機率：回歸分析和統計分類

高斯過程回歸
線性判別分析
最近鄰居法
徑向基函式核


通過再生模型構造機率密度函式：

最大期望演算法
機率圖模型：包括貝葉斯網和Markov隨機場
Generative Topographic Mapping


近似推斷技術：

馬爾可夫鏈
蒙特卡羅方法
變分法


最最佳化：大多數以上方法，直接或者間接使用最最佳化演算法。

參考文獻[編輯]
參照[編輯]

書目[編輯]

Bishop, C. M. (1995). 《模式識別神經網路》，牛津大學出版社。ISBN 0-19-853864-2
Bishop, C. M. (2006). 《模式識別與機器學習》，Springer。ISBN 978-0-387-31073-2
Richard O. Duda, Peter E. Hart, David G. Stork (2001) 《模式分類》（第2版）, Wiley, New York, ISBN 0-471-05669-3.
MacKay, D. J. C. (2003). 《資訊理論、推理和學習演算法》，劍橋大學出版社。ISBN 0-521-64298-1
Mitchell, T. (1997). 《機器學習》, McGraw Hill. ISBN 0-07-042807-7
Sholom Weiss, Casimir Kulikowski (1991). Computer Systems That Learn, Morgan Kaufmann. ISBN 1-55860-065-5

外部連結[編輯]


UCI description
機器學習軟體Weka
Pablo Castro首頁
機器學習網郵寄清單
機器學習和自然語言處理-弗萊堡大學
機器學習和資料探勘，生物資訊學小組，慕尼黑工業大學
機器學習和生物計算-Bristol大學
機器學習和應用統計學@微軟研究
機器學習研究月刊
機器學習期刊
機器學習-Kmining，資料探勘和KDD科學參考
Book "智慧型系統社群" by Walter Fritz
開放目錄專案
機器學習論文-CiteSeer
Orange，使用Python手稿語言的機器學習元件和視覺化編程介面



參見[編輯]


資訊科技主題
統計學主題




人工智慧
計算智慧型
資料探勘
模式識別
機器學習方面重要出版物（電腦科學）
機器學習方面重要出版物（統計學）
自主控制機器人
歸納邏輯編程
決策樹
神經網路
強化學習
貝葉斯學習
最近鄰居法
計算學習理論











閱
論
編


電腦科學






數學基礎

數理邏輯 · 集合論 · 數論 · 圖論 · 類型論 · 範疇論 · 數值分析 · 資訊理論






計算理論

自動機 · 可計算性理論 · 計算複雜性理論 · 量子計算 · 數值計算方法






演算法和資料結構

演算法分析 · 演算法設計 · 計算幾何






程式語言和編譯器

語法分析器 · 直譯器 · 編程典範（程序化編程 · 物件導向程式編程 · 函數語言程式設計 · 邏輯編程等）






並行、並列和分散式系統

多處理器 · 網格計算 · 並行控制






軟體工程

需求分析 · 軟體設計 · 程式設計 · 形式化方法 · 軟體測試 · 軟體開發過程






系統架構

電腦系統架構 · 微處理器體系結構 · 作業系統






電信與網路

路由 · 網路拓撲 · 密碼學






資料庫

資料庫管理系統 · 關聯式資料庫 · 結構化查詢語言 · NoSQL · 事務處理 · 資料庫索引 · 資料探勘






人工智慧

自動推理 · 計算語言學 · 電腦視覺 · 進化計算 · 專家系統 · 機器學習 · 自然語言處理 · 機器人學






電腦圖形學

視覺化 · 電腦動畫 · 圖像處理






人機互動

電腦輔助功能 · 使用者介面 · 可穿戴電腦 · 普適計算 · 虛擬現實 · 聊天機器人






科學計算

人工生命 · 生物資訊學 · 認知科學 · 計算化學 · 計算神經科學 · 計算物理學 · 數值演算法 · 符號計算







註：電腦科學領域也可根據ACM-2012分類系統進行分類。










 
						取自 "https://zh.wikipedia.org/w/index.php?title=機器學習&oldid=44802676"					
3 個分類：機器學習人工智慧人工智慧應用隱藏分類：使用ISBN魔術連結的頁面 



導覽選單


個人工具

沒有登入對話貢獻建立帳號登入 



命名空間

條目
討論




台灣正體



不轉換
簡體
繁體
大陸簡體
香港繁體
澳門繁體
馬新簡體
台灣正體






查看

閱讀
編輯
檢視歷史



更多







搜尋



 







導航


首頁分類索引特色內容新聞動態近期變更隨機條目 



說明


說明維基社群方針與指引互助客棧知識問答字詞轉換IRC即時聊天聯絡我們關於維基百科資助維基百科 



其他專案


維基共享資源 



列印/匯出


下載成 PDF 



工具


連結至此的頁面相關變更上傳檔案特殊頁面可列印版靜態連結頁面資訊維基數據 項目引用此頁面 



其他語言


العربيةঅসমীয়াAzərbaycancaБългарскиCatalàČeštinaDanskDeutschΕλληνικάEnglishEspañolEestiفارسیSuomiFrançaisעבריתहिन्दीMagyarՀայերենBahasa IndonesiaÍslenskaItaliano日本語ಕನ್ನಡ한국어LietuviųLatviešuМакедонскиമലയാളംमराठीNederlandsNorsk nynorskNorsk bokmålPolskiPortuguêsРусскийSrpskohrvatski / српскохрватскиSimple EnglishSlovenščinaShqipСрпски / srpskiSvenskaதமிழ்ไทยTagalogTürkçeУкраїнськаTiếng Việt 
編輯連結 





 本頁面最後修訂於2017年6月17日 (週六) 06:01。
本站的全部文字在創用CC 姓名標示-相同方式分享 3.0 協議之條款下提供，附加條款亦可能應用（請參閱使用條款）。
Wikipedia®和維基百科標誌是維基媒體基金會的註冊商標；維基™是維基媒體基金會的商標。
維基媒體基金會是在美國佛羅里達州登記的501(c)(3)免稅、非營利、慈善機構。


隱私政策
關於維基百科
免責聲明
開發人員
Cookie 聲明
手機版檢視



 

 









































機器學習技法 | qwerty











qwerty



QWERTY




























機器學習技法





發表於

                2014-11-21
              
|



更新於

                2017-04-08
              


|



分類於


筆記




|








|



閱讀次數 






尚未補圖
從這裡開始我就不太行了…
Chap01 SVM

All line is the same using PLA, but WHICH line is best? → 可以容忍的誤差愈大愈好(最近的點與分隔線的距離愈遠愈好) → fat hyperplane (large margin)(分隔線可以多寬) 
若只有兩個點，必通過兩點連線之中垂線  
Standard large-margin hyperplane problemmax fatness(w) (max margin)= min distance($x_n$, w) (n = 1 ~ N)= min distance($x_n$, b, w) – (1)因為 w0 不列入計算(w0 = 常數項參數 = 截距b, x0 必為 1)
$w^tx$ = 0 → x除去x0成為x’ → $w^tx’$ + b = 0設x’, x’’都在$w^tx’$ + b = 0平面上，$w^tx’$ = -b, $w^tx’’$ = -b → $w^t(x’’-x’)$ = 0$w^t$ 垂直於 $w^tx’$ + b = 0平面distance(x, b, w) = x 到 平面的距離  
單一data的distance: 因 $y_n(w^t x_n + b) > 0$$distance(x_n, b, w) = (1/|w|) * y_n(w^t x_n + b)$ – (2)  
specialize令 $min y_n(w^t x_n + b) = 1$→ $distance(x_n, b, w) = 1/|w|$由式子(1),(2)可得 max margin = min distance(x_n, b, w) = 1/|w|條件為 $min y_n(w^t x_n + b) = 1$
放鬆條件$y_n(w_t x_n + b) >= 1$ is necessary constraints for $ min y_n(w_t x_n + b) = 1$if all y_n(w_t x_n + b) > p > 1 -> can generate more optimal answer (b/p, w/p) -> distance 1/|w| is smallerso y_n(w_t x_n + b) >= 1 → y_n(w_t x_n + b) = 1  
max 1/|w| -> min 1/2  w_t   w

can solve w by solve N inequality
Support Vector Machine(SVM)只須找最近的點即可算出wsupport vector: bounary data(胖線的邊界點)
gradient? : not easy with constraintsbut we have:

(convex) quadratic objective function(b, w)
linear constraints (b, w)

can use quadratic programming (QP, 二次規劃): easy 

Linear Hard-Margin(all wxy > 0) SVMthe same as ‘weight-decay regularization’ within Ein = 0
Restricts Dichotomies(堅持胖的線): if margin < p, no answerfewer dichotomies -> small VC dim -> better generalization
最大胖度有極限：在圓上，最大 根號3/2
can be good with transform控制複雜度的方法！控制複雜度的方法！
Chap02 dual SVMif d_trans is big, or infinite?(非常複雜的轉換)find: SVM without dependence of d_trans去除計算(b, w)與轉換函式複雜度的關係(QP of d_trans+1 variables -> N variables)    
Use lagrange multiplier: Dual SVM: 將lambda視為變數來解若不符條件，則L()的sigma部分是正的，MaxL()為無限大若符條件，則L()的sigma部分最大值為0 -> MaxL() = 1/2w^tw所以Min(MaxL()) = Min(1/2w^tw)
交換max min的位置，可求得原問題的下限
可以得到strong duality(強對偶關係，=)？若在二次規劃滿足constraint qualification  

convex
feasible(有解)
linear constraints

則存在 primal-dual 最佳解(對左右邊均是最佳解)
Dual SVM 最佳解為？b可以被消除(=0)w代入得
Karush-Kuhn-Tucker (KKT) conditionsprimal-inner -> an = 0 或 1-yn(wtzn+b) = 0(complementary slackness)=> at optimal all ‘Lagrange terms’ disappear
用KKT求得a(原lambda)之後，即可得原來的(b, w)w = sigma anynznb 僅有邊界(primal feasible)，但b = yn - wtzn when an > 0 (primal-inner，代表必在SVM邊界上)
重訂support vector的條件(a_n >0)=> b, w 都可以只用SV求到
SVM:找到有用的點(SV)
Standard hard-margin SVM dual經過整理現在有N個a, 並有N+1個條件了
when N is big, qn,m is dense array and very big(N >30000, use >3G ram)use special QP solver
SVM 和 PLA 比較w = linear combination of data => w represented by dataSVM: represent w by only SV
Primal: 對(b,w)做特別縮放Dual: 找到SV 和其 lagrange multiplier
問題：q_n,m 需要做O(d_trans)的運算，如何避免？
Chap03 Kernel SVM(z_n^T)(z_m)如何算更快
轉換+內積 -> Kernel functionuse O(d) instead of O(d^2)
用kernel簡化！(gsvm -> 無w)
Kernel Hard-Margin SVM
polynomial Kernel簡化的kernel: 對應到同等大小，不同幾何特性(如內積)的空間r影響SV的選擇
可以快速做高次轉換(和二次相同複雜度)
特例: linear只需用 K1 = (0+1xtx)^1
infinite Kerneltaylor展開
無限維度的Gaussian Kernel (Radial Basis Funtion(RBF))
large => sharp Gaussians => ‘overfit’?
Kernel選擇linear kernel: 等於沒有轉換，linear first, 計算快polynomial: 轉換過，限制小，strong physical control, 維度太大K會趨向極端值-> 平常只用不大的維度infinite dimension:most powerfulless numerical difficulty than poly(僅兩次式)one parameter onlycons: mysterious – no w , and too powerful
define new kernel is hard:Mercer’s condition:
Chap04 Soft-Margin SVMoverfit reason: transform & hard-margin(全分開)
Soft-Margin – 容忍錯誤，有錯誤penalty，只有對的需要符合條件
缺點：No QP anymoreerror大小:離fat boundary的距離 
改良：求最小(犯錯的點與boundary的距離和)(linear constraint, can use QP)
parameter C: large when want less violate marginsmall when want large margin, tolerate some violation
Soft-margin Dual: 將條件加入min中化簡後得到和dual svm相同的式子(不同條件)C is exactly the upper bound of an
Kernel Soft Margin SVMmore flexible: always solvable
(3)->solve b:若as < C(unbounded, free), 則b的求法和hard-margin一樣
但soft-margin還是會overfit…
physical meaningnot SV(an = 0): C-an != 0 -> En = 0unbounded SV(0 < an < C，口) -> En = 0 -> on fat boundarybounded SV(an = C, △) -> En >= 0(有違反，不在boundary上)-> 只有bounded SV才可違反
difficult to optimize(C, r)
SVM validationleave-one-out error <= #SV/N若移除non-SV的點，則得出的g不變-> 可以靠此特性做參數選擇(不選#SV太大的)
Chap05 Kernel Logistic SVM實用library: linear:LIBLINEAR nonlinear:LIBSVM  
將E替代 -> 像是 L2 regularization
缺點：不能QP, 不能微分(難解)
large margin <=> fewer choices <=> L2 regularization of short wsoft margin <=> special errlarger C(in soft-margin or in regularization) <=> smaller lagrange multiplier <=> less regularization  
We can extend SVM to other learning models!
look (wtzn + b) as linear score(f(x) in PLA)we can have Err_svm is upper bound of Err0/1(hinge error measure)
Err_sce: 與svm相似的一個logistic regression
L2 logistic regression is similar to SVM,所以SVM可以用來approximate Logistic regression?-> SVM當作Log regression的起始點? 沒有比較快(SVM優點)-> 將SVM答案當作Log的近似解(return theta(wx + b))? 沒有log reg的意義(maximum likelyhood)=> 加兩個自由度，return theta(A*(wx+b) + B)-> often A > 0(同方向), B~=0(無位移)將原本的SVM視為一種轉換
Platt’s Modelkernel SVM在Z空間的解 – 用Log Reg微調後 –> 用來近似Log Reg在Z空間的解(並不是在z空間最好的解)
solve LogReg to get(A, B)
能使用kernel的關鍵：w為z的線性組合
Representer Theorem: 若解L2-正規化問題，最佳w必為z的線性組合將w分為(與z垂直)+(與z平行), 希望w垂直 = 0證：(原本的w) 和 (與z平行的w) 所得的err是一樣的(因為w垂直 * z = 0)且w平行比較短所以min w 必(與z平行)結果：L2的linear model都可以用kernel解！
將w = sum(B*z) = sum(B*Kernel)代入logistic regression-> 解B
Kernel Logistic Regression(KLR)= linear model of B把 kernel當作轉換, kernel當作regularizer= linear model of wwith embedded-in-kernel transform & L2 regularizer把 kernel內部(z)當作轉換(?), L2-regularizer
警告：算出的B不會有很多零
soft margin SVM ~= L2 LOG REG, special error measure:hinge在z空間解log reg -> 用representor theorem 轉換為一般log reg, 有代價
Chap 06 Support Vector Regression(SVR)ridge regression : 有regularized的regression如何加入kernel?
Kernel Ridge Regression用representor theorem代入後得到regularization term 和 regression term
因為kernal必為為psd，所以B必有解 O(N^3)
g(x) = wz = sum(bz)z = sum(bk)
與linear的比較：kernel自由度高linear為O(d^3+d^2N)kernel和資料量有關，為O(N^3)，檔案大時不快
LS(least-squares)SVM = kernel ridge regression:和一般regression boundary差不多，但SV很多(B dense)=> 代表計算時間長=> 找一個sparse B?
tube regression:insensitive error:容忍一小段的差距(在誤差內err = 0，若超過, err只算超過的部分)error增加的速度變慢
學SVM，解QP, 用DUAL, KKT->sparseregulizer 和 超過tube上界的值，超過tube下界的值
參數：C(violation重視程度), tube範圍
作dual: lagrange multiplier + KKT condition
在tube裡面的點：B=0=> 只要tube夠寬，B為sparse
Linear, SVM Summary
first row: less used due to worse performancethird row: less used due to dense Bfourth row: popular in LIBSVM
Chap07 Blending and BaggingSelection: rely on only once hypothesisAggregation: mix or combine hypothesissselect trust-worthy from their usual performance=> validationmix the prediction => vote with different weight of ballotcombine predictions conditionally(when some situation, give more ballots to friend t)

Aggregation可做到：

feature transform(?), 將hypothesis變強
regularization(?)控制 油門 和 煞車

uniform blending: 一種model一票，取平均證明可以比原本的Eout小: 
一個演算法A的表現，可以用其hypothesis set中的”共識”來表示，等於共識的表現，加上共識的變異數，uniform blending就是將某些在A的hypothesis取平均(變成新的演算法A’)來減少A’的變異數expected performance of A = expected deviation to consensus + performance of consensus
linear blending: 加權(線性)平均，權重>0求類似linear regression的式子: 兩段式學習，先算出許多g，再做  linear regression -> 得到答案G限制：權重a>0 -> 將error rate大的model反過來用(error rate = 99%, 取其相反答案即可將error rate = 1%)   
any blending(stacking): 可用non-linear model(???)
算出g1-, g2- ...   
phi-1 = (g1-, g2-, ...)   
transform validation data to Z = (phi-1(x), y)   
compuate g = AnyModel(Z, Y)   
return G = g(phi(x))
phi = (g1, g2 ... )
比較：linear blending
compuate a = AnyModel(Z, Y)   
return G = a * phi(x)
learning: 邊學邊合，
bootstrapping: 從有限的資料模擬出新的資料bootstrap data: 從原本資料選擇N筆資料(可重複)Virtual aggregationbootstrap aggregation(bagging): 由bootstrap data訓練g，而非原資料-> meta algorithm for [base algorithm(可使用不同演算法)]

Chap08 Adaptive Boosting教小學生辨認蘋果:由一個演算法提供[會混淆的資料]由其他hypothesis提出一個不同的小規則來區分
給不同的data權重，會混淆的占較大比例，取min Ein = avg(Wn * err(xn, yn))，可用SVM, lin_reg, log_reg解Wn
gt = argmin(sum(ut  err))gt+1 = argmin(sum(ut+1  err))
找完gt後，gt+1應該要找和gt不相似的->找ut+1使gt的err rate接近0.5(隨機)。
err rate = 錯誤資料權重和 / (錯誤資料權重和 + 正確資料權重和) = 1/2=> 希望 正確資料權重和 = 錯誤資料權重和在gt中正確的資料, 權重要乘(err rate)在gt中錯誤的資料, 權重要乘(1-err rate)如此一來兩者之和將會相等
若scale factor = S = sqrt((1-err rate) / err rate)incorrect = Scorrect = 1/S若 S>1:→ err rate <= 1/2→ incorrect↑, correct↓, close to 1/2
u1 可設所有為1/N，得到min EinG 設uniform會使成績變差
Adaptive Boosting(皮匠法)邊做邊算at
希望愈好的gt，at愈大-> 設at = ln(St) (S = scale)if(err rate == 1/2) -> St = 1 -> at = 0if(err rate == 0) -> St = inf -> at = inf
只要err rate < 1/2 , 就可以參與投票：群眾的力量
adapative boosting 的 algorithm 選擇(不需強演算法):decision stump: 三個參數：which feature, threshold(線), direction(ox)，可以使Ein <= 1/2
Chap09 Decision Tree
Traditional learning model that realize conditional aggregation模仿人類決策過程
Path View:G = sum(q * g)q = condition (is x on this path?)g = base hypothesis, only constant, leaf in tree
Recursive View:G(x) = sum([b(x) == c] * Gc(x))G: full treeb: branching criteriaGc: sub-tree hypothesis
advantage: human-explainable, simple, efficient, missing feature handle, categorical features easily, multiclass easilydisadvantage: heuristic, little theoreticalEx. C&RT, C4.5, J48…
four choices: number of branches, branchingcriteria, termination criteria, & base hypothesis
C&RT(Classification and Regression Tree):Tree which is fully-grown with constant leavesC = 2(binary tree)，可用decision stumpgt(x) = 在此分類下output最有可能(出現最多次的yn or yn平均)-> 分得愈純愈好(同一類的output皆相同)
impurity = 變異數 or 出現最多次的yn的比率popular to use :Gini for classificationregression error for regression
terminate criteria:

all yn is the same: impurity = 0
all xn the same: cannot cut

if all xn different: Ein = 0low-level tree built with small D -> overfit 
regularizer: number of leavesargmin(Ein(G) + c * number of leaves(G))實作：一次剪一片葉子，選最好的  
相較數字的feature, 處理類型問題較簡單  
Surrogate(代理) branch:找一些與最好切法相近的，若data features missing, 則使用之
與adaboost相比：片段切割，只在自身subtree切
Chap10 Random ForestRandom Forest = bagging + fully-grown random-subspace random-combination C&RT decision tree
highly parallel, 減少 decision tree的variance  
增加decision tree diversity
random sample features from x(random subspace of X)-> efficient, can be used for any learning models10000個features, 只用100個維度來learn

將 x 作 低維度random projection -> 產生新的feature(斜線切割), random combination


Out-of-bagout-of-bag: not sampled after N drawingsN個data抽N次，沒被抽到機率 ~= 1/e=> 將沒抽到的DATA作g的validation(通常不做，因為g只為G的其中之一)=> 將沒抽到的DATA作G的validation，Eoob = sum(err(G-(xn))) (G-不包含用到xn的g)Eoob: self-validation
Feature Selectionwant to remove redundant, irrelevant features…
learn a subset-transform for the final hypothesis
advantage: interpretability, remove ‘feature noise’, efficientdisadvantage: total computation time increase, ‘select feature overfit’, mis-interpretability(過度解釋)
decision tree: built-in feature selection
idea: rate importance of every featureslinear model: 看w的大小non-linear model: not easy to estimate
idea: random testput some random value into feature, check performance↓，下降愈多代表愈重要
random value 

by original P(X = x)
bootstrap, permutation

performance: 算很久importance(i) = Eoob(G, D) - Eoob(G, Dp) (Dp = data with permutation in xn_i)
strength-correlation decompositions = average voting margin(投票最多-投票第二多…) with Gp = gt之間的相似度bias-variance decomposition
Chap11 Gradient Boost Decision TreeChap12 Neural Network




















          Please enable JavaScript to view the
          comments powered by Disqus.















            文章目錄
          

            本站概覽
          




HCL






              RSS
            





                  
                  GitHub
                













My Main Page


My First Blog -- qwerty


My Second Blog -- wysiwyg







1. Chap01 SVM1.1. Standard large-margin hyperplane problem1.2. Support Vector Machine(SVM)1.3. Chap02 dual SVM1.3.1. Standard hard-margin SVM dual1.3.2. SVM 和 PLA 比較1.4. Chap03 Kernel SVM1.4.1. Kernel Hard-Margin SVM1.4.2. polynomial Kernel1.4.3. infinite Kernel1.4.4. Kernel選擇1.5. Chap04 Soft-Margin SVM1.6. Kernel Soft Margin SVM1.6.1. SVM validation1.7. Chap05 Kernel Logistic SVM2. Chap 06 Support Vector Regression(SVR)2.1. Linear, SVM Summary3. Chap07 Blending and Bagging4. Chap08 Adaptive Boosting5. Chap09 Decision Tree6. Chap10 Random Forest6.1. 增加decision tree diversity6.2. Out-of-bag6.3. Feature Selection7. Chap11 Gradient Boost Decision Tree8. Chap12 Neural Network










0%









































Reviews for 機器學習技法 (Machine Learning Techniques) from Coursera | Class Central

































































































































































































subject


















































































Home







Coursera







機器學習技法 (Machine Learning Techniques)











Sponsored
Creative Applications of Deep Learning with TensorFlow

             via Kadenze





Sponsored
Intro to Data Analysis

             Facebook via Udacity







Intro











Coursera: 機器學習技法 (Machine Learning Techniques)



National Taiwan University

  with 
Hsuan-Tien Lin





                    

                    
                    

                    Welcome! The instructor has decided to teach the course in Mandarin on 
Coursera, while the slides of the course will be in English to ease the 
technical illustrations. We 
hope that this choice can help introduce Machine Learning to more students in the Mandarin-speaking world. The English-written slides 
will not require advanced English ability to understand, though. If you 
can understand the following descriptions of this course, you can 
probably follow the slides. [歡迎大家！這門課將採用英文投影片配合華文的教學講解，我們希望能藉這次華文教學的機會，將機器學習介紹給更多華人世界的同學們。課程中使用的英文投影片不會使用到艱深的英文，如果你能瞭解以下兩段的課程簡介，你應該也可以瞭解課程所使用的英文投影片。]In the prequel of this course, Machine Learning Foundations, we have illustrated the necessary fundamentals that give any student of machine learning a 
solid foundation to explore further techniques. While many new techniques are being designed every day, some techniques stood the test of time and became popular tools nowadays. The course roughly corresponds to the second 
half-semester of the National Taiwan University course "Machine 
Learning." Based 
on five years of teaching this popular course successfully (including 
winning the most prestigious teaching award of National Taiwan 
University) and discussing with many other scholars actively, the 
instructor chooses to focus on three of those popular tools, namely embedding numerous features (kernel models, such as support vector machine), combining predictive features (aggregation models, such as adaptive boosting), and distilling hidden features (extraction models, such as deep learning). 
Syllabus
Each of the following items correspond to approximately one hour of video lecture. [以下的每個小項目對應到約一小時的線上課程]Embedding Numerous Features [嵌入大量的特徵]   -- Linear Support Vector Machine [線性支持向量機]   -- Dual Support Vector Machine [對偶支持向量機]   -- Kernel Support Vector Machine [核型支持向量機] -- Soft-Margin Support Vector Machine [軟式支持向量機] -- Kernel Logistic Regression [核型羅吉斯迴歸]-- Support Vector Regression [支持向量迴歸]Combining Predictive Features [融合預測性的特徵]   -- Bootstrap Aggregation [自助聚合法]   -- Adaptive Boosting [漸次提昇法]   -- Decision Tree [決策樹] -- Random Forest [隨機森林] -- Gradient Boosted Decision Tree [梯度提昇決策樹]Distilling Hidden Features [萃取隱藏的特徵]   -- Neural Network [類神經網路]   -- Deep Learning [深度學習]-- Radial Basis Function Network [逕向基函數網路] -- Matrix Factorization [矩陣分解] Summary [總結]



Tags
asia
                                
taiwan
                                











1


                            Student
                            review                        









                    Read reviews
                
Write review







                            Cost
                        

                                                                                    Free Online Course (Audit)
                                                                        




                                Pace
                            
Finished





                        Subject
                    
Artificial Intelligence




                                Institution
                            
National Taiwan University




                            Provider
                        
Coursera




                            Language
                        
Chinese




                            Certificates
                        
Certificate Available




                            Hours
                        
8-24 hours a week




                            Calendar
                        




                                                    10th Nov, 2015
                                                

                                                    23rd Dec, 2014
                                                

8 weeks long




0
Interested students

















































































































Sign up for free?
Learn how


Go to Class


Get notified for next session






                            +                     
                        Add to My Courses
                        









Interested



Enrolled



Taking right now



Partially Completed



Completed



Audited



Dropped









                Learn Data Analysis
                

                    udacity.com
                
Learn to become a Data Analyst. Job offer guaranteed or get a full refund.

Advertisement




                Become a Data Scientist
                

                    datacamp.com
                
Learn Python & R at your own pace. Start now for free!

Advertisement








                FAQ
                View All



What are MOOCs?








                    MOOCs stand for Massive Open Online Courses. These are free online courses from universities around the world (eg. Stanford Harvard MIT) offered to anyone with an internet connection.
                



How do I register?








                    To register for a course, click on "Go to Class" button on the course page. This will take you to the providers website where you can register for the course.
                



How do these MOOCs or free online courses work?








                    MOOCs are designed for an online audience, teaching primarily through short (5-20 min.) pre recorded video lectures, that you watch on weekly schedule when convenient for you.  They also have student discussion forums, homework/assignments, and online quizzes or exams.
                








Related Courses





Machine Learning 1—Supervised Learning


Brown University


Georgia Institute of Technology

via
 Udacity





Machine Learning: Unsupervised Learning


Brown University


Georgia Institute of Technology

via
 Udacity





Reinforcement Learning


Brown University


Georgia Institute of Technology

via
 Udacity





Artificial Intelligence for Robotics


Stanford University

via
 Udacity





機器學習基石上 (Machine Learning Foundations)---Mathematical Foundations


National Taiwan University

via
 Coursera





 » Browse more Artificial Intelligence courses






1 review for Coursera's 機器學習技法 (Machine Learning Techniques) 


Write a review





 a year ago




Shaodong Qin
completed this course.
        














Was this review helpful to you?
            YES | NO





Write a review







How would you rate this course? *
Rating is required



How much of the course did you finish? *
This is a required field

Taking right now

Partially Completed

Completed

Audited

Dropped



Review
Review should be at least 20 words



Create Review















Close

Love to Learn?
Like our Facebook page to receive updates about the best online courses in the world










 





















 


機器學習技法 機器學習技法 (Machine Learning Techniques) 











Toggle Navigation





                    MOOC學院
                





搜索




註冊


登錄





首頁
MOOC職業課程專題演講社區下載MOOC同學



機器學習技法 (Machine Learning Techniques)普通證書

機器學習技法




關註
                (1392)


學過
                (28)


寫點評
            (11)

曬證書
        














8.8
(11人)


知識量：10
教師參與：10
趣味性：9.7
課程設計：9.7

難度：困難


學校：國立臺灣大學
平臺：
                Coursera
            
語言：中文授課
         中文字幕
教師：Hsuan-Tien Lin


開始時間：2015-11-10
持續時間：8.0周/每周8.0-24.0小時
去上課


分享

新浪
微信
豆瓣
QQ空間




你可能感興趣換一換





簡介
點評11
筆記0
討論4




Welcome! The instructor has decided to teach the course in Mandarin on Coursera, while the slides of the course will be in English to ease the technical illustrations. We hope that this choice can help introduce Machine Learning to more students in the Mandarin-speaking world. The English-written slides will not require advanced English ability to understand, though. If you can understand the following descriptions of this course, you can probably follow the slides. [歡迎大家！這門課將採用英文投影片配合華文的教學講解，我們希望能藉這次華文教學的機會，將機器學習介紹給更多華人世界的同學們。課程中使用的英文投影片不會使用到艱深的英文，如果你能瞭解以下兩段的課程簡介，你應該也可以瞭解課程所使用的英文投影片。]In the prequel of this course, Machine Learning Foundations, we have illustrated the necessary fundamentals that give any student of machine learning a solid foundation to explore further techniques. While many new techniques are being designed every day, some techniques stood the test of time and became popular tools nowadays.The course roughly corresponds to the second half-semester of the National Taiwan University course "Machine Learning." Based on five years of teaching this popular course successfully (including winning the most prestigious teaching award of National Taiwan University) and discussing with many other scholars actively, the instructor chooses to focus on three of those popular tools, namely embedding numerous features (kernel models, such as support vector machine), combining predictive features (aggregation models, such as adaptive boosting), and distilling hidden features (extraction models, such as deep learning). Course SyllabusEach of the following items correspond to approximately one hour of video lecture.[以下的每個小項目對應到約一小時的線上課程]Embedding Numerous Features [嵌入大量的特徵]-- Linear Support Vector Machine [線性支持向量機]-- Dual Support Vector Machine [對偶支持向量機]-- Kernel Support Vector Machine [核型支持向量機]-- Soft-Margin Support Vector Machine [軟式支持向量機]-- Kernel Logistic Regression [核型羅吉斯迴歸]-- Support Vector Regression [支持向量迴歸]Combining Predictive Features [融合預測性的特徵]-- Bootstrap Aggregation [自助聚合法]-- Adaptive Boosting [漸次提昇法]-- Decision Tree [決策樹]-- Random Forest [隨機森林]-- Gradient Boosted Decision Tree [梯度提昇決策樹]Distilling Hidden Features [萃取隱藏的特徵]-- Neural Network [類神經網路]-- Deep Learning [深度學習]-- Radial Basis Function Network [逕向基函數網路] -- Matrix Factorization [矩陣分解] Summary [總結]Recommended BackgroundThe basic knowledge on Calculus (differentiation), Linear Algebra (vector and matrix operations) and Probability (independent and dependent events) will be helpful. Some homeworks will require writing simple code so some programming background (on any platform) is recommended. We assume that the students have taken the NTU-Coursera "Machine Learning Foundations" class or equivalent. [我們希望修課的同學對於基本的微分、向量與矩陣運算、及機率的工具有所瞭解。有些作業會需要寫作或執行一些程式，所以我們建議修課的同學能在你所熟悉的平臺上有一些程式寫作的背景。我們假設修課的同學們已經學過「機器學習基石」或同等課程。] Suggested ReadingsThe lectures are designed to be self-contained. For reading before the course starts, we recommend (but do not require) that students refer to the book Learning from Data, which contains most of the background materials for this course. [這門課的錄影課程及投影片應該足以幫大家瞭解所有的內容。有關開課前的預備知識，我們推薦有興趣的同學們閱讀 Learning from Data 一書，該書包含了本課程所需的大部份背景知識。] Course FormatThe class will consist of lecture videos that contain integrated quiz questions. There will also be bi-weekly homeworks that are not part of video lectures. [這門課主要以線上錄影課程及其中的小測驗組成，每兩週我們會有另外的作業練習。]FAQWill I get a statement of accomplishment after completing this class? [我在完成課程後，是否能得到「修業合格證明」?]Yes. Students who pass the basic course requirements will receive a statement of accomplishment signed by the instructor. [是的，當同學成功地達成課程的基本要求後，即可收到由授課老師簽署的「修業合格證明」。]What equipment/resource do I need for taking this class? [修習此課需要哪些設備/資源？]You need access to some computing platform to run the code for some of the homework problems, and you can basically use any programming language of your choice. Each homework set should take no more than a whole day of your "machine time" on a modern PC if the algorithms are properly implemented. So you do not need a super-fast computing resource. [在有些作業的問題中，你需要在某些計算平臺上執行程式，而你可以使用任何你所愛的程式語言。如果你正確的撰寫演算法，在一般的個人電腦上，每次作業所需的「機器時間」應該不到一天。所以你不需要超快的運算資源。]Ｗhat can I learn from this course? [我在此課程可以有什麼收穫？]A solid understanding of the most popular tools in machine learning! [對機器學習中最熱門的工具有堅固的瞭解！] 




課程點評
            

寫點評
            



綜合評分：8.8難度：困難


知識量：10
教師參與：10
趣味性：9.7
課程設計：9.7



作業難
2


講解清楚
2


老師牛
1


乾貨
1


偏理論 
1


最新點評






毒毒程










已完成這門課估計是我在Coursera上單門課收穫最大的一門了。
這門課乾貨十足，課程內容清晰且實用，林老師人也非常好，和大家互動很多，很積極的回答大家的問題。課程作業難度很大，每次估計得花15小時，理論題目很像組合數學的題目，也需要算積分。編程題目的話基本上都是玩具類型的數據，但是很有利於自己去使用各種模型，如SVM，AdaBoost，Random Forest，NN等等，都是面向問題本身的quiz，不限定編程語言和方法。而且如果你不自己分析一下，完全按照要求，要得到正確結果很花時間。。。比如運行NN500次取平均，很明顯是林老師想要大家不那麼老師。雖然不想自己造輪子，但是還是造了一個AdaBoost和一個NN的輪子。。。AdaBoost是當時沒註意sklearn上有，NN的話真是在python上找不到好用的簡潔的實現啊。。。不得不說自己寫模型對自己鍛煉很大。
由於錯過了基石課程，上課過程中看完了基石課程的視頻，雖沒做作業。看過這門課有利於技法課程的理解，比如課程中經常說的VC，對於想要深入學習的同學來說真是非常有益。
最後超級感謝林老師提供這麼好的課程！聽說以後這門課可能絕種了，但是仍希望林老師能繼續開課。
曬證書





2016-02-09 15:02


0





1


                                                    毒毒程
                                            







取消









悶聲敲代碼123










已完成這個冬天靠幾門課程過活：
密歇根大學的NLP
Ng的ML
臺大林軒田的機器學習基石和機器學習技法。
這幾門課中，NLP我放棄了，因為時間不足。
剩下幾門課，從心裡評個分的話，我會給機器學習技法最高分。
基石那門課略顯枯燥，雖然非常有用，但是更偏向理論
這門課更偏向實際，講解的算法也都是ML非常典型和經典的算法。
所以我上課的時候非常開心，開心能上這麼一門好課程。
註意，ML不只有神經網絡，那些經典的回歸分類和聚類算法，都是值得一看一學的。（雖然最後數據量足夠的，算法都會轉到NN上面去ORZ）
曬證書





2015-12-24 16:10


2





0









取消









zju的PhD










已完成這門課程是據林老師說是臺大機器學習課程的第二部分，我上過NG的機器學習導引，但是沒有上過機器學習基石課程，最後也跟下來了，我的感受是技法課程無論是在課程內容上還是作業上都會有基石的影子，所以說技法課程是基石課程的延伸再準確不過了，比如導引課程中說training data, validation data, testing data 的error, 基石和技法課程則稱之為E_in, E_out，又如在作業中會出現decision_stump，這在基石課程中出現過的，我當時就花了些時間來理解。總的來說，這門課程理論性更高，作業基本上都是需要你自己去選擇編程語言和相應的第三方庫函數來幫助完成的，認真做作業的話一周花個10小時不為過吧。
曬證書





2015-04-05 20:50


0





0









取消









Frank_Xu










已完成好課，以後要把基石課給補上，基石課什麼時候重新開班啊。。。
曬證書





2015-03-11 21:38


0





0









取消









所羅門捷列夫










已完成如果聽過《機器學習基石》的小伙伴一定不會對這門課的課程設計和難度感到驚訝。相反地，我反而還覺得《技法》課程比《基石》課程更簡單一些。雖然如此，還是花了很多時間去完成作業。每次的作業雖然感覺只有20道選擇題，但每道題都不容易，特別是編程實現算法的題目，雖然都是上課講過的算法，但老師並沒有過多地講一些實現的細節，而這些細節在你運行程序的時候就會暴露出來。這也是很多人不能跟上課程的原因。

課程內容包括SVM、AdaBoost和Deep Learning三種算法為核心的三個機器學習的方向。課程不僅僅是簡單地把算法講過一遍，還比較重視每種算法的變體，以及各種算法之間的差異。

這門課也讓自己再次感受到機器學習的強大，同時也期待林軒田老師開設其他課程，比如《資料結構》等等，一定會前排圍觀。
曬證書





2015-03-11 18:31


0





2


                                                    所羅門捷列夫
                                            
                                                    Dirischlecht
                                            







取消









LaSalle










已完成同基石課程一樣，投入了很多時間和精力，但堅持下來收穫很大。對一些算法有了新的認識，作業是很好的練習，對編程有一定要求，認真做下來會加深對算法的理解。林老師及其整個團隊認真，負責，令人敬佩！課件太贊了！
2015-03-11 17:56


0





0









取消









iV0id










已完成1. 這門課是林軒田老師另一門課《機器學習基石》的後續課程，需要現學基石，再學技法；
2. 同Andrew Ng的機器學習課程相比，林老師的兩門課更有深度，更重理論，趣味性上不輸Ng的課；Ng的課強在註重實踐，Ng的課上有一些書本上不會寫的ML黑魔法；
3. 課程視頻不甚難，有一點本科微積分和統計的基礎基本都能看懂；
4. 課後習題很重公式推導和程序實驗，理論與實踐並重。數學基礎不好的人推導公式時會感覺十分吃力；程序實驗題需要用到一些常用的機器學習包比如cvxopt，libsvm等，但是課程里並沒有相應的引導，所以之前沒接觸過這些包的人也會十分吃力。 過年回家做作業做到半夜兩點鐘這種事情你以為我會告訴你嗎
曬證書





2015-03-11 17:17


0





0









取消









ern










已完成1. 這門課程是《機器學習基石》的後續，所以最好是兩門一起學；
2. 還是需要線性代數和微積分的基礎，特別是線性代數；
3. 作業還是一如既往的側重基礎概念+部分代碼實現，不過這個課程的代碼要求比《機器學習基石》要多一點，後面三次作業都大概有40-50%的題目靠代碼；
4. 課程負擔、主要內容，還是看我對《機器學習基石》的評價吧。這個課程雖說是技法，但也是基礎、推導占大頭，和Ng的Coursera課程側重點有些差異。
不說那麼多，補習線性代數去了。真心覺得我可以再從基礎開始自學一次，仍然有些東西不很懂。ML真是個大坑。
曬證書





2015-03-11 14:32


0





0









取消





查看全部點評 >


課程筆記沒有人寫筆記耶……放著我來！
            
寫筆記
            


最新討論
            
發新帖

有人下載了視頻嗎？現在看不了課了

野生妹子保護中心
8
2017-03-19

請問有同學有留著技法的考試題嗎？現在coursera上面進不到課程裡面了。。。

ninicarol
4
2016-07-13

【影片】神之賣萌！！！ ML也能這麼可愛！

台灣大學MOOC
5
2014-12-24

機器學習系列課程 -- 技法 將在12月23日開課！

台灣大學MOOC
0
2014-11-21


相關課程






The Data Scientist's Toolbox
數據師的工具箱








Image and Video Processing: From Mars to Hollywood with A Stop at The Hospital
圖像和視頻處理：從火星、醫院到好萊塢








機器學習基石 (Machine Learning Foundations)
機器學習基石








Neural Networks for Machine Learning
機器學習中使用的神經網絡








R Programming
R語言程序開發








媒體報道
關於我們
使用幫助
去果殼網
聯繫我們


關註我們，隨時掌握全球最新好課
新浪微博



                京ICP證100430號    京網文[2015] 0609-239號    新出發京零字東150005號    
                京公網安備11010502007133號
©2017果殼網




關於我們
新手指南























林軒田教授機器學習技法 Machine Learning Techniques 第 1 講學習筆記
























Fukuball



fukuball
我是林志傑，網路上常用的名字是 Fukuball，前 iNDIEVOX 技術長。我使用 PHP 及 Python，最近對機器學習感到興趣，所以空閒時會將 Python 有關機器學習的 Github Project 翻譯成 PHP 版本。 / 我也是一個快樂的吉他手～
Taipei, Taiwan
























NT$200
NT$500
NT$1000
NT$5000
NT$10000


        贊助 Fukuball
    








林軒田教授機器學習技法 Machine Learning Techniques 第 1 講學習筆記


 a year ago

 machine learning



前言
本系列部落格文章將分享我在 Coursera 上臺灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 機器學習基石系列 的碼農們，我建議可以先回頭去讀一下再回來喔！
範例原始碼：FukuML - 簡單易用的機器學習套件
我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 GitHub 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。
如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 FukuML 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 FukuML 了。不過我還是有寫 Tutorial 啦，之後會不定期更新，讓大家可以容易上手比較重要！
熱身回顧一下
從機器學習基石課程中，我們已經瞭解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。



線性分類回憶
回憶一下之前的課程中，我們使用 PLA 及 Pocket 來學習出可以分出兩類的線。



哪條線最好？
但其實可以將訓練資料分類的線可能會有很多條線，如下圖所示。我們要怎麼選呢？如果用眼睛來看，你或許會覺得右邊的這條線最好。



為何右邊這條線最好？
為何會覺得右邊這條線最好呢？假設先在我們再一次取得資料，可以預期資料與訓練資料會有點接近，但並不會完全一樣，這是因為 noise 的原因。所以偏差了一點點的 X 及 O 再左邊這條線可能就會不小心超出現，所以就會被誤判了，但在右邊這條線就可以容忍更多的誤差，也就比較不容易 overfitting，也因此右邊這條線最好。
如何描述這條線？我們可以說這條線與最近的訓練資料距離是所有的線中最大的。



胖的線
我們希望得到的線與最近的資料點的距離最大，換的角度，我們也可以說，我們想要得到最胖的線，而且這個胖線還可以將訓練資料分好分類。



Large-Margin Separating Hyperplane
這種胖的線名稱就叫 Large-Margin Separating Hyperplane，原本的問題就可以定義成要找最大的 margin，而且還要分好類，也就是 yn = sign(wTXn)。
最大的 margin 可以轉換成點與超平面之間最小的距離 distance(Xn, w)，然後 yn = sign(wTXn)，就代表 Yn 與 score 同號，所以可以轉換成 YnwTXn > 0，我們需要求解滿足這些條件的超平面。



點與超平面的距離 - 符號解釋
點與超平面的距離怎麼算呢？在這邊的推導，我們需要暫時將 w0 分出來，寫成 b，所以我們之前熟悉的 wTXn 在這邊暫時變成 wTxn + b，以方便推導。



點與超平面的距離 - 推導
如果我們現在有一個超平面 wTx + b = 0，假設 x' 與 x'' 都在這個超平面上，也就是 wTx' + b 及 wTx'' + b 都會是 0，如此就會得到 wTx' = -b 及 wTx'' = -b。現在我們將 wT 與 (X''- X') 相乘，由於剛剛的式子，我們會得到 0。(X''- X')是一個在 wTx + b = 0 超平面上的向量，W 與這個向量相乘會是 0 就代表 w 是這個超平面的法向量，要算 x 與超平面的距離，就是將 (x-x') 這個向量投影到 w，就可以算出點與超平面的距離了，公式如下所示。



分開訓練資料的超平面
由於我們要求的事可以分開訓練資料的超平面，因此已有 yn(wTXn+ b) > 0 這個條件，也因此距離公式中的 |wTx+b| 可以用 yn(wTXn + b) 來取代，這樣會比較容易求解。



減少超平面解的數量
觀察一下下圖中所有求解的條件，我們可以再進一步簡化。假設我們要找的是 wTx + b = 0 這個超平面，我們對這個超平面進行縮放其實是沒有任何影響的，現在我們也將 wTx + b 進行放縮，讓它跟 yn 相乘會是 1，也就是 yn(wTXn + b) = 1，這樣原本的 margin(b,w) 就是可以轉換成 1 除以 w 的長度，我們只要求讓這個值最大的平面就可以了。 



再次簡化問題
經過上述的推導，我們的問題變成求滿足(1) max 1/||w|| 及 (2) min yn(wTXn + b) = 1 這兩個條件的超平面，但這樣我們好像還是覺得有些複雜不會解，可以再這麼簡化呢？
min yn(wTXn + b) = 1 這個條件我們可以讓它的限制再鬆一點，只要 yn(wTxn+b)>=1 就好了，理論上保證最後得到的解，一定會有等於 1 的情況，而不會全部都大於 1。
另外 max 1/||w|| 我們改成 min ||w||，||w|| 是 wTw 開根號，我們可以不理根號然後乘上 1/2 以方便後面的推導，所以轉換成 min 1/2(wTw)。 



解一個簡單的問題來看看
現在我們求解的條件變成求 min 1/2(wTw) 且 yn(wTXn + b)>=1 的超平面，我們用一個簡單的例子來求解看看。如下圖所示，我們可以找出 w1 = 1, w2 = -1, b = -1 滿足我們的條件，這個超平面就是 x1 - x2 - 1，這個超平面也稱為 Support Vector Machine（SVM）。 



支持向量機（Support Vector Machine）
從上面這個簡單例子，讓我們來瞭解一下支持向量機。首先，這個向量可以由 margin 的公式得出 marging 的寬度值。我們會發現有些點會剛好在這個 margin 的邊界上，這些點就是所謂的支持向量（Support Vector）。這些支持向量可以標出胖線的位置，其他的點則無法，所以其他的點在這個問題上是不重要的點。
所以 SVM 的意思就是：透過 Support Vector 的協助來學習出最胖的超平面。



實際上怎麼解這個問題？
我們剛剛只是從一個簡單的例子來解 SVM，那實際上怎麼解這個問題呢？之前我們學過 gradient descent，在這邊好像沒用，因為有很多限制，我們不能讓演算法自由自在的計算 gradient descent。
但這個問題其實有現存的方法可以解，觀察所有的限制式就會發現 SVM 可以用二次規劃（quadratic programming）來找出最佳解。



二次規劃
我們把 SVM 的限制式，跟二次規劃的限制式做一個比較，就發現可以將問題轉換成二次規劃的相關參數，找出 SVM 問題在二次規劃時的 u, Q, p, a, c，我們就可以把這些參數丟到 QP Solver 來幫我們解 SVM。
目前各語言都有提供 QP Solver，但是介面參數可能會不相同，要自己讀文件去瞭解各個參數，自己做轉換。



第一個 SVM
使用 QP Solver 你就可以解第一個學會的 SVM，這個 SVM 是 hard margin（需要訓練資料線性可分），且學習出來的是線性模型，如果要做非線性模型，只要將資料做非線性轉換就可以了。



探討一下其中的理論
為何胖的超平面會比較好呢？除了我們前述用例子說明之外，有沒有什麼理論基礎呢？其實如果觀察 SVM 的限制式，我們把它拿來與正規化做比較，會發現 SVM 與正規化實際上做的事情很類似，也因此兩個方法都有避免 Overfitting 的能力。



SVM 的優勢
SVM 的優勢在哪呢？之前我們學過可以將資料做特徵轉換到高維度，讓假設集合變多，可以學習更複雜的模型，但這很可能會造成 overfitting。但簡單的模型假設集合太少，無法學習複雜的模型。
我們希望可以讓假設集合不是太多，但又可以學習較複雜的模型，SVM 就可以兩全其美，他比起正規化更好的地方就在於正規化需要對原來的演算法作調整，但 SVM 本身就像是一個具備正規化的演算法。



總結
在這一講我們瞭解了 SVM，且知道 SVM 的特性就是去找出可以將訓練資料分好的一條最胖的超平面。而實務上我們會用二次規劃的工具來解 SVM。













Please enable JavaScript to view the comments powered by Disqus.
comments powered by Disqus


















沒有符合條件的頁面。 – 資料科學年會網站












































404.php
 







 歡迎踴躍報名  





徵求講者、投稿
邀請對資料科學及人工智慧有熱情、有心得的專家上場演出，帶領與會者一起攀爬領域的高峰！











 即將開始招募
 





徵求志工
台灣資料科學年會四歲了，歡迎認同年會宗旨又喜歡辦活動的朋友、夥伴們，加入我們的志工籌備團隊！











 敬請期待
 





報名年會
敬請期待











籌辦單位 | 媒體夥伴 | 工作人員 |
                第一屆年會 |
                第二屆年會 |
                第三屆年會 |
                年會粉絲頁
© Copyright 2017 - 台灣資料科學協會 

































































































































































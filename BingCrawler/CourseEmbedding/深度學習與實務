





天瓏網路書店-TensorFlow+Keras 深度學習人工智慧實務應用



































天瓏網路書店
全台最齊全
中英文電腦書專賣店







天瓏資訊圖書粉絲專頁




搜尋


資料科學
簡體書
Packt
微服務
工程師必讀經典
英文書新到貨
Python
Tensorflow
無瑕程式碼








        TensorFlow+Keras 深度學習人工智慧實務應用
        


        林大貴 著
        
      




 預覽內頁



出版商:
博碩


出版日期:
2017-06-08


定價:
$590


售價:

7.8 折
          $460



語言:
繁體中文


頁數:
384


ISBN:
9864342169

ISBN-13:
9789864342167


相關標籤:

機器學習、深度學習、TensorFlow、人工智慧






      立即出貨
        (庫存 > 10)




























買這商品的人也買了...







 
$360
              售價: $281
          
深度學習快速入門—使用 TensorFlow (Getting started with TensorFlow)





 
$520
              售價: $411
          
資料結構--使用Python





 
$450
              售價: $356
          
Effective SQL 中文版 | 寫出良好SQL的61個具體做法





 
$590
              售價: $502
          
今天不學機器學習，明天就被機器取代：從 Python入手+演算法





 
              貴賓價: $450
          
Tensorflow：實戰Google深度學習框架





 
$580
              售價: $452
          
Python 機器學習 (Python Machine Learning)





 
              貴賓價: $393
          
面向機器智能的TensorFlow實踐





 
              貴賓價: $450
          
TensorFlow 實戰





 
              貴賓價: $450
          
深度學習原理與TensorFlow實踐





 
$360
              售價: $284
          
完整學會 Git, GitHub, Git Server 的24堂課





 
$580
              售價: $458
          
網站擷取｜使用Python (Web Scraping with Python: Collecting Data from the Modern Web)





 
$680
              售價: $537
          
資料視覺化｜使用Python與JavaScript (Data Visualization with Python and JavaScript: Scrape, Clean, Explore & Transform Your Data)





 
$580
              售價: $458
          
演算法技術手冊, 2/e (Algorithms in a Nutshell: A Practical Guide, 2/e)





 
              貴賓價: $450
          
深度學習 : Caffe 之經典模型詳解與實戰





 
              貴賓價: $730
          
深度學習、優化與識別 (Deep Learning,Optimization and Recognition)





 
$520
              售價: $406
          
R 語言資料分析：從機器學習、資料探勘、文字探勘到巨量資料分析, 2/e





 
$450
              售價: $356
          
Effective C# 中文版 | 寫出良好 C# 程式的 50個具體做法, 3/e (Effective C# : 50 Specific Ways to Improve Your C#(Covers C# 6.0), 3/e)





 
$790
              售價: $616
          
無瑕的程式碼 ── 敏捷完整篇 ── 物件導向原則、設計模式與C#實踐 (Agile principles, patterns, and practices in C#)





 
$860
              售價: $679
          
王者歸來：和大師一起動手--撰寫一個完整的作業系統





 
$560
              售價: $437
          
不止是測試：Python 網路爬蟲王者 Selenium




產品描述

人工智慧時代來臨，必須學習的新技術
輕鬆學會「深度學習」：先學Keras再學TensorFlow

★成長最快領域：深度學習與類神經網路，是人工智慧成長最快的領域，讓電腦更接近人類的思考。
★應用深入生活：手機語音助理、人臉識別、影像辨識、手寫辨識、醫學診斷、自然語言處理。
★實作快速上手：只需Python基礎，依照本書Step by Step學習，就可以輕鬆學會深度學習概念與應用。

TensorFlow功能強大、執行效率高、支援各種平臺，然而TensorFlow是低階的深度學習程式庫，學習門檻高。所以本書先介紹Keras，Keras是高階的深度學習程式庫（以TensorFlow作為後端引擎），對初學者學習門檻低，可以很容易地建立深度學習模型，並且進行訓練、預測。等讀者熟悉深度學習模型概念與應用後，再來學習TensorFlow就很輕鬆了。

【在Windows安裝TensorFlow 1.0＋Keras2.0】
對於初學者而言，在Windows安裝非常簡單容易上手。本書詳細步驟說明，如何在Windows作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【在Linux Ubuntu安裝TensorFlow 1.0＋Keras2.0】
因為Linux作業系統是大數據分析與機器學習很常用的平臺。本書詳細步驟說明，如何在Linux Ubuntu作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【使用GPU大幅加快深度學習訓練】
GPU的平行運算架構，可讓深度學習訓練比CPU快數十倍。您必須有Nvidia顯示卡。然後依照本書步驟說明，安裝Cuda、CudNN以及TensorFlow GPU版本，就可以使用GPU大幅加快深度學習訓練。

【MNIST手寫數字影像辨識，可辨識0~9的手寫數字】
以實際範例說明，如何使用Keras與TensorFlow建構MLP（多層感知器）、CNN（捲積神經網路）模型，可辨識0~9的手寫數字。

【CIFAR-10照片影像物體辨識，可辨識10種物體】
以實際範例說明，如何使用Keras建構CNN（捲積神經網路）模型，可辨識照片類別：飛機、汽車、鳥、貓、鹿、狗、青蛙、船、卡車。

【預測鐵達尼號旅客生存機率】
以實際範例說明，如何使用Keras建構MLP（多層感知器）模型、可以預測旅客及鐵達尼號電影男女主角生存機率，並且找出鐵達尼號其他旅客的感人故事。

【IMDb影評文字「自然語言處理」與「情緒分析」】
情緒分析的商業價值，在於透過文字分析，得知顧客對公司或產品的評價，以調整營運策略。本書以實際範例說明，如何運用Keras自然語言處理，並且建構MLP（多層感知器）、RNN（遞歸神經網路）、LSTM（長短期記憶）等模型，可以預測影評文字是正面或負面評價。

作者簡介

林大貴
作者從事IT產業多年，系統設計、網站開發、數位行銷、商業智慧、大數據、機器學習等領域，具備豐富的實務經驗。目前從事大數據分析、機器學習、深度學習與人工智慧，相關的研究、教學與企業顧問。

【facebook粉絲團】
https://www.facebook.com/TensorflowKeras/

【部落格】
http://tensorflowkeras.blogspot.tw/

目錄大綱

CHAPTER01 人工智慧、機器學習、深度學習介紹
CHAPTER02 深度學習的原理
CHAPTER03 TensorFlow與Keras介紹
CHAPTER04 在Windows安裝TensorFlow與Keras
CHAPTER05 在Linux Ubuntu安裝TensorFlow與Keras
CHAPTER06 Keras MNIST手寫數字辨識資料集介紹
CHAPTER07 Keras多元感知器（MLP）辨識手寫數字
CHAPTER08 Keras捲積神經網路（CNN）辨識手寫數字
CHAPTER09 Keras Cifar-10影像辨識資料集介紹
CHAPTER10 Keras捲積神經網路（CNN）辨識Cifar-10影像
CHAPTER11 Keras鐵達尼號旅客資料集介紹
CHAPTER12 Keras多層感知器（MLP）預測鐵達尼號旅客生存機率
CHAPTER13 IMDb網路電影資料集與自然語言處理介紹
CHAPTER14 Keras建立MLP、RNN、LSTM模型，進行IMDb情緒分析
CHAPTER15 TensorFlow程式設計模式介紹
CHAPTER16 以TensorFlow張量運算模擬神經網路運作
CHAPTER17 TensorFlow Mnist手寫數字辨識資料集介紹
CHAPTER18 TensorFlow多層感知器MLP辨識手寫數字
CHAPTER19 TensorFlow捲積神經網路CNN辨識手寫數字
CHAPTER20 TensorFlow GPU版本安裝
CHAPTER21 使用GPU加快TensorFlow與Keras訓練
附錄A 本書範例程式下載與安裝說明








中文書籍分類

最新新書
2016 年度暢銷排行
暢銷排行 (2017/06)
暢銷排行 (2017/05)
暢銷排行 (2017/04)



活動主題列表

資料科學
CRC好書上架
簡體中文書最新到貨
PACKT 最新到貨
Effective 系列書
微服務系列書
Ruddy老師的敏捷教室
程式設計必讀經典系列
Springer好書上架
英文書最新到貨
資料視覺化系列書
Python 系列書籍
深度學習系列書籍
無瑕的程式碼 超值合購
Maker小天地



特價書籍

Bootstrap系列76折起
松崗暢銷書展5折起










TensorFlow+Keras 深度學習人工智慧實務應用-金石堂網路書店














































會員功能列

 


加入會員
登入




您好
登出


會員專區
客服中心
查訂單


 
（ 0 件）0 元
結帳












購物車
（ 0 件）0 元
結帳



金石堂及銀行均不會請您操作ATM!  如接獲電話要求您前往ATM提款機，請不要聽從指示，以免受騙上當!
    (詳情)




TensorFlow+Keras 深度學習人工智慧實務應用－金石堂網路書店歡迎您加入博客和facebook 臉書粉絲團！The largest retail books store chains in Taiwan! 





TOP關鍵字

2017希望書包
怪獸的產地
七龍珠
肆一 
蠟筆小新套票
抽EC






全館搜尋
中文書
英文書
簡體書
雜誌
MOOK
文具
玩具親子
美妝配飾
3C
家電
運動休閒
居家生活
動漫部屋
日用清潔
食品




 

天花版bn

12345678910





購物目錄


首頁
3h快送
中文書
英文書
簡體書
雜誌
MOOK
文具
美妝配飾
玩具親子
3C
家電
運動休閒
居家生活
日用清潔
動漫精品
食品






商品次目錄


新書
預購書
推薦書
套書
暢銷書排行榜
書展 / 特惠
讀者書評
出版社專區
香港出版品




 

    您的位置：金石堂網路書店
中文書
電腦資訊
資料庫
其它資料庫
商品詳情

 


TensorFlow+Keras 深度學習人工智慧實務應用
                        










作者：林大貴
追蹤



出版社：博碩文化  
出版社追蹤


出版日：2017/6/3

ISBN：9789864342167
語言：中文繁體
適讀年齡：全齡適讀


定價：590 元

特價：79 折 466 元 (可得紅利4點)
紅利優惠價：77 折 452 元 (折抵說明)紅利可抵：14 元 
信用卡紅利：可折抵多家銀行 (扣抵說明)
運送方式：全球配送　香港到店　國內宅配國內店取　












<<



>>


https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219b.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-01.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-02.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-03.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-04.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-05.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-06.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-07.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-08.jpg
https://cdn.kingstone.com.tw/book/images/product/20131/2013120434219/2013120434219-09.jpg














                           參考庫存量:2本立即購買 預計出貨日：2017/7/21



金石堂讀者好評
 
0 個人說讚，看排行 >






電腦資訊分類追蹤
使用此功能請先登入金石堂網路書店正式會員。這是什麼？




分享：
                            
                           	  










書籍介紹
其他讀者也買
強力推薦
讀者好評(0)
下標籤
團體專屬服務






內容簡介作者目錄
詳細資料






內容簡介 top 
《TensorFlow+Keras 深度學習人工智慧實務應用》

人工智慧時代來臨，必須學習的新技術
輕鬆學會「深度學習」：先學Keras再學TensorFlow


	★成長最快領域：深度學習與類神經網路，是人工智慧成長最快的領域，讓電腦更接近人類的思考。
	★應用深入生活：手機語音助理、人臉識別、影像辨識、手寫辨識、醫學診斷、自然語言處理。
	★實作快速上手：只需Python基礎，依照本書Step by Step學習，就可以輕鬆學會深度學習概念與應用。

	TensorFlow功能強大、執行效率高、支援各種平臺，然而TensorFlow是低階的深度學習程式庫，學習門檻高。所以本書先介紹Keras，Keras是高階的深度學習程式庫（以TensorFlow作為後端引擎），對初學者學習門檻低，可以很容易地建立深度學習模型，並且進行訓練、預測。等讀者熟悉深度學習模型概念與應用後，再來學習TensorFlow就很輕鬆了。

【在Windows安裝TensorFlow 1.0＋Keras2.0】
	對於初學者而言，在Windows安裝非常簡單容易上手。本書詳細步驟說明，如何在Windows作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【在Linux Ubuntu安裝TensorFlow 1.0＋Keras2.0】
	因為Linux作業系統是大數據分析與機器學習很常用的平臺。本書詳細步驟說明，如何在Linux Ubuntu作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【使用GPU大幅加快深度學習訓練】
	GPU的平行運算架構，可讓深度學習訓練比CPU快數十倍。您必須有Nvidia顯示卡。然後依照本書步驟說明，安裝Cuda、CudNN以及TensorFlow GPU版本，就可以使用GPU大幅加快深度學習訓練。

【MNIST手寫數字影像辨識，可辨識0~9的手寫數字】
	以實際範例說明，如何使用Keras與TensorFlow建構MLP（多層感知器）、CNN（捲積神經網路）模型，可辨識0~9的手寫數字。

【CIFAR-10照片影像物體辨識，可辨識10種物體】
	以實際範例說明，如何使用Keras建構CNN（捲積神經網路）模型，可辨識照片類別：飛機、汽車、鳥、貓、鹿、狗、青蛙、船、卡車。

【預測鐵達尼號旅客生存機率】
	以實際範例說明，如何使用Keras建構MLP（多層感知器）模型、可以預測旅客及鐵達尼號電影男女主角生存機率，並且找出鐵達尼號其他旅客的感人故事。

【IMDb影評文字「自然語言處理」與「情緒分析」】
	情緒分析的商業價值，在於透過文字分析，得知顧客對公司或產品的評價，以調整營運策略。本書以實際範例說明，如何運用Keras自然語言處理，並且建構MLP（多層感知器）、RNN（遞歸神經網路）、LSTM（長短期記憶）等模型，可以預測影評文字是正面或負面評價。



作者top 


作者介紹


林大貴

	作者從事IT產業多年，系統設計、網站開發、數位行銷、商業智慧、大數據、機器學習等領域，具備豐富的實務經驗。目前從事大數據分析、機器學習、深度學習與人工智慧，相關的研究、教學與企業顧問。

	【facebook粉絲團】
	www.facebook.com/TensorflowKeras/

	【部落格】
	tensorflowkeras.blogspot.tw/





目錄 top 
TensorFlow+Keras 深度學習人工智慧實務應用－目錄導覽說明


	CHAPTER01 人工智慧、機器學習、深度學習介紹
	CHAPTER02 深度學習的原理
	CHAPTER03 TensorFlow與Keras介紹
	CHAPTER04 在Windows安裝TensorFlow與Keras
	CHAPTER05 在Linux Ubuntu安裝TensorFlow與Keras
	CHAPTER06 Keras MNIST手寫數字辨識資料集介紹
	CHAPTER07 Keras多元感知器（MLP）辨識手寫數字
	CHAPTER08 Keras捲積神經網路（CNN）辨識手寫數字
	CHAPTER09 Keras Cifar-10影像辨識資料集介紹
	CHAPTER10 Keras捲積神經網路（CNN）辨識Cifar-10影像
	CHAPTER11 Keras鐵達尼號旅客資料集介紹
	CHAPTER12 Keras多層感知器（MLP）預測鐵達尼號旅客生存機率
	CHAPTER13 IMDb網路電影資料集與自然語言處理介紹
	CHAPTER14 Keras建立MLP、RNN、LSTM模型，進行IMDb情緒分析
	CHAPTER15 TensorFlow程式設計模式介紹
	CHAPTER16 以TensorFlow張量運算模擬神經網路運作
	CHAPTER17 TensorFlow Mnist手寫數字辨識資料集介紹
	CHAPTER18 TensorFlow多層感知器MLP辨識手寫數字
	CHAPTER19 TensorFlow捲積神經網路CNN辨識手寫數字
	CHAPTER20 TensorFlow GPU版本安裝
	CHAPTER21 使用GPU加快TensorFlow與Keras訓練
	附錄A 本書範例程式下載與安裝說明






詳細資料top 

                               語言：中文繁體規格：平裝分級：普級開數：23*17頁數：384
                               出版地：台灣















其它資料庫相關書籍
延伸閱讀推薦
延伸推薦









延伸推薦top 





Oracle安全實踐 來自第...

特價 179元
立即購買 




數據結構案例教程（C語言版）...

特價 143元
立即購買 




數據庫技術與應用：Acces...

特價 117元
立即購買 




數據結構教程：C++版（簡體...

特價 184元
立即購買 











共0篇好評top 
寫書評去 >










商品標籤 (什麼是標籤？)

作業系統



我的標籤新增









團體專屬服務top

團體專屬服務














訂購須知top 
防治詐騙，提醒您!!金石堂及銀行均不會請您操作ATM! 如接獲電話要求您前往ATM提款機，請不要聽從指示，以免受騙上當! 

商品運送說明：
當商品送達金石堂門市或便利商店後，您會收到E-mail及APP出貨/到貨通知，您也可透過【訂單查詢】確認到貨情況。
建議您可下載『金石堂APP』並開啟推撥設定，即可收到相關出貨/到貨通知訊息。
並請您於指定期限內取貨付款，若逾期未取，您取貨的金石堂門市或便利商店將會辦理退貨作業。
產品顏色可能會因網頁呈現與拍攝關係產生色差，圖片僅供參考，商品依實際供貨樣式為準。 

退換貨說明：
依據「消費者保護法」第19條及行政院消費者保護處公告之「通訊交易解除權合理例外情事適用準則」，以下商品購買後，除商品本身有瑕疵外，將不提供7天的猶豫期：

                            1、 易於腐敗、保存期限較短或解約時即將逾期。（如：生鮮食品）
                            2、 依消費者要求所為之客製化給付。（客製化商品）
                            3、 報紙、期刊或雜誌。（含MOOK、外文雜誌）
                            4、 經消費者拆封之影音商品或電腦軟體。
                            5、 非以有形媒介提供之數位內容或一經提供即為完成之線上服務，經消費者事先同意始提供。（如：電子書、電子雜誌、下載版軟體、虛擬商品…等）
                            6、 已拆封之個人衛生用品。（如：內衣褲、刮鬍刀、除毛刀…等）
若非上列種類商品，商品均享有到貨7天的猶豫期（含例假日）。
辦理退換貨時，商品（組合商品恕無法接受單獨退貨）必須是您收到商品時的原始狀態（包含商品本體、配件、贈品、保證書、所有附隨資料文件及原廠內外包裝…等），請勿直接使用原廠包裝寄送，或於原廠包裝上黏貼紙張或書寫文字。退回商品若無法回復原狀，將請您負擔回復原狀所需費用，嚴重時將影響您的退貨權益。










中文書籍分類

文學財經企管生活風格飲食料理心理勵志醫療保健旅遊宗教命理教育/親子教養童書羅曼史輕小說漫畫語言／字辭典藝術設計電腦資訊
程式設計
網頁設計
繪圖／影音／多媒體
辦公軟體／應用軟體
作業系統
資料庫
SQL Server
Access
Oracle
其它資料庫
資料庫概論


3C數位生活
APPLE專區
認證考試
網路／安全／概論
其他電腦資訊

自然科普人文歷史社會哲思考試書／政府出版品參考書全部的分類 >>



手機版
TensorFlow+Keras 深度學習人工智慧實務應用




相關商品


雜誌



PC DIY! 7月2017第245期
數位狂潮DIGITREND 2017第44期
PC HOME 電腦家庭 7月2017第258期
MAXIMUM PC Spcl: BUILD IT:The Perfect PC Vol.2 2017
PC DIY! 6月2017第244期









外嵌連結
TensorFlow+Keras 深度學習人工智慧實務應用



在金石堂門市找此書
                         選擇縣市：

請選擇
基隆市
臺北市
新北市
桃園市
新竹市
新竹縣
宜蘭縣
苗慄市
苗慄縣
南投縣
臺中市
雲林縣
彰化縣
嘉義市
臺南市
高雄市
屏東縣

查詢












↑回上方

金石堂網路書店

首頁
關於金石堂網路書店
人才招募
客服中心
異業合作
出版情報
手機版
關於金石堂書店
金石堂書店全台門市



客服專線：02-2364-9989
傳真：02-2364-4672
客服時間：週一至週五 9：00∼12：30 及 13：30∼18：00（例假日除外）
地址：100 台灣臺北市中正區汀州路三段 160 巷 3 號 2 樓
Copyright©2016, Digital Kingstone Co., Ltd. 金石網絡股份有限公司






瀏覽本站建議使用：Internet Explorer 8.0 以上或 FireFox、Google Chrome、Safari 等瀏覽器。
本網站已依台灣網站內容分級規定處理且符合電子商務、安全交易















金石堂APP出/到貨提醒不漏接，讓您便利隨行
















.20


R軟體之深度學習應用























經濟部工業局製造業價值鏈資訊應用計畫



資策會•數位教育研究所•科技化服務（ITeS）中心


【資策會臺北課程】Big Data、物聯網系列課程

							【Big Data課程】Big Data之處理與分析實務班

							【Big Data課程】Big Data之處理與分析進階班

							【Big Data課程】R軟體實作

							【Big Data課程】R軟體與資料探勘

							【Big Data課程】R軟體與資料視覺化

							【Big Data課程】R軟體與網頁資料擷取應用

							【Big Data課程】R軟體之金融大數據分析與應用

							【Big Data課程】R軟體與Shiny Web應用程式設計

							【Big Data課程】Python資料探勘實作

							【Big Data課程】大數據分析-使用SparkR

							【Big Data課程】Elasticsearch分散式系統實務班

							【Big Data課程】Spark大數據分析實務班

							【Big Data課程】網路爬蟲與Spark大數據流處理實務

							【Big Data課程】文字資料探勘實作班

							【Big Data課程】Big Data資料倉儲應用實務

							【Big Data課程】整合Big Data與BI實戰班

							【Big Data課程】大數據資料探勘與Weka分析

							【Big Data課程】Cloud Computing 雲端運算國際認證班

							【物聯網課程】物聯網規劃與應用實務班

							【物聯網課程】物聯網營運與提案企劃實務班

							【物聯網課程】物聯網工程師(EPCIE)國際認證班

							【主題館】資料科學家主題館

							【主題館】雲端運算課程主題館

							【主題館】服務科學主題館
 


















　




《8/12 開課，8/4前報名可享補助40%優惠！》





								「R軟體與Shiny Web應用程式設計」課程，同步招生中！










 ◆ 課程源起











													大資料(Big 
													Data)時代的來臨使得複雜資料建模的工作日形重要，Google、Microsoft、Facebook等公司正積極養成深度學習團隊，發展符合複雜資料建模的學習算法，架構進階的智能化系統。因此，深度學習已然成為資料科學家必備的武器之一，它除了應用在常見結構化資料上，亦擅長於處理語音、文字、視頻、影像等低結構化資料，本課程運用R語言深度學習可用之套件和函數(deepnet, 
													rnn, H2O, TensorFlow)，探討各類深度學習模型，優化超參數及避免過度配適，以提高模型應用的準確度。實作案例涵蓋時間序列分析、圖像辨識、以及自然語言處理等應用。

      
													「R語言深度學習應用」課程從類神經網路理論發展沿革開始介紹，涵蓋非監督式與監督式深度學習模型，透過案例介紹常用工具框架，輔以整合式開發環境RStudio進行實機操作，搭配遠端多CPU與GPU的工作站演示，逐步達成深度學習精準建模的目標 
													。







◆ 課程目標










												學習深度學習各種方法的理論與應用實務，並能運用H2O、TensorFlow、及其它R軟體深度學習函式庫，期能讓學員短時間內掌握深度學習的重要知識。








◆ 課程特色












本課程涵蓋深度學習主要技術，以簡單易懂的實作方式，期能讓學員短時間內擁有深度學習的基礎知識。



													除了實機操作外，並分享在氣象、交通、互聯網、零售、金融與製造等領域的實戰分析經驗，讓學員掌握大資料know-how的全貌，明瞭不同方法、工具與應用領域之間的差異，活用資料分析技術與抓住未來發展趨勢。










◆ 預備知識










												此課程將使用到基本的R軟體操作，建議學員至少先修習過「Big 
												Data資料分析首部曲-R軟體實作」，或具R軟體基礎，再修習本課程。







◆ 招生對象











													資料分析師/科學家、程式設計師、資訊/電機/機械/生醫/工工/化工/土木等相關工程師、醫師、教育人員、統計人員、財金人員、營銷人員、管理人員、法務人員、公務機關人員。

對此議題有興趣者。








◆ 課程內容








   



課程單元 
課程內容

時數



類神經網路基礎



														l


														R語言類神經網路建模 (ex. 
														單層&多層、神經網路架構&最佳化函數選擇...)


														l


														深度學習簡介(ex. H2O)


														l


														過度配適與係數縮減


														4



非監督式學習



														l

														自動編碼器(Autoencoders)應用

														2



監督式學習



														l


														重現式類神經網路
    
														(Recurrent Neural 
														Network)應用

														l


														受限波茲曼機


   
														(Restricted Boltzmann 
														Machines)應用


														l


														深度信念網路(Deep 
														Belief Networks)應用

														6


 課程執行單位保留調整課程內容與講師之權利    







◆ 課程時數








 2017年 
8/12(六)、8/13(日) 
												，每日 9:30~16:30上課，共二天，計12小時。







◆ 上課地點








資訊工業策進會數位教育研究所，臺北市信義路三段153號10樓。


												上課地點位於捷運大安站1號出口左轉約20公尺。
                                      * 上課地點與教室之確認，以上課通知函為主。







◆ 報名方式









本課程採線上報名，請按右方【我要報名】進入報名系統(依據工業局人才培訓專案規範，接受補助之學員須完整填具學員基本資料表並於開課當天簽署一份「工業局個資告知同意書」，否則無法接受補助，結訓學員應配合經濟部工業局培訓後電訪調查。)

課程諮詢：(02)6631-6533 課程經理 
												黃小姐，E-mail：julie620@iii.org.tw
												報名確認：(02)6631-6535 孫小姐，E-mail：chelseasun@iii.org.tw
 











◆ 課程費用與繳費









1.
本課程原費用NT$9,000元，費用含課程、講義及餐點。


2.


 
									本課程由工業局負擔40%，凡於8月4日前完成報名與繳費資料之學員，可享特惠價 NT$5,400元 (含稅)。( 
						政府捐助(贈)財團法人、學校教職員工及非本國國籍者不在補助範圍內 )



3.

團報優惠：二人團報可打95折、四人團報可打9折優惠。


　






									●團報優惠與早鳥優惠可一併使用。


									●課程報名系統每個帳號僅提供一位學員報名，若為團報學員，每位皆必須完成線上報名，並於其他金額說明欄位註記「與XXX一起團報」，以利事後核對名單。


4.


						特殊身份優惠：如為「身心障礙者、原住民、低收入戶、中堅企業員工」等特殊身份學員，享有經濟部工業局負擔60%之優惠，前三者學員需提供政府機關核發之身份證明文件影本，中堅企業員工需提供在職證明文件正本，請參中堅企業名單與中堅企業躍升計畫介紹。




												5.

請以信用卡或即期支票支付—


　

● 以信用卡支付者：請務必將正確信用卡資料填寫於繳費方式資料表。
● 以即期支票支付者：抬頭為「財團法人資訊工業策進會」， 劃線並禁止背書轉讓
                                                 　
                                               ，以掛號寄至臺北市106信義路三段153號9樓之1 邱小姐 收。








◆ 報名確認與取消









1.
已完成報名與繳費之學員，課程主辦單位將於開課三天前以E-mail方式寄發上課通知函；若課程因故取消或延期，亦將以E-mail方式通知，如未收到任何通知，敬請來電確認。


2.
已完成繳費之學員如欲取消報名，請於實際上課日前以書面通知業務承辦人，主辦單位將退還90%課程費用。


3.
學員於培訓期間如因個人因素無法繼續參與課程，將依課程退費規定辦理之︰上課未逾總時數三分之一，欲辦理退費，可退還
												所有課程費用二分之一；上課逾總時數三分之一，則不退費。


4.
課程執行單位保留是否接受報名之權利。


5.

                                                             
												如遇不可抗拒之因素，課程主辦單位保留修訂課程日期及取消課程的權利。








◆ 
										餐點服務












												本課程上課期間，每天都會提供午餐，學員不需自行準備或額外付費。








◆ 結業證書









  依資訊工業策進會ISO品質系統之「教育訓練服務程序」規定，本課程為短期班，參訓學員缺課未超過總時數五分之一者，結業時由本會核發結業證書。（本計畫課程需參加並通過課程評量，才可取得結業證書。）








◆ 學習護照











												資策會為人事行政局首批認證審定之民間學習機構，參加本課程之中小企業員工，亦可獲得「中小企業終身學習護照」認可之時數；此外，公務人員參加資策會課程，學習時數可登錄「公務人員終身學習護照」。








◆ 
										最低招生人數









 

												最低招生人數至少為12人，預計開班人數為25人。








◆ 
										相關課程









 

Big Data之處理與分析(Hadoop)實務班

Big Data之處理與分析(Hadoop)進階班

Big Data資料分析首部曲-R軟體實作

Big Data資料分析二部曲-R軟體與資料探勘

Big Data資料分析三部曲-R軟體與資料視覺化

R軟體與網頁資料擷取應用

R軟體之金融大數據分析與應用

R軟體與Shiny Web應用程式設計

Python資料探勘實作

大數據分析-使用SparkR

Elasticsearch分散式系統實務班

Spark大數據分析實務班

網路爬蟲與大數據流處理實務班

文字資料探勘實作班
















資訊工業策進會數位教育研究所  版權所有，禁止侵害，違者必究。
Copyright (c) 2013 III Digital Education Institute. All Rights
      Reserved 








沒有符合條件的頁面。 – 資料科學年會網站












































404.php
 







 歡迎踴躍報名  





徵求講者、投稿
邀請對資料科學及人工智慧有熱情、有心得的專家上場演出，帶領與會者一起攀爬領域的高峰！











 即將開始招募
 





徵求志工
台灣資料科學年會四歲了，歡迎認同年會宗旨又喜歡辦活動的朋友、夥伴們，加入我們的志工籌備團隊！











 敬請期待
 





報名年會
敬請期待











籌辦單位 | 媒體夥伴 | 工作人員 |
                第一屆年會 |
                第二屆年會 |
                第三屆年會 |
                年會粉絲頁
© Copyright 2017 - 台灣資料科學協會 






































































































































































[系列活動] 手把手的深度學習實務





































































































      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our User Agreement and Privacy Policy.
    

      Slideshare uses cookies to improve functionality and performance, and to provide you with relevant advertising. If you continue browsing the site, you agree to the use of cookies on this website. See our Privacy Policy and User Agreement for details.
    






SlideShare



Explore



Search



You








Home


Technology


Education


More Topics




For Uploaders





                    Get Started




                    Tips & Tricks




                    Tools














































    [系列活動] 手把手的深度學習實務
  





































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Upcoming SlideShare










Loading in …5
×




 










1

















1 of 252






























Like this presentation? Why not share!

Share
Email


 



 








[DSC 2016] 系列活動：李宏毅 / 一天搞懂深度學習
by 台灣資料科學年會
217129 views







[系列活動] Machine Learning 機器學習課程
by 台灣資料科學年會
9460 views







[系列活動] 機器學習速遊
by 台灣資料科學年會
12975 views







[系列活動] 使用 R 語言建立自己的演算法交易事業
by 台灣資料科學年會
12530 views







[系列活動] 手把手教你R語言資料分析實務
by 台灣資料科學年會
5467 views







[系列活動] 給工程師的統計學及資料分析 123
by 台灣資料科學年會
5697 views





 






Share SlideShare







Facebook




Twitter




LinkedIn




Google+





Email










Email sent successfully!



Embed



Size (px)



Start on




Show related SlideShares at end




WordPress Shortcode



Link



























                  [系列活動] 手把手的深度學習實務
                




                  15,822 views





Share


Like



                  Download
                











台灣資料科學年會




 Follow
                        





















              Published on Dec 13, 2016






                    本課程利用六個小時的時間，介紹 Keras 這個熱門的深度學習工具，從最簡單的前饋類神經網路 (Feedforward Neural Network) 開始，用 Keras 加入各種訓練技巧 (Regularization, Early Stopping, Dropout) 以得到好的預測模型。亦介紹深度學習模型的另一個變形：捲積式類神經網路 (Convolutional Neural Network, CNN)，以完整的實務操作，讓你邁出成為深度學習訓練大師的第一步。
                  



                    ...





Published in:
Data & Analytics







                    1 Comment
                





                  164 Likes
                





                Statistics
              




                Notes
              














Full Name






                          Comment goes here.
                        
12 hours ago  

                        

Delete
Reply
Spam
Block




Are you sure you want to
Yes
No



                          Your message goes here
                        





















Post

















鎮 梁


                                    , 
                                    BigFootCar - CEO Assistant


                                     at 
                                    Bigfootcar





                              識別人物圖片生成對應骨骼圖的機器學習工具有了嗎
                            


                                2 months ago
                              
  

                            

Reply 


                              



Are you sure you want to 
                                Yes 
                                No



                              Your message goes here
                            



















Shayuan Wei


                                , 
                                Senior Project Manager


                                 at 
                                Chuango Security Technology Corp.




                              1 day ago
                            













Yuqi Li


                                , 
                                SE


                                 at 
                                Bluetechnology




                              3 days ago
                            













Li-Chin Huang


                                , 
                                Systems Analyst at Department of Information Management Executive Yuan


                                 at 
                                Department of Information Management Executive Yuan




                              1 week ago
                            













Ji Jhe Zou


                                , 
                                --







                              2 weeks ago
                            













road labs


                                , 
                                --







                              3 weeks ago
                            







                    Show More
                    




No Downloads




Views

Total views

                      15,822
                    
On SlideShare

                      0
                    
From Embeds

                      0
                    
Number of Embeds

                      64
                    



Actions

Shares
0
Downloads

                      1,884
                    
Comments

                      1
                    
Likes

                      164
                    




                    Embeds
                    0


No embeds






















No notes for slide






                  [系列活動] 手把手的深度學習實務
              


      1.
    手把手教你深度學習實務
張鈞閔, 許之凡
中央研究院資訊科學研究所資料洞察實驗室
 
  


        2.
      
    Lecturers
 畢業於臺大電機系、臺大電機所
 中研院資訊所研究助理
 研究領域
 線上平臺定價自動化
 線上平臺使用者分析
 計算社會學
 臺大電機系博士班二年級
 中研院資訊所研究助理
 研究領域
 多媒體系統效能測量
 使用者滿意度分析
2
 
  


        3.
      
    3
DATA TOOL
MAN
今天目的：一把 ______ 鑰匙 to deep learning通用
 
  


        4.
      
    Outline
 What is Machine Learning?
 What is Deep Learning?
 Hands-on Tutorial of Deep Learning
 Tips for Training DL Models
 Variants - Convolutional Neural Network
4
 
  


        5.
      
    What is Machine Learning?
5
 
  


        6.
      
    一句話說明 Machine Learning
6
Field of study that gives computers the ability
to learn without being explicitly programmed.
- Arthur Lee Samuel, 1959
『
』
 
  


        7.
      
    Machine Learning vs Artificial Intelligence
 AI is the simulation of human intelligence processes
 Outcome-based: 從結果來看，是否有 human intelligence
 一個擁有非常詳盡的 rule-based 系統也可以是 AI
 Machine learning 是達成 AI 的一種方法
 從資料當中學習出 rules
 找到一個夠好的 function
能解決特定的問題
7
Artificial
Intelligence
Machine
Learning
 
  


        8.
      
    Goal of Machine Learning
 For a specific task, find a best function to complete
 Task: 每集寶可夢結束的“猜猜我是誰”
8
f*( ) =
f*( ) =
Ans: 妙蛙種子
Ans: 超夢
 
  


        9.
      
    Framework
9
Define a set
of functions
Evaluate
and
Search
Pick the best
function
 
  


        10.
      
    1. Define a Set of Functions
10
Define a set
of functions
Evaluate
and
Search
Pick the best
function
A set of functions, f(‧)
{f(ө1),…,f(ө*),…,f(өn)}
在北投公園的寶可夢訓練師
 
  


        11.
      
    2. Evaluate and Search
11
Define a set
of functions
Evaluate
and
Search
Pick the best
function
f1( ) =
f1( ) =
根據結果，修正 ө ：
避免找身上有皮卡丘的人 (遠離 ө1 )
f1=f(ө1)
 
  


        12.
      
    3. Pick The Best Function
12
Define a set
of functions
Evaluate
and
Search
Pick the best
function
找到寶可夢訓練大師
 
  


        13.
      
    Machine Learning Framework
Define a set
of functions
Evaluate
and
Search
Pick the best
function
北投公園的訓練師
評估、修正
找到最好的寶可夢專家
 
  


        14.
      
    What is Deep Learning?
14
 
  


        15.
      
    Deep Learning vs Machine Learning
 Deep learning is a subset of machine learning
15
Artificial Intelligence
Machine Learning
Deep Learning
 
  


        16.
      
    最近非常夯的技術
16
(Slide Credit: Hung-Yi Lee)
 
  


        17.
      
    17
 
  


        18.
      
    18
 
  


        19.
      
    Image Captioning
19
 
  


        20.
      
    Applications of Deep Learning
20
 *f
 *f
 *f
 *f
“2”
“Morning”
“5-5”
“Hello”“Hi”
(what the user said) (system response)
(step)
(Slide Credit: Hung-Yi Lee)
 Speech Recognition
 Handwritten Recognition
 Playing Go
 Dialogue System
 
  


        21.
      
    Fundamentals of Deep Learning
 Artificial neural network (ANN, 1943)
Multi-layer perceptron
 模擬人類神經傳導機制的設計
 由許多層的 neurons 互相連結而形成 neural network
21
 
  


        22.
      
    為什麼叫做 Deep Learning？
 當 hidden layers 層數夠多 (一般而言大於三層)
就稱為 Deep neural network
22
http://cs231n.stanford.edu/sli
des/winter1516_lecture8.pdf
AlexNet (2012) VGG (2014) GoogleNet (2014)
16.4%
7.3%
6.7%
(Slide Credit: Hung-Yi Lee)
8 layers
19 layers
22 layers
 
  


        23.
      
    A Neuron
23
Input
x1
xn
……
z = w1x1+w2x2+……+wnxn+b
̂y
wn
w1
Output
Weight
+
z
σ: Activation
function
b
Bias
σ(z)
̂y = σ(z)
…
Neuron
 
  


        24.
      
    Neuron 的運作
 Example
24
σ(z)=z linear function
z = w1x1+w2x2+……+wnxn+b
A neuron
5
2
z = w1x1+w2x2+……+wnxn+b
3
-1
Output
+
z
σ(z)
3
̂y
̂y = z = (-1)*5+3*2+3 = 4 Ө: weights, bias
 
  


        25.
      
    Fully Connected Neural Network
 很多個 neurons 連接成 network
 Universality theorem: a network with enough number
of neurons can present any function
26
X1
Xn
w1,n
w1,1
w2,n
w2,1
 
  


        26.
      
    Fully Connected Neural Network
 A simple network with linear activation functions
5
2
-0.5
+0.2
-0.1
+0.5
-1.5
+0.8
 
  


        27.
      
    Fully Connected Neural Network
5
2
+0.4
+0.1
+0.5
+0.9
0.12
0.55
-0.5
+0.2
-0.1
+0.5
-1.5
+0.8
 A simple network with linear activation functions
 
  


        28.
      
    給定 Network Weights
29
f( ) =
5
2
-0.5
+0.5
-0.1
+0.2
+0.4
+0.1
+0.5
+0.9 0.12
0.55
Given & ,
f(x,Ө)
Ө: weights, bias
A Neural Network = A Function
 
  


        29.
      
    Recall: Deep Learning Framework
32
Define a set
of functions
Evaluate
and
Search
Pick the best
function
特定的網絡架構
A set of functions, f(‧)
{f(ө1),…,f(ө*),…,f(өn)}
f(ө94)
f(ө87)
f(ө945)…
不斷修正 f 的參數
找到最適合的參數
f(ө*)
 
  


        30.
      
     output values 跟 actual values 越一致越好
 A loss function is to quantify the gap between
network outputs and actual values
 Loss function is a function of Ө
如何評估模型好不好？
33
X1
X2
̂y1
̂y2
y1
y2
L
f(x,Ө)
(Ө)
 
  


        31.
      
    目標：最佳化 Total Loss
 Find the best function that minimize total loss
 Find the best network weights, ө*
 𝜃∗ = argm𝑖𝑛
𝜃
𝐿(𝜃)
 最重要的問題: 該如何找到 ө* 呢？
 踏破鐵鞋無覓處 (enumerate all possible values)
 假設 weights 限制只能 0.0, 0.1, …, 0.9，有 500 個 weights
全部組合就有 10500 組
 評估 1 秒可以做 106 組，要約 10486 年
 宇宙大爆炸到現在才 1010 年
 Impossible to enumerate
34
 
  


        32.
      
    Gradient Descent
 一種 heuristic 最佳化方法，適用於連續、可微的目
標函數
 核心精神
每一步都朝著進步的方向，直到沒辦法再進步
35
當有選擇的時候，國家還是
要往進步的方向前進。
『
』http://i.imgur.com/xxzpPFN.jpg
 
  


        33.
      
    Gradient Descent
36
L(Ө)
Ө
𝜃0
lim
Δ𝜃→0
𝐿 𝜃0 + Δ𝜃 − 𝐿(𝜃0)
𝜃0 + Δ𝜃 − 𝜃0
=
𝜕𝐿
𝜕𝜃 𝜃=𝜃0
想知道在 𝜃0
這個點時，𝐿 隨著 𝜃 的變化
𝐿 𝜃0 + Δ𝜃 − 𝐿(𝜃0)
𝜃0 + Δ𝜃 − 𝜃0
翻譯年糕：𝜃 變化一單位，會讓 𝐿 改變多少 𝑳 對 𝜽 的 gradient
(𝑡 = 0)
Δ𝜃
In this case: 𝑳 對 𝜽 的 gradient < 0
 𝜃 增加會使得 Loss 降低
 𝜃 改變的方向跟 gradient 相反
 
  


        34.
      
    Gradient Descent
37
L(Ө)
Ө
𝜃1 = 𝜃0 − 𝜂
𝜕𝐿
𝜕𝜃 𝜃=𝜃0
𝜃0
𝝏𝑳
𝝏𝜽 𝜽=𝜽 𝟎
𝜼, Learning rate
一步要走多大
𝜃1
𝜃∗ = 𝜃 𝑛 − 𝜂
𝜕𝐿
𝜕𝜃 𝜃=𝜃 𝑛
沿著 gradient 的反方向走
𝜃∗
相信會有一天…
 
  


        35.
      
    影響 Gradient 的因素
38
x1
xn
…… z = w1x1+w2x2+……+wnxn+b
y
wn
w1
+
z
b
σ(z)
̂y = σ(z)
̂y
L(Ө)
Ө
𝜕𝐿
𝜕𝜃
=
𝜕𝐿
𝜕 𝑦
𝜕 𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃
=
𝜕𝐿
𝜕 𝑦
𝜕 𝑦
𝜕𝜃
1. 受 loss function 影響
2. 受 activation function 影響
…
𝜃1
= 𝜃0
− 𝜂
𝜕𝐿
𝜕𝜃 𝜃=𝜃0
 
  


        36.
      
    Learning Rate 的影響
39
loss
Too low
Too high
Good
epoch
𝜃1 = 𝜃0 − 𝜂
𝜕𝐿
𝜕𝜃 𝜃=𝜃0
 
  


        37.
      
    Summary – Gradient Descent
 用來最佳化一個連續的目標函數
 朝著進步的方向前進
 Gradient descent
 Gradient 受 loss function 影響
 Gradient 受 activation function 影響
 受 learning rate 影響
40
 
  


        38.
      
    Gradient Descent 的缺點
 一個 epoch 更新一次，收斂速度很慢
 一個 epoch 等於看過所有 training data 一次
 Problem 1
有辦法加速嗎？
A solution: stochastic gradient descent (SGD)
 Problem 2
Gradient based method 不能保證找到全域最佳解
 可以利用 momentum 降低困在 local minimum 的機率
41
 
  


        39.
      
    Gradient Descent 的缺點
 一個 epoch 更新一次，收斂速度很慢
 一個 epoch 等於看過所有 training data 一次
 Problem 1
有辦法加速嗎？
A solution: stochastic gradient descent (SGD)
 Problem 2
Gradient based method 不能保證找到全域最佳解
 可以利用 momentum 降低困在 local minimum 的機率
42
 
  


        40.
      
    Stochastic Gradient Descent
 隨機抽一筆 training sample，依照其 loss 更新一次
 另一個問題，一筆一筆更新也很慢
 Mini-batch: 每一個 mini-batch 更新一次
 Benefits of mini-batch
 相較於 SGD: faster to complete one epoch
 相較於 GD: faster to converge (to optimum)
43
Update once Update once Update once
Loss Loss Loss
 
  


        41.
      
    Mini-batch vs. Epoch
 一個 epoch = 看完所有 training data 一次
 依照 mini-batch 把所有 training data 拆成多份
 假設全部有 1000 筆資料
 Batch size = 100 可拆成 10 份  一個 epoch 內會更新 10 次
 Batch size = 10 可拆成 100 份  一個 epoch 內會更新 100 次
 如何設定 batch size?
 不要設太大，常用 28, 32, 128, 256, …
44
 
  


        42.
      
    Mini-batch 的影響
 本質上已經不是最佳化 total loss 而是在最佳化
batch loss
45
(Slide Credit: Hung-Yi Lee)
Gradient Descent Mini-batch
 
  


        43.
      
    Gradient Descent 的缺點
 一個 epoch 更新一次，收斂速度很慢
 一個 epoch 等於看過所有 training data 一次
 Problem 1
有辦法加速嗎？
A solution: stochastic gradient descent (SGD)
 Problem 2
Gradient based method 不能保證找到全域最佳解
 可以利用 momentum 降低困在 local minimum 的機率
46
 
  


        44.
      
    Momentum
47
L(Ө)
Ө
𝜃0
𝜃1
Gradient=0  不更新，陷在 local minimum
g0
m*g0
參考前一次 gradient (g0) 當作 momentum
 即使當下在 local minimum，也有機會翻過
g1=0
抵抗其運動狀態被改變的性質
 
  


        45.
      
    Introduction of Deep Learning
 Artificial neural network
 Activation functions
 Loss functions
 Gradient descent
 Loss function, activation function, learning rate
 Stochastic gradient descent
 Mini-batch
 Momentum
48
 
  


        46.
      
    Frequently Asked Questions
 要有幾層 hidden layers？
 每層幾個 neurons？
 Neurons 多寡跟資料多寡有關
 Intuition + trial and error
 深會比較好嗎？
 Deep for modulation
49
Output
Input
Input
Output
or
 
  


        47.
      
    Visualization of Modulation
50
Ref:Visualizing Higher-Layer Features of a Deep Network
1st hidden layer 2nd hidden layer 3rd hidden layer
各司其職、由簡馭繁，組織出越來越複雜的 feature extractors
 
  


        48.
      
    Visualization of Modulation
51
Ref: Deep Learning andConvolutional Neural Networks: RSIPVision Blogs
 
  


        49.
      
    Hands-on Tutorial
寶可夢雷達 using Pokemon Go Dataset on Kaggle
52
圖
 
  


        50.
      
    範例資料
 寶可夢過去出現的時間與地點紀錄 (dataset from Kaggle)
53
Ref: https://www.kaggle.com/kostyabahshetsyan/d/semioniy/predictemall/pokemon-geolocation-visualisations/notebook
 
  


        51.
      
    Raw Data Overview
54
問題: 會出現哪一隻神奇寶貝呢？
 
  


        52.
      
    寶可夢雷達 Data Field Overview
 時間: local.hour, local.month, DayofWeek…
 天氣: temperature, windSpeed, pressure…
 位置: longitude, latitude, pokestop…
 環境: closeToWater, terrainType…
 十分鐘前有無出現其他寶可夢
 例如: cooc_1=1 十分鐘前出現過 class=1 之寶可夢
 class 就是我們要預測目標
55
 
  


        53.
      
    Sampled Dataset for Fast Training
 挑選在 New York City 出現的紀錄
 挑選下列五隻常見的寶可夢
56
No.4 小火龍 No.43 走路草 No.56 火爆猴 No. 71 喇叭芽 No.98 大鉗蟹
 
  


        54.
      
    開始動手囉！Keras Go！
57
 
  


        55.
      
    Input 前處理
 因為必須跟 weights 做運算
Neural network 的輸入必須為數值 (numeric)
 如何處理非數值資料？
 順序資料
 名目資料
 不同 features 的數值範圍差異會有影響嗎？
 溫度: 最低 0 度、最高 40 度
 距離: 最近 0 公尺、最遠 10000 公尺
58
 
  


        56.
      
    處理順序資料
 Ordinal variables (順序資料)
 For example: {Low, Medium, High}
 Encoding in order
 {Low, Medium, High}  {1,2,3}
 Create a new feature using mean or median
59
UID Age
P1 0-17
P2 0-17
P3 55+
P4 26-35
UID Age
P1 15
P2 15
P3 70
P4 30
 
  


        57.
      
    處理名目資料
 Nominal variables (名目資料)
 {"SugarFree","Half","Regular"}
 One-hot encoding
 假設有三個類別
 Category 1  [1,0,0]
 Category 2  [0,1,0]
 給予類別上的解釋  Ordinal variables
 {"SugarFree","Half","Regular"}  1,2,3
 特殊的名目資料：地址
 臺北市南港區研究院路二段128號
 轉成經緯度 {25.04,121.61}
60
 
  


        58.
      
    處理不同的數值範圍
 先說結論：建議 re-scale！但為什麼？
61
X1
X2
w1
w2
1,2,…
1000,2000,…
W2 的修正(ΔW)對於 loss 的影響比較大
y
w1
w2
w1
w2
X1
X2
w1
w2
1,2,…
1,2,…
y
Loss 等高線 Loss 等高線
 
  


        59.
      
    處理不同的數值範圍
 影響訓練的過程
 不同 scale 的 weights 修正時會需要不同的 learning rates
 不用 adaptive learning rate 是做不好的
 在同個 scale 下，loss 的等高線會較接近圓形
 gradient 的方向會指向圓心 (最低點)
62
w1
w2
w1
w2
 
  


        60.
      
    小提醒
 輸入 (input) 只能是數值
 名目資料、順序資料
 One-hot encoding
 順序轉成數值
 建議 re-scale 到接近的數值範圍
 今天的資料都已經先幫大家做好了 
63
 
  


        61.
      
    Read Input File
64
import numpy as np
# 讀進檔案，以 , (逗號)分隔的 csv 檔，不包含第一行的欄位定義
my_data = np.genfromtext('pkgo_city66_class5_v1.csv',
delimiter=',',
skip_header=1)
# Input 是有 200 個欄位(index 從 0 – 199)
X_train = my_data[:,0:200]
# Output 是第 201 個欄位(index 為 200)
y_train = my_data[:,200]
# 確保資料型態正確
X_train = X_train.astype('float32')
y_train = y_train.astype('int')
 
  


        62.
      
    Input
65
# 觀察一筆 X_train
print(X_train[1,:32])
 
  


        63.
      
    Output 前處理
 Keras 預定的 class 數量與值有關
 挑選出的寶可夢中，最大 Pokemon ID = 98
Keras 會認為『有 99 個 classes 分別為 Class 0, 1, 2, …, 98 class』
 zero-based indexing (python)
 把下麵的五隻寶可夢轉換成
66
No.4 小火龍 No.43 走路草 No.56 火爆猴 No. 71 喇叭芽 No.98 大鉗蟹
Class 0 Class 1 Class 2 Class 3 Class 4
 
  


        64.
      
    Output
67
# 轉換成 one-hot encoding 後的 Y_train
print(Y_train[1,:])
# [重要] 將 Output 從特定類別轉換成 one-hot encoding 的形式
from keras.utils import np_utils
Y_train = np_utils.to_categorical(y_train,5)
# 觀察一筆 y_train
print(y_train[0])
 
  


        65.
      
    接下來的流程
 先建立一個深度學習模型
 邊移動邊開火
68
就像開始冒險前要先選一隻寶可夢
 
  


        66.
      
    六步完模 – 建立深度學習模型
1. 決定 hidden layers 層數與其中的 neurons 數量
2. 決定該層使用的 activation function
3. 決定模型的 loss function
4. 決定 optimizer
 Parameters: learning rate, momentum, decay
5. 編譯模型 (Compile model)
6. 開始訓練囉！(Fit model)
69
 
  


        67.
      
    步驟 1+2: 模型架構
70
import theano
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD
# 宣告這是一個 Sequential 次序性的深度學習模型
model = Sequential()
# 加入第一層 hidden layer (128 neurons)
# [重要] 因為第一層 hidden layer 需連接 input vector
故需要在此指定 input_dim
model.add(Dense(128, input_dim=200))
Model 建構時，是以次序性的疊加 (add) 上去
 
  


        68.
      
    基本款 activation function
 Sigmoid function
71
 
  


        69.
      
    步驟 1+2: 模型架構 (Cont.)
72
# 宣告這是一個 Sequential 次序性的深度學習模型
model = Sequential()
# 加入第一層 hidden layer (128 neurons) 與指定 input 的維度
model.add(Dense(128, input_dim=200))
# 指定 activation function
model.add(Activation('sigmoid'))
# 加入第二層 hidden layer (256 neurons)
model.add(Dense(256))
model.add(Activation('sigmoid'))
# 加入 output layer (5 neurons)
model.add(Dense(5))
model.add(Activation('softmax'))
# 觀察 model summary
model.summary()
 
  


        70.
      
    Softmax
 Classification 常用 softmax 當 output 的 activation function
 Normalization: network output 轉換到[0,1] 之間且
softmax output 相加為 1  像 “機率”
 保留對其他 classes 的 prediction error
73
Output
0.6
2.6
2.2
0.1
e0.6
e2.6
e2.2
e0.1 e0.6+e2.6+e2.2+e0.1
Normalized by the sum
0.07
0.53
0.36
0.04
Exponential
Softmax
 
  


        71.
      
    Model Summary
74
 
  


        72.
      
    可以設定 Layer 名稱
75
# 另外一種寫法
model.add(Dense(5,activation=‘softmax’,name='output'))
# 觀察 model summary
model.summary()
 
  


        73.
      
    步驟 3: 選擇 loss function
 Mean_squared_error
 Mean_absolute_error
 Mean_absolute_percentage_error
 Mean_squared_logarithmic_error
76
0.9
0.1
AnswerPrediction
0.8
0.2
(0.9 − 0.8)2+(0.1 − 0.2)2
2
= 0.01
|0.9 − 0.8| + |0.1 − 0.2|
2
= 0.1
0.9 − 0.8 /|0.9| + |0.1 − 0.2|/|0.1|
2
∗ 100 = 55
[log 0.9 − log(0.8)]2
+[log 0.1 − log(0.2)]2
2
∗ 100 = 0.247
常用於 Regression
 
  


        74.
      
    Loss Function
 binary_crossentropy (logloss)
 categorical_crossentropy
 需要將 class 的表示方法改成 one-hot encoding
Category 1  [0,1,0,0,0]
 用簡單的函數 keras.np_utils.to_category(input)
 常用於 classification
77
−
1
𝑁
𝑛=1
𝑁
[𝑦 𝑛 log 𝑦 𝑛 + (1 − 𝑦 𝑛)log(1 − 𝑦 𝑛)]
0
1
AnswerPrediction
0.9
0.1
−
1
2
0 log 0.9 + 1 − 0 log 1 − 0.9 + 1 log 0.1 + 0 log 1 − 0.1
= −
1
2
log 0.1 + log 0.1 = − log 0.1 = 2.302585
 
  


        75.
      
    步驟 4: 選擇 optimizer
 SGD – Stochastic Gradient Descent
 Adagrad – Adaptive Learning Rate
 RMSprop – Similar with Adagrad
 Adam – Similar with RMSprop + Momentum
 Nadam – Adam + Nesterov Momentum
78
 
  


        76.
      
    SGD: 基本款 optimizer
 Stochastic gradient descent
 設定 learning rate, momentum, learning rate decay,
Nesterov momentum
 設定 Learning rate by experiments (later)
79
# 指定 optimizier
from keras.optimizers import SGD, Adam, RMSprop, Adagrad
sgd = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)
 
  


        77.
      
    就決定是你了!
80
# 指定 loss function 和 optimizier
model.compile(loss='categorical_crossentropy',
optimizer=sgd)
 
  


        78.
      
    Validation Dataset
 Validation dataset 用來挑選模型
 Testing dataset 檢驗模型的普遍性 (generalization)
避免模型過度學習 training dataset
81
Cross validation:
切出很多組的 (training, validation) 再
拿不同組訓練模型，挑選最好的模型
Testing
ValTraining
手邊收集到的資料
理論上
挑選出最好的模型後，拿
testing 檢驗 generalization
 
  


        79.
      
    Validation Dataset
 利用 model.fit 的參數 validation_split
 從輸入(X_train,Y_train) 取固定比例的資料作為 validation
 不會先 shuffle 再取 validation dataset
 固定從資料尾端開始取
 每個 epoch 所使用的 validation dataset 都相同
 手動加入 validation dataset
validation_data=(X_valid, Y_valid)
82
 
  


        80.
      
    Fit Model
 batch_size: min-batch 的大小
 nb_epoch: epoch 數量
 1 epoch 表示看過全部的 training dataset 一次
 shuffle: 每次 epoch 結束後是否要打亂 training dataset
 verbose: 是否要顯示目前的訓練進度，0 為不顯示
83
# 指定 batch_size, nb_epoch, validation 後，開始訓練模型!!!
history = model.fit( X_train,
Y_train,
batch_size=16,
verbose=0,
epochs=30,
shuffle=True,
validation_split=0.1)
 
  


        81.
      
    練習 00_firstModel.py
(5 minutes)
84
 
  


        82.
      
    Alternative: Functional API
 The way to go for defining a complex model
 For example: multiple outputs, multiple input source
 Why “Functional API” ?
 All layers and models are callable (like function call)
 Example
85
from keras.layers import Input, Dense
input = Input(shape=(200,))
output = Dense(10)(input)
 
  


        83.
      
    86
# Sequential (依序的)深度學習模型
model = Sequential()
model.add(Dense(128, input_dim=200))
model.add(Activation('sigmoid'))
model.add(Dense(256))
model.add(Activation('sigmoid'))
model.add(Dense(5))
model.add(Activation('softmax'))
model.summary()
# Functional API
from keras.layers import Input, Dense
from keras.models import Model
input = Input(shape=(200,))
x = Dense(128,activation='sigmoid')(input)
x = Dense(256,activation='sigmoid')(x)
output = Dense(5,activation='softmax')(x)
# 定義 Model (function-like)
model = Model(inputs=[input], outputs=[output])
 
  


        84.
      
    Good Use Case for Functional API (1)
 Model is callable as well, so it is easy to re-use the
trained model
 Re-use the architecture and weights as well
87
# If model and input is defined already
# re-use the same architecture of the above model
y1 = model(input)
 
  


        85.
      
    Good Use Case for Functional API (2)
 Easy to manipulate various input sources
88
x2
Dense(100) Dense(200)y1x1 outputnew_x2
x1 = input(shape=(10,))
y1 = Dense(100)(x1)
x2 = input(shape=(20,))
new_x2 = keras.layers.concatenate([y1,x2])
output = Dense(200)(new_x2)
Model = Model(inputs=[x1,x2],outputs=[output])
 
  


        86.
      
    Today
 Our exercise uses “Sequential” model
 Because it is more straight-forward to understand the
details of stacking layers
89
 
  


        87.
      
    Result
90
 
  


        88.
      
    這樣是好是壞？
 我們選用最常見的
91
Component Selection
Loss function categorical_crossentropy
Activation function sigmoid + softmax
Optimizer SGD
用下麵的招式讓模型更好吧
 
  


        89.
      
    Tips for Training DL Models
不過盲目的使用招式，會讓你的寶可夢失去戰鬥意識
92
 
  


        90.
      
    Tips for Deep Learning
93
No
Activation Function
YesGood result on
training dataset?
Loss Function
Good result on
testing dataset?
Optimizer
Learning Rate
 
  


        91.
      
    Tips for Deep Learning
94
No
Activation Function
YesGood result on
training dataset?
Loss Function
Good result on
testing dataset?
Optimizer
Learning Rate
𝜕𝐿
𝜕𝜃
=
𝜕𝐿
𝜕 𝑦
𝜕 𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃
受 loss function 影響
 
  


        92.
      
    Using MSE
 在指定 loss function 時
95
# 指定 loss function 和 optimizier
model.compile(loss='categorical_crossentropy',
optimizer=sgd)
# 指定 loss function 和 optimizier
model.compile(loss='mean_squared_error',
optimizer=sgd)
 
  


        93.
      
    練習 01_lossFuncSelection.py
(10 minutes)
96
 
  


        94.
      
    Result – CE vs MSE
97
 
  


        95.
      
    為什麼 Cross-entropy 比較好？
98
Cross-entropy
Squared error
The error surface of logarithmic functions is steeper than
that of quadratic functions. [ref]
Figure source
 
  


        96.
      
    How to Select Loss function
 Classification 常用 cross-entropy
 搭配 softmax 當作 output layer 的 activation function
 Regression 常用 mean absolute/squared error
 對特定問題定義 loss function
 Unbalanced dataset, class 0 : class 1 = 99 : 1
Self-defined loss function
99
Loss Class 0 Class 1
Class 0 0 99
Class 1 1 0
 
  


        97.
      
    Current Best Model Configuration
100
Component Selection
Loss function categorical_crossentropy
Activation function sigmoid + softmax
Optimizer SGD
 
  


        98.
      
    Tips for Deep Learning
101
No
Activation Function
YesGood result on
training data?
Loss Function
Good result on
testing data?
Optimizer
Learning Rate
 
  


        99.
      
    練習 02_learningRateSelection.py
(5-8 minutes)
102
# 指定 optimizier
from keras.optimizers import SGD, Adam, RMSprop, Adagrad
sgd = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)
試試看改變 learning rate，挑選出最好的 learning rate。
建議一次降一個數量級，如: 0.1 vs 0.01 vs 0.001
 
  


        100.
      
    Result – Learning Rate Selection
103
觀察 loss，這樣的震盪表示 learning rate 可能太大
 
  


        101.
      
    How to Set Learning Rate
 大多要試試看才知道，通常不會大於 0.1
 一次調一個數量級
 0.1  0.01  0.001
 0.01  0.012  0.015  0.018 …
 幸運數字！
104
 
  


        102.
      
    Tips for Deep Learning
105
No
Activation Function
YesGood result on
training data?
Loss Function
Good result on
testing data?
Optimizer
Learning Rate 𝜕𝐿
𝜕𝜃
=
𝜕𝐿
𝜕 𝑦
𝜕 𝑦
𝜕𝑧
𝜕𝑧
𝜕𝜃
受 activation function 影響
 
  


        103.
      
    Sigmoid, Tanh, Softsign
 Sigmoid
 f(x)=
 Tanh
 f(x)=
 Softsign
 f(x)=
106
Saturation 到下一層的數值在 [-1,1] 之間
(1+e-x)
1
(1+e-2x)
(1-e-2x)
(1+|x|)
x
 
  


        104.
      
    Derivatives of Sigmoid, Tanh, Softsign
107
gradient 小  學得慢
 Sigmoid
 df/dx=
 Tanh
 df/dx= 1-f(x)2
 Softsign
 df/dx=
(1+e-x)2
e-x
(1+|x|)2
1
 
  


        105.
      
    Drawbacks of Sigmoid, Tanh, Softsign
 Vanishing gradient problem
 原因: input 被壓縮到一個相對很小的output range
 結果: 很大的 input 變化只能產生很小的 output 變化
 Gradient 小  無法有效地學習
 Sigmoid, Tanh, Softsign 都有這樣的特性
 特別不適用於深的深度學習模型
108
 
  


        106.
      
    ReLU, Softplus
 ReLU
 f(x)=max(0,x)
 df/dx=
 Softplus
 f(x) = ln(1+ex)
 df/dx = ex/(1+ex)
109
1 if x > 0,
0 otherwise.
 
  


        107.
      
    Derivatives of ReLU, Softplus
110
ReLU 在輸入小於零時， gradient 等於零，會有問題嗎？
 
  


        108.
      
    Leaky ReLU
 Allow a small gradient while the input to activation
function smaller than 0
112
α=0.1
f(x)= x if x > 0,
αx otherwise.
df/dx=
1 if x > 0,
α otherwise.
 
  


        109.
      
    Leaky ReLU in Keras
 更多其他的 activation functions
https://keras.io/layers/advanced-activations/
113
# For example
From keras.layer.advanced_activation import LeakyReLU
lrelu = LeakyReLU(alpha = 0.02)
model.add(Dense(128, input_dim = 200))
# 指定 activation function
model.add(lrelu)
 
  


        110.
      
    嘗試其他的 activation functions
114
# 宣告這是一個 Sequential 次序性的深度學習模型
model = Sequential()
# 加入第一層 hidden layer (128 neurons) 與指定 input 的維度
model.add(Dense(128, input_dim=200))
# 指定 activation function
model.add(Activation('softplus'))
# 加入第二層 hidden layer (256 neurons)
model.add(Dense(256))
model.add(Activation('softplus'))
# 加入 output layer (5 neurons)
model.add(Dense(5))
model.add(Activation('softmax'))
# 觀察 model summary
model.summary()
 
  


        111.
      
    練習 03_activationFuncSelection.py
(5-8 minutes)
115
 
  


        112.
      
    Result – Softplus versus Sigmoid
116
 
  


        113.
      
    How to Select Activation Functions
 Hidden layers
 通常會用 ReLU
 Sigmoid 有 vanishing gradient 的問題較不推薦
 Output layer
 Regression: linear
 Classification: softmax
117
 
  


        114.
      
    Current Best Model Configuration
118
Component Selection
Loss function categorical_crossentropy
Activation function softplus + softmax
Optimizer SGD
 
  


        115.
      
    Tips for Deep Learning
119
No
YesGood result on
training dataset?
Good result on
testing dataset?
Activation Function
Loss Function
Optimizer
Learning Rate
 
  


        116.
      
    Optimizers in Keras
 SGD – Stochastic Gradient Descent
 Adagrad – Adaptive Learning Rate
 RMSprop – Similar with Adagrad
 Adam – Similar with RMSprop + Momentum
 Nadam – Adam + Nesterov Momentum
120
 
  


        117.
      
    Optimizer – SGD
 Stochastic gradient descent
 支援 momentum, learning rate decay, Nesterov momentum
 Momentum 的影響
 無 momentum: update = -lr*gradient
 有 momentum: update = -lr*gradient + m*last_update
 Learning rate decay after update once
 屬於 1/t decay  lr = lr / (1 + decay*t)
 t: number of done updates
121
 
  


        118.
      
    Learning Rate with 1/t Decay
122
lr = lr / (1 + decay*t)
 
  


        119.
      
    Momentum
 先算 gradient
 加上 momentum
 更新
Nesterov momentum
 加上 momentum
 再算 gradient
 更新
123
Nesterov Momentum
 
  


        120.
      
    Optimizer – Adagrad
 因材施教：每個參數都有不同的 learning rate
 根據之前所有 gradient 的 root mean square 修改
124
𝜃 𝑡+1 = 𝜃 𝑡 − 𝜂𝑔 𝑡
Gradient descent Adagrad
𝜃 𝑡+1 = 𝜃 𝑡 −
𝜂
𝜎 𝑡
𝑔 𝑡𝑔 𝑡
=
𝜕𝐿
𝜕𝜃 𝜃=𝜃 𝑡
第 t 次更新
𝜎 𝑡 =
2 (𝑔0)2+…+(𝑔 𝑡)2
𝑡 + 1
所有 gradient 的 root mean square
 
  


        121.
      
    Adaptive Learning Rate
 Feature scales 不同，需要不同的 learning rates
 每個 weight 收斂的速度不一致
 但 learning rate 沒有隨著減少的話  bumpy
 因材施教：每個參數都有不同的 learning rate
125
w1
w2
w1
w2
 
  


        122.
      
    Optimizer – Adagrad
 根據之前所有 gradient 的 root mean square 修改
126
𝜃 𝑡+1 = 𝜃 𝑡 − 𝜂𝑔 𝑡
Gradient descent Adagrad
𝜃 𝑡+1
= 𝜃 𝑡
−
𝜂
𝜎 𝑡
𝑔 𝑡𝑔 𝑡 =
𝜕𝐿
𝜕𝜃 𝜃=𝜃 𝑡
第 t 次更新
𝜎 𝑡 =
2 (𝑔0)2+…+(𝑔 𝑡)2
𝑡 + 1
所有 gradient 的 root mean square
 
  


        123.
      
    Step by Step – Adagrad
127
𝜃 𝑡 = 𝜃 𝑡−1 −
𝜂
𝜎 𝑡−1
𝑔 𝑡−1
𝜎 𝑡 =
(𝑔0)2+(𝑔1)2+...+(𝑔 𝑡)2
𝑡 + 1
𝜃2 = 𝜃1 −
𝜂
𝜎1
𝑔1
𝜎1 =
(𝑔0)2+(𝑔1)2
2
𝜃1 = 𝜃0 −
𝜂
𝜎0
𝑔0
𝜎0
= (𝑔0)2
 𝑔 𝑡
是一階微分，那 𝜎 𝑡
隱含什麼資訊？
 
  


        124.
      
    An Example of Adagrad
 老馬識途，參考之前的經驗修正現在的步伐
 不完全相信當下的 gradient
128
𝑔 𝒕
𝑔0
𝑔 𝟏
𝑔 𝟐 𝑔 𝟑
W1 0.001 0.003 0.002 0.1
W2 1.8 2.1 1.5 0.1
𝜎 𝑡
𝜎0 𝜎1 𝜎2 𝜎3
W1 0.001 0.002 0.002 0.05
W2 1.8 1.956 1.817 1.57
𝑔 𝒕/𝜎 𝑡
t=0 t=1 t=2 t=3
W1 1 1.364 0.952 2
W2 1 1.073 0.826 0.064
 
  


        125.
      
    Optimizer – RMSprop
 另一種參考過去 gradient 的方式
129
𝜎 𝑡
=
2 (𝑔0)2+…+(𝑔 𝑡)2
𝑡 + 1
Adagrad
𝜃 𝑡+1 = 𝜃 𝑡 −
𝜂
𝜎 𝑡
𝑔 𝑡
𝑟 𝑡 = (1 − 𝜌)(𝑔 𝑡)2+𝜌𝑟 𝑡−1
𝜃 𝑡+1 = 𝜃 𝑡 −
𝜂
𝑟 𝑡
𝑔 𝑡
RMSprop
 
  


        126.
      
    Optimizer – Adam
 Close to RMSprop + Momentum
 ADAM: A Method For Stochastic Optimization
 In practice, 不改參數也會做得很好
130
 
  


        127.
      
    比較 Optimizers
131
來源
 
  


        128.
      
    練習 04_optimizerSelection.py
(5-8 minutes)
132
# 指定 optimizier
from keras.optimizers import SGD, Adam, RMSprop, Adagrad
sgd =
SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False,clip_value
=10)
# 指定 loss function 和 optimizier
model.compile(loss='categorical_crossentropy',
optimizer=sgd)
1. 設定選用的 optimizer
2. 修改 model compilation
 
  


        129.
      
    Result – Adam versus SGD
133
 
  


        130.
      
     一般的起手式： Adam
 Adaptive learning rate for every weights
 Momentum included
 Keras 推薦 RNN 使用 RMSProp
 在訓練 RNN 需要註意 explosive gradient 的問題
 clip gradient 的暴力美學
 RMSProp 與 Adam 的戰爭仍在延燒
134
How to Select Optimizers
 
  


        131.
      
    Tips for Deep Learning
135
No
YesGood result on
training data?
Good result on
testing data?
Activation Function
Loss Function
Optimizer
Learning Rate
 
  


        132.
      
    Current Best Model Configuration
136
Component Selection
Loss function categorical_crossentropy
Activation function softplus + softmax
Optimizer Adam
50 epochs 後
90% 準確率！
 
  


        133.
      
    進度報告
137
我們有90%準確率了！
但是在 training dataset 上的表現
這會不會只是一場美夢？
 
  


        134.
      
    見真章
138
Overfitting 啦!
 
  


        135.
      
    Tips for Deep Learning
139
YesGood result on
training dataset?
YesGood result on
testing dataset?
No
什麼是 overfitting？
training result 進步，但 testing result 反而變差
Early Stopping
Regularization
Dropout
 
  


        136.
      
    Tips for Deep Learning
140
YesGood result on
training dataset?
YesGood result on
testing dataset?
No
Early Stopping
Regularization
Dropout
 
  


        137.
      
    Regularization
 限制 weights 的大小讓 output 曲線比較平滑
 為什麼要限制呢？
141
X1
X2
W1=10
W2=10
0.6
0.4
10
b=0
X1
X2
W1=1
W2=1
0.6
0.4
10
b=9
+0.1
+1 +0.1
+0.1
wi 較小  Δxi 對 ̂y 造成的影響(Δ̂y)較小
 對 input 變化比較不敏感  generalization 好
 
  


        138.
      
    Regularization
 怎麼限制 weights 的大小呢？
加入目標函數中，一起優化
 α 是用來調整 regularization 的比重
 避免顧此失彼 (降低 weights 的大小而犧牲模型準確性)
142
Lossreg=∑(y-(b+∑wixi))
̂y
+α(regularizer)
 
  


        139.
      
    L1 and L2 Regularizers
 L1 norm
 L2 norm
143
𝐿1 =
𝑖=1
𝑁
|𝑊𝑖|
𝐿2 =
𝑖=1
𝑁
|𝑊𝑖|2
Sum of absolute values
Root mean square of
absolute values
 
  


        140.
      
    Regularization in Keras
144
''' Import l1,l2 (regularizer) '''
from keras.regularizers import l1,l2
model_l2 = Sequential()
# 加入第一層 hidden layer 並加入 regularizer (alpha=0.01)
Model_l2.add(Dense(128,input_dim=200,kernel_regularizer=l2(0.01))
Model_l2.add(Activation('softplus'))
# 加入第二層 hidden layer 並加入 regularizer
model_l2.add(Dense(256, kernel_regularizer=l2(0.01)))
model_l2.add(Activation('softplus'))
# 加入 Output layer
model_l2.add(Dense(5, kernel_regularizer=l2(0.01)))
model_l2.add(Activation('softmax'))
 
  


        141.
      
    練習 06_regularizer.py
(5-8 minutes)
145
''' Import l1,l2 (regularizer) '''
from keras.regularizers import l1,l2,l1l2
# 加入第一層 hidden layer 並加入 regularizer (alpha=0.01)
Model_l2.add(Dense(128,input_dim=200,kernel_regularizer=l2(0.01))
Model_l2.add(Activation('softplus'))
1. alpha = 0.01 會太大嗎？該怎麼觀察呢？
2. alpha = 0.001 再試試看
 
  


        142.
      
    Result – L2 Regularizer (alpha=0.01)
146
 
  


        143.
      
    Tips for Deep Learning
147
YesGood result on
training dataset?
YesGood result on
testing dataset?
No
Early Stopping
Regularization
Dropout
 
  


        144.
      
    Early Stopping
 假如能早點停下來就好了…
148
 
  


        145.
      
    Early Stopping in Keras
 Early Stopping
 monitor: 要監控的 performance index
 patience: 可以容忍連續幾次的不思長進
149
''' EarlyStopping '''
from keras.callbacks import EarlyStopping
earlyStopping=EarlyStopping(monitor = 'val_loss',
patience = 3)
 
  


        146.
      
    加入 Early Stopping
150
# 指定 batch_size, nb_epoch, validation 後，開始訓練模型!!!
history = model.fit( X_train,
Y_train,
batch_size=16,
verbose=0,
epochs=30,
shuffle=True,
validation_split=0.1,
callbacks=[earlyStopping])
''' EarlyStopping '''
from keras.callbacks import EarlyStopping
earlyStopping=EarlyStopping( monitor = 'val_loss',
patience = 3)
 
  


        147.
      
    練習 07_earlyStopping.py
(5-8 minutes)
151
 
  


        148.
      
    Result – EarlyStopping (patience=3)
152
 
  


        149.
      
    Tips for Deep Learning
153
YesGood result on
training dataset?
YesGood result on
testing dataset?
No
Early Stopping
Regularization
Dropout
 
  


        150.
      
    Dropout
 What is Dropout?
 原本為 neurons 跟 neurons 之間為 fully connected
 在訓練過程中，隨機拿掉一些連結 (weight 設為0)
154
X1
Xn
w1,n
w1,1
w2,n
w2,1
 
  


        151.
      
    Dropout 的結果
 會造成 training performance 變差
 用全部的 neurons 原本可以做到
 只用某部分的 neurons 只能做到
 Error 變大  每個 neuron 修正得越多  做得越好
155
( 𝑦 − 𝑦) < 𝜖
( 𝑦′ − 𝑦) < 𝜖 + ∆𝜖
 
  


        152.
      
    Implications
1.
156
增加訓練的難度 在真正的考驗時爆發
2.
Dropout 2 Dropout 2NDropout 1
Dropout 可視為一種終極的 ensemble 方法
N 個 weights 會有 2N 種 network structures
 
  


        153.
      
    Dropout in Keras
157
from keras.layers.core import Dropout
model = Sequential()
# 加入第一層 hidden layer 與 dropout=0.4
model.add(Dense(128, input_dim=200))
model.add(Activation('softplus'))
model.add(Dropout(0.4))
# 加入第二層 hidden layer 與 dropout=0.4
model.add(Dense(256))
model.add(Activation('softplus'))
model.add(Dropout(0.4))
# 加入 output layer (5 neurons)
model.add(Dense(5))
model.add(Activation('softmax'))
 
  


        154.
      
    練習 08_dropout.py
(5-8 minutes)
158
 
  


        155.
      
    Result – Dropout or not
159
 
  


        156.
      
    How to Set Dropout
 不要一開始就加入 Dropout
 不要一開始就加入 Dropout
不要一開始就加入 Dropout
a) Dropout 會讓 training performance 變差
b) Dropout 是在避免 overfitting，不是萬靈丹
c) 參數少時，regularization
160
 
  


        157.
      
    大家的好朋友 Callbacks
善用 Callbacks 幫助你躺著 train models
161
 
  


        158.
      
    Callbacks Class
162
from keras.callbacks import Callbacks
Class LossHistory(Callbacks):
def on_train_begin(self, logs={}):
self.loss = []
self.acc = []
self.val_loss = []
self.val_acc = []
def on_batch_end(self, batch, logs={}):
self.loss.append(logs.get('loss'))
self.acc.append(logs.get('acc'))
self.val_loss.append(logs.get('val_loss'))
self.val_acc.append(logs.get('val_acc'))
loss_history = LossHistory()
 
  


        159.
      
    Callback 的時機
 on_train_begin
 on_train_end
 on_batch_begin
 on_batch_end
 on_epoch_begin
 on_epoch_end
163
 
  


        160.
      
    LearningRateScheduler
164
* epoch
感謝同學指正！
from keras.callbacks import LearningRateScheduler
def step_decay(epoch):
initial_lrate = 0.1
lrate = initial_lrate * (0.999^epoch)
return lrate
Lrate = LearningRateScheduler(step_decay)
 
  


        161.
      
    ModelCheckpoint
165
from keras.callbacks import ModelCheckpoint
checkpoint = ModelCheckpoint('model.h5',
monitor = 'val_loss',
verbose = 1,
save_best_only = True,
mode = ‘min’)
 
  


        162.
      
    在 model.fit 時加入 Callbacks
166
history = model.fit(X_train, Y_train,
batch_size=16,
verbose=0,
epochs=30,
shuffle=True,
validation_split=0.1,
callbacks=[early_stopping,
loss_history,
lrate,
checkpoint])
 
  


        163.
      
    Tips for Training Your Own DL Model
167
Yes Good result on
testing dataset?
No
Early Stopping
Regularization
Dropout
No
Activation Function
Loss Function
Optimizer
Learning Rate
Good result on
training dataset?
 
  


        164.
      
    Yeah, Win
168
 
  


        165.
      
    Variants of Deep Neural Network
Convolutional Neural Network (CNN)
169
 
  


        166.
      
    2-dimensional Inputs
 DNN 的輸入是一維的向量，那二維的矩陣呢? 例如
圖形資料
170
Figures reference
https://twitter.com/gonainlive/status/507563446612013057
 
  


        167.
      
    Ordinary Feedforward DNN with Image
 將圖形轉換成一維向量
 Weight 數過多，造成 training 所需時間太長
 左上的圖形跟右下的圖形真的有關係嗎?
171
300x300x3
300x300x3
100
27 x 106
Figure reference
http://www.ettoday.net/dalemon/post/12934
 
  


        168.
      
    Characteristics of Image
 圖的構成：線條  圖案 (pattern)物件 場景
172
Figures reference
http://www.sumiaozhijia.com/touxiang/471.html
http://122311.com/tag/su-miao/2.html
Line Segment
Pattern
Object
Scene
 
  


        169.
      
    Patterns
 猜猜看我是誰
 辨識一個物件只需要用幾個特定圖案
173
皮卡丘 小火龍
Figures reference
http://arcadesushi.com/charmander-drunken-pokemon-tattoo/#photogallery-1=1
 
  


        170.
      
    Property 1: What
 圖案的類型
174
皮卡丘
的耳朵
 
  


        171.
      
    Property 2: Where
 重複的圖案可能出現在很多不同的地方
175
Figure reference
https://www.youtube.com/watch?v=NN9LaU2NlLM
 
  


        172.
      
    Property 3: Size
 大小的變化並沒有太多影響
176
Subsampling
 
  


        173.
      
     Common applications
 模糊化, 銳利化, 浮雕
 http://setosa.io/ev/image-kernels/
Convolution in Computer Vision (CV)
177
 
  


        174.
      
     Adding each pixel and its local neighbors which are
weighted by a filter (kernel)
 Perform this convolution process to every pixels
Convolution in Computer Vision (CV)
178
Image
Filter
 
  


        175.
      
     Adding each pixel and its local neighbors which are
weighted by a filter (kernel)
 Perform this convolution process to every pixels
Convolution in Computer Vision (CV)
179
Image Filter
A filter could be
seen as a pattern
 
  


        176.
      
    Real Example: Sobel Edge Detection
 edge = 亮度變化大的地方
180
𝑝𝑖,𝑗 𝑝𝑖+1,𝑗
𝐺ℎ 𝑖, 𝑗 = 𝑝𝑖+1,𝑗 − 𝑝𝑖,𝑗
Multiply by a constant c
𝑐 ∗ 𝐺ℎ 𝑖, 𝑗
凸顯兩像素之間的差異
X-axis
Pixel value
Figure reference
https://en.wikipedia.org/wiki/Sobel_operator#/media/File:Bikesgraygh.jpg
 
  


        177.
      
    Real Example: Sobel Edge Detection
 相鄰兩像素值差異越大，convolution 後新像素絕對值越大
181
-1 0 1
-2 0 2
-1 0 1
x-gradient
-1 -2 -1
0 0 0
1 2 1
y-gradient
3 3 0 0 0
3 3 0 0 0
3 3 0 0 0
3 3 0 0 0
3 3 0 0 0
3 3 3 3 3
3 3 3 3 3
3 3 3 3 3
0 0 0 0 0
0 0 0 0 0
*
*
Original Image
-12 -12 0
-12 -12 0
-12 -12 0
=
=
0 0 0
-12 -12 -12
-12 -12 -12
New Image
 
  


        178.
      
    Real Example: Sobel Edge Detection
182
-1 0 1
-2 0 2
-1 0 1
x-gradient
-1 -2 -1
0 0 0
1 2 1
y-gradient
 
  


        179.
      
    Real Example: Sobel Edge Detection
 Edge image
183
 
  


        180.
      
    Feedforward DNN
CNN Structure
184
Convolutional Layer
&
Activation Layer
Pooling Layer
Can be performed
may times
Flatten
PICACHU!!!
Pooling Layer
Convolutional Layer
&
Activation Layer
New image
New image
New image
 
  


        181.
      
    Convolution Layer vs. Convolution in CV
185
Convolutional Layer
&
Activation Layer
-1 0 1
-2 0 2
-1 0 1
x-gradient
-1 -2 -1
0 0 0
1 2 1
y-gradient
New image
 
  


        182.
      
    An Example of CNN Model
186
CNN DNN
Flatten
 
  


        183.
      
    Convolutional Layer
 Convolution 執行越多次影像越小
187
0 0 0 0 0
0 0 1 1 0
1 1 1 1 1
0 1 0 1 1
0 0 1 0 1
Image
1 2 2
1 2 3
3 1 3
New image
1 0 0
0 1 0
0 0 1
Filter
6
1 2 2
1 2 3
3 1 3
1 0 0
0 1 0
0 0 1
Layer 1
Layer 2
=*
=*
這個步驟有那些地方是可以
變化的呢?
 
  


        184.
      
    Hyper-parameters of Convolutional Layer
 Filter Size
 Zero-padding
 Stride
 Depth (total number of filters)
188
 
  


        185.
      
    Filter Size
 5x5 filter
189
0 0 0 0 0
0 0 1 1 0
1 1 1 1 1
0 1 0 1 1
0 0 1 0 1
Image New imageFilter
3
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
=*
 
  


        186.
      
    Zero-padding
 Add additional zeros at the border of image
190
0 0 0 0 0 0 0
0 0 0 0 0 0 0
0 0 0 1 1 0 0
0 1 1 1 1 1 0
0 0 1 0 1 1 0
0 0 0 1 0 1 0
0 0 0 0 0 0 0
Image
1 0 0
0 1 0
0 0 1
Filter 0 1 1 0 0
1 1 2 2 0
2 1 2 3 2
0 3 1 3 2
0 0 2 0 2
New image
=*
Zero-padding 不會影響
convolution 的性質
 
  


        187.
      
    Stride
 Shrink the output of the convolutional layer
 Set stride as 2
191
0 0 0 0 0
0 0 1 1 0
1 1 1 1 1
0 1 0 1 1
0 0 1 0 1
Image
1 2
3 3
New image
1 0 0
0 1 0
0 0 1
Filter
=*
 
  


        188.
      
    Convolution on RGB Image
192
0 0 1 1
1 1 1 1
0 1 0 1
0 0 1 0
0 0 1 0
1 0 1 1
0 1 0 1
1 0 1 0
0 1 0 1
1 1 0 1
0 0 0 1
0 1 1 1
RGB image
1 0 0
0 1 0
0 0 1
Filters
0 1 0
0 1 0
0 1 0
0 0 1
0 1 0
1 0 0
2 3 5 3
5 3 5 5
3 4 5 6
1 2 5 2
+
1 1 2 1
2 1 2 2
0 3 1 2
0 0 2 0
1 0 2 1
1 1 2 2
2 1 2 2
1 1 1 1
0 2 1 1
2 1 1 1
1 0 2 2
0 1 2 1
Conv. image
*
*
*
=
=
=
New image
 
  


        189.
      
    2 3 5 3
5 3 5 5
3 4 5 6
1 2 5 2
2 3 5 3
5 3 5 2
3 4 5 1
3 4 5 2
Depth n
193
0 1 0 1
1 1 0 1
0 0 0 1
0 1 1 1
RGB images
0 0 1 0
1 0 1 1
0 1 0 1
1 0 1 0
0 0 1 1
1 1 1 1
0 1 0 1
0 0 1 0
0 0 1
0 1 0
1 0 0
0 1 0
0 1 0
0 1 0
Filter sets
1 0 0
0 1 0
0 0 1
2 3 5 3
5 3 5 5
3 4 5 6
1 2 5 2
2 3 5 3
5 3 5 2
3 4 5 1
3 4 5 2
2 3 5 3
5 3 5 2
3 4 5 1
3 4 5 2
2 3 5 3
5 3 5 5
3 4 5 6
1 2 5 2
4x4xn
New images
0 0 1
0 1 0
1 0 0
0 1 0
0 1 0
0 1 0
1 1 0
0 1 0
1 0 1
0 0 1
0 1 0
1 0 0
0 1 0
0 1 0
0 1 0
0 0 1
0 1 0
0 0 1
n * =
如果 filters 都給定了
，那 CNN 是在學什麼？
 
  


        190.
      
    An Example of CNN Model
194
256 個 filters，filter size 5x5x96
CNN DNN
Flatten
 
  


        191.
      
    Total Number of Weights
 Zero-padding
 With : 𝑊𝑛+1, 𝐻 𝑛+1,× = (𝑊𝑛, 𝐻 𝑛,×)
 Without: 𝑊𝑛+1, 𝐻 𝑛+1,× = (𝑊𝑛 −
𝑊 𝑓−1
2
, 𝐻 𝑛 −
𝐻 𝑓−1
2
,×)
 Stride = 𝑠
 𝑊𝑛+1, 𝐻 𝑛+1,× =
𝑊𝑛
𝑠
,
𝐻 𝑛
𝑠
,×
 𝑘 filters
 𝑊𝑛+1, 𝐻 𝑛+1, 𝑘 = 𝑊𝑛, 𝐻 𝑛, 𝐷 𝑛
 Total number of weights is needed from 𝐿 𝑛 to 𝐿 𝑛+1
 𝑊𝑓 × 𝐻𝑓 × 𝐷 𝑛 × 𝑘 + 𝑘
195
 
  


        192.
      
    Feedforward DNN
CNN Structure
196
Convolutional Layer
&
Activation Layer
Pooling Layer
Can be performed
may times
Flatten
PICACHU!!!
Pooling Layer
Convolutional Layer
&
Activation Layer
 
  


        193.
      
    Pooling Layer
 Why do we need pooling layers?
 Reduce the number of weights
 Prevent overfitting
 Max pooling
 Consider the existence of patterns in each region
197
1 2 2 0
1 2 3 2
3 1 3 2
0 2 0 2
2 3
3 3
Max pooling
*How about average pooling?
 
  


        194.
      
    An Example of CNN Model
198
CNN DNN
Flatten
 
  


        195.
      
    A CNN Example (Object Recognition)
 CS231n, Stanford [Ref]
199
 
  


        196.
      
    Filters Visualization
 RSIP VISION [Ref]
200
 
  


        197.
      
    CNN in Keras
Object Recognition
Dataset: CIFAR-10
201
 
  


        198.
      
    Differences between CNN and DNN
202
CNN DNN
Input Convolution Layer
 
  


        199.
      
    Differences between CNN and DNN
203
CNN DNN
Input Convolutional Layer
 
  


        200.
      
    CIFAR-10 Dataset
 60,000 (50,000 training + 10,000 testing) samples,
32x32 color images in 10 classes
 10 classes
 airplane, automobile, ship, truck,
bird, cat, deer, dog, frog, horse
 Official website
 https://www.cs.toronto.edu/~kriz/
cifar.html
204
 
  


        201.
      
    Overview of CIFAR-10 Dataset
 Files of CIFAR-10 dataset
 data_batch_1, …, data_batch_5
 test_batch
 4 elements in the input dataset
 data
 labels
 batch_label
 filenames
205
 
  


        202.
      
    How to Load Samples form a File
 This reading function is provided from the official site
206
# this function is provided from the official site
def unpickle(file):
import cPickle
fo = open(file, 'rb')
dict = cPickle.load(fo)
fo.close()
return dict
# reading a batch file
raw_data = unpickle(dataset_path + fn)
 
  


        203.
      
    How to Load Samples form a File
 Fixed function for Python3
207
# this function is provided from the official site
def unpickle(file):
import pickle
fo = open(file, 'rb')
dict = pickle.load(fo, encoding='latin1')
fo.close()
return dict
# reading a batch file
raw_data = unpickle(dataset_path + fn)
 
  


        204.
      
    Checking the Data Structure
 Useful functions and attributes
208
# [1] the type of input dataset
type(raw_data)
# <type 'dict'>
# [2] check keys in the dictionary
raw_data_keys = raw_data.keys()
# ['data', 'labels', 'batch_label', 'filenames']
# [3] check dimensions of pixel values
print "dim(data)", numpy.array(raw_data['data']).shape
# dim(data) (10000, 3072)
 
  


        205.
      
    Pixel Values and Labels
 Pixel values (data) x 5
 Labels x 5
209
0 32 x 32 = 1,024 1,024 1,024
1 1,024 1,024 1,024
… … … …
9,999 1,024 1,024 1,024
0 1 2 3 … 9,999
6 9 9 4 … 5
 
  


        206.
      
    Datasets Concatenation
 Pixel values
 Labels
210
0 1,024 1,024 1,024
1 1,024 1,024 1,024
… … … …
9,999 1,024 1,024 1,024
10,000 1,024 1,024 1,024
… … … …
0 1 … 9,999 10,000 …
6 9 … 5 7 …
 
  


        207.
      
    Input Structures in Keras
 Depends on the configuration parameter of Keras
 "image_dim_ordering": "tf " → (Height, Width, Depth)
 "image_dim_ordering": "th" → (Depth, Height, Width)
211
Width
Height
Depth
32
32
{
"image_dim_ordering": "tf",
"epsilon": 1e-07,
"floatx": "float32",
"backend": "theano"
}
 
  


        208.
      
    Concatenate Datasets by Numpy Functions
 hstack, dim(6,)
 vstack , dim(2,3)
 dstack, dim(1, 3, 2)
212
A B
[1,2,3] [4,5,6]
[1, 2, 3, 4, 5, 6]
[[1, 2, 3],
[4, 5, 6]]
[[[1, 4],
[2, 5],
[3, 6]]]
Dimensions
Pixel values
Labels
 
  


        209.
      
    Concatenating Input Datasets
 利用 vstack 連接 pixel values；用 hstack 連接 labels
213
img_px_values = 0
img_lab = 0
for fn in train_fns:
raw_data = unpickle(dataset_path + fn)
if fn == train_fns[0]:
img_px_values = raw_data['data']
img_lab = raw_data['labels']
else:
img_px_values = numpy.vstack((img_px_values,
raw_data['data']))
img_lab = numpy.hstack((img_lab,
raw_data['labels']))
 
  


        210.
      
    Reshape the Training/Testing Inputs
 利用影像的長寬資訊先將 RGB 影像分開，再利用
reshape 函式將一維向量轉換為二維矩陣，最後用
dstack 將 RGB image 連接成三維陣列
214
X_train = numpy.asarray(
[numpy.dstack(
(
r[0:(width*height)].reshape(height,width),
r[(width*height):(2*width*height)].reshape(height,width),
r[(2*width*height):(3*width*height)].reshape(height,width))
) for r in img_px_values]
)
Y_train = np_utils.to_categorical(numpy.array(img_lab), classes)
 
  


        211.
      
    Saving Each Data as Image
 SciPy library
 Dimension of "arr_data" should be (height, width, 3)
 Supported image format
 .bmp, .png
215
''' saving ndarray to image'''
from scipy.misc import imsave
def ndarray2image(arr_data, image_fn):
imsave(image_fn, arr_data)
 
  


        212.
      
    Saving Each Data as Image
 PIL library (Linux OS)
 Dimension of "arr_data" should be (height, width, 3)
 Supported image format
 .bmp, .jpeg, .png, etc.
216
''' saving ndarray to image'''
from PIL import Image
def ndarray2image(arr_data, image_fn):
img = Image.fromarray(arr_data, 'RGB')
img.save(image_fn)
 
  


        213.
      
    Differences between CNN and DNN
217
CNN DNN
Input Convolutional Layer
 
  


        214.
      
    '''CNN model'''
model = Sequential()
model.add(
Convolution2D(32, 3, 3, border_mode='same',
input_shape=X_train[0].shape)
)
model.add(Activation('relu'))
model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(10))
model.add(Activation('softmax'))
Building Your Own CNN Model
218
32 個 3x3 filters ‘same’: perform padding
default value is zero
‘valid’ : without padding
CNN
DNN
 
  


        215.
      
    Model Compilation
219
'''setting optimizer'''
# define the learning rate
learning_rate = 0.01
learning_decay = 0.01 / 32
# define the optimizer
sgd = SGD(lr=learning_rate, decay=learning_dacay, momentum=0.9,
nesterov=True)
# Let’s compile
model.compile(loss='categorical_crossentropy', optimizer=sgd,
metrics=['accuracy'])
 
  


        216.
      
     Only one function
 Total parameters
Number of Parameters of Each Layers
220
# check parameters of every layers
model.summary()
32*3*3*3 + 32=896
32*3*3*32 + 32=9,248
7200*512 + 512=3,686,912
 
  


        217.
      
    Let’s Start Training
 Two validation methods
 Validate with splitting training samples
 Validate with testing samples
221
''' training'''
# define batch size and # of epoch
batch_size = 128
epoch = 32
# [1] validation data comes from training data
fit_log = model.fit(X_train, Y_train, batch_size=batch_size,
nb_epoch=epoch, validation_split=0.1,
shuffle=True)
# [2] validation data comes from testing data
fit_log = model.fit(X_train, Y_train, batch_size=batch_size,
nb_epoch=epoch, shuffle=True,
validation_data=(X_test, Y_test))
 
  


        218.
      
    Saving Training History
 Save training history to .csv file
222
'''saving training history'''
import csv
# define the output file name
history_fn = 'ccmd.csv'
# create the output file
with open(history_fn, 'wb') as csv_file:
w = csv.writer(csv_file)
# convert the data structure from dictionary to ndarray
temp = numpy.array(fit_log.history.values())
# write headers
w.writerow(fit_log.history.keys())
# write values
for i in range(temp.shape[1]):
w.writerow(temp[:,i])
 
  


        219.
      
    Model Saving and Prediction
 Saving/loading the whole CNN model
 Predicting the classes with new image samples
223
'''saving model'''
from keras.models import load_model
model.save('cifar10.h5')
del model
'''loading model'''
model = load_model('cifar10.h5')
'''prediction'''
pred = model.predict_classes(X_test, batch_size, verbose=0)
 
  


        220.
      
    Let’s Try CNN
224
Figure reference
https://unsplash.com/collections/186797/coding
 
  


        221.
      
    Practice 1 – Dimensions of Inputs
 Find the dimensions of image and class of labels in
read_dataset2vec.py and read_dataset2img.py
 Following the dimension transformation from raw
inputs to training inputs (Line 50-110)
225
# define the information of images which can be obtained from
official website
height, width, dim = 32, 32, 3
classes = 10
 
  


        222.
      
    Practice 2 – Design a CNN Model
 設計一個 CNN model，並讓他可以順利執行 (Line
16-25)
226
# set dataset path
dataset_path = './cifar_10/'
exec(open("read_dataset2img.py").read())
'''CNN model'''
model = Sequential()
# 請建立一個 CNN model
# CNN
model.add(Flatten())
# DNN
 
  


        223.
      
    Let’s Try CNN
 Hints
 Check the format of training dataset / validation dataset
 Design your own CNN model
 Don’t forget saving the model
227
Figure reference
https://unsplash.com/collections/186797/coding
 
  


        224.
      
    Tips for Setting Hyper-parameters (1)
 影像的大小須要能夠被 2 整除數次
 常見的影像 size 32, 64, 96, 224, 384, 512
 Convolutional Layer
 比起使用一個 size 較大的 filter (7x7)，可以先嘗試連續使用數個 size
小的 filter (3x3)
 Stride 的值與 filter size 相關，通常 𝑠𝑡𝑟𝑖𝑑𝑒 ≤
𝑊 𝑓−1
2
 Very deep CNN model (16+ Layers) 多使用 3x3 filter 與 stride 1
228
 
  


        225.
      
    Tips for Setting Hyper-parameters (2)
 Zero-padding 與 pooling layer 是選擇性的結構
 Zero-padding 的使用取決於是否要保留邊界的資訊
 Pooling layer 旨在避免 overfitting 與降低 weights 的數量，
但也減少影像所包含資訊，一般不會大於 3x3
 嘗試修改有不錯效能的 model，會比建立一個全新的模型容
易收斂，且 model weights 越多越難 tune 出好的參數
229
 
  


        226.
      
    Training History of Different Models
 cp32_3 → convolution layer with zero-padding, 32 3x3 filters
 d → fully connected NN layers
230
 
  


        227.
      
    Semi-supervised Learning
妥善運用有限的標籤資料 (optional)
231
 
  


        228.
      
    常面對到的問題
 收集到的標籤遠少於實際擁有的資料量
 有 60,000 張照片，只有 5,000 張知道照片的標籤
 該如何增加 label 呢？
 Crowd-sourcing
 Semi-supervised learning
232
 
  


        229.
      
    Semi-supervised Learning
 假設只有 5000 個圖有 label
 先用 labeled dataset to train model
 至少 train 到一定的程度 (良心事業)
 拿 unlabeled dataset 來測試，挑出預測好的
unlabeled dataset
 Example: softmax output > 0.9  self-define
 假設預測的都是對的 (unlabeled  labeled)
 有更多 labeled dataset 了！
 Repeat the above steps
233
 
  


        230.
      
    七傷拳
 加入品質不佳的 labels 反而會讓 model 變差
 例如：加入的圖全部都是“馬” ，在訓練過程中，模型很
容易變成 “馬” 的分類器
 慎選要加入的 samples
 Depends on your criteria 
234
 
  


        231.
      
    Transfer Learning
Utilize well-trained model on YOUR dataset (optional)
235
 
  


        232.
      
    Introduction
 “transfer”: use the knowledge learned from task A to
tackle another task B
 Example: 綿羊/羊駝 classifier
236
綿羊
羊駝
其他動物的圖
 
  


        233.
      
    Use as Fixed Feature Extractor
 A known model, like VGG, trained on ImageNet
 ImageNet: 10 millions images with labels
237
OutputInput
取某一個 layer output
當作 feature vectors
Train a classifier based on the features
extracted by a known model
 
  


        234.
      
    Use as Initialization
 Initialize your net by the
weights of a known model
 Use your dataset to further
train your model
 Fine-tuning the known
model
238
OutputInput
OutputInput
VGG model
Your model
 
  


        235.
      
    Short Summary
 Unlabeled data (lack of y)
 Semi-supervised learning
 Insufficient data (lack of both x and y)
 Transfer learning (focus on layer transfer)
 Use as fixed feature extractor
 Use as initialization
 Resources: https://keras.io/applications/
Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, “How transferable are
features in deep neural networks?”, https://arxiv.org/abs/1411.1792, 2014
239
 
  


        236.
      
    Summarization
What We Have Learned Today
240
 
  


        237.
      
    Recap – Fundamentals
 Fundamentals of deep learning
 A neural network = a function
 Gradient descent
 Stochastic gradient descent
 Mini-batch
 Guidelines to determine a network structure
241
 
  


        238.
      
    Recap – Improvement on Training Set
 How to improve performance on training dataset
242
Activation Function
Loss Function
Optimizer
Learning Rate
 
  


        239.
      
    Recap – Improvement on Testing Set
 How to improve performance on testing dataset
243
Early Stopping
Regularization
Dropout
 
  


        240.
      
    Recap – CNN
 Fundamentals of CNN
 Concept of filters
 Hyper-parameters
 Filter size
 Zero-padding
 Stride
 Depth (total number of filters)
 Pooling layers
 How to train a CNN in Keras
 CIFAR-10 dataset
244
 
  


        241.
      
    補充資料
245
 
  


        242.
      
    How to Get Trained Weights
 weights = model.get_weights()
 model.layers[1].set_weights(weights[0:2])
246
# get weights
myweight = model.get_weights()
# set weights
model.layers[1].set_weights(myweights[0:2])
# BTW, use model.summary() to check your layers
model.summary()
 
  


        243.
      
    Fit_Generator
 當資料太大無法一次讀進時 (memory limitation)
247
# get weights
def train_generator(batch_size):
while 1:
data = np.genfromtext('pkgo_city66_class5_v1.csv',
delimiter=',',
skip_header=1)
for i in range(0,np.floor(batch_size/len(data)))
x = data[i*batch_size:(i+1)*batch_size,:200]
y = data[i*batch_size:(i+1)*batch_size,200]
yield x,y
Model.fit_generator(train_generator(28),
epochs=30,
steps_per_epoch=100,
validation_steps=100) # or validation_data
 
  


        244.
      
    Residual Network
248
X
y = H(x)
F(x)+x = H(x)
F(x) = H(x) - x
The authors hypothesize that it is easier to optimize the
residual mapping than to optimize the original mapping
H(x)
https://arxiv.org/pdf/1512.03385.pdf
 
  


        245.
      
    Deep Learning Applications
249
 
  


        246.
      
    Visual Question Answering
source: http://visualqa.org/
(Slide Credit: Hung-Yi Lee)
 
  


        247.
      
    Video Captioning
251
Answer: a woman is carefully slicing tofu.
Generated caption: a woman is cutting a block of tofu.
 
  


        248.
      
    Text-To-Image
252
https://arxiv.org/pdf/1701.00160.pdf
 
  


        249.
      
    Vector Arithmetic for Visual Concepts
253
https://arxiv.org/pdf/1511.06434.pdf
 
  


        250.
      
    Go Deeper in Deep Learning
 “Neural Networks and Deep Learning”
 written by Michael Nielsen
 http://neuralnetworksanddeeplearning.com/
 “Deep Learning”
 Written by Yoshua Bengio, Ian J. Goodfellow and Aaron
Courville
 http://www.iro.umontreal.ca/~bengioy/dlbook/
 Course: Machine learning and having it deep and
structured
 http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_
2.html
(Slide Credit: Hung-Yi Lee)
 
  


        251.
      
    References
 Keras documentation Keras 官方網站，非常詳細
 Keras Github 可以從 example/ 中找到適合自己應用的範例
 Youtube 頻道 – 臺大電機李宏毅教授
 Convolutional Neural Networks for Visual Recognition cs231n
 若有課程上的建議，歡迎來信
cmchang@iis.sinica.edu.tw and chihfan@iis.sinica.edu.tw
255
 
  


        252.
      
    256
一起往訓練大師的路邁進吧！
謝謝大家！
 
  








        Recommended
      











        [DSC 2016] 系列活動：李宏毅 / 一天搞懂深度學習
      
台灣資料科學年會










        [系列活動] Machine Learning 機器學習課程
      
台灣資料科學年會










        [系列活動] 機器學習速遊
      
台灣資料科學年會










        [系列活動] 使用 R 語言建立自己的演算法交易事業
      
台灣資料科學年會










        [系列活動] 手把手教你R語言資料分析實務
      
台灣資料科學年會










        [系列活動] 給工程師的統計學及資料分析 123
      
台灣資料科學年會










        [系列活動] 資料探勘速遊
      
台灣資料科學年會










        [系列活動] 智慧製造與生產線上的資料科學 (製造資料科學：從預測性思維到處方性決策)
      
台灣資料科學年會










        [系列活動] 資料探勘速遊 - Session4 case-studies
      
台灣資料科學年會










        [系列活動] 智慧城市中的時空大數據應用
      
台灣資料科學年會
















×





Share Clipboard

×


Email









Email sent successfully..





Facebook


Twitter


LinkedIn



Google+






Link







Public clipboards featuring this slide

×




    No public clipboards found for this slide
  






×



Save the most important slides with Clipping




Clipping is a handy way to collect and organize the most important slides from a presentation. You can keep your great finds in clipboards organized around topics.
Start clipping
No thanks. Continue to download.








Select another clipboard

×






Looks like you’ve clipped this slide to  already.












Create a clipboard






You just clipped your first slide!

        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.
      






Name*
          






Description
          





Visibility
        
Others can see my Clipboard







Cancel
Save



























入門 AI 從「深度學習」開始，五本必讀的深度學習聖經書籍 | TechOrange
















































 




























逛其他頻道


BuzzOrange 報橘


 TechOrange 科技報橘



VidaOrange 生活報橘



訂閱電子報





職缺訊息
來看看有哪些職缺 https://buzzorange.com/recruit/


聯絡報橘

 寫信給編輯 pr@fusionmedium.com
 寫信給業務 sales@fusionmedium.com
 其他聯絡需求 tips@fusionmedium.com



 

我要投稿 edit@fusionmedium.com


來信請附上投稿人真實姓名（刊登可用筆名，但請同時提供真實姓名）、背景（已就業者請提供所任職產業，學生請提供就讀科系），並直接附上投稿內容。
稿件經編輯檯審核評估合宜性，並編寫導讀與修訂標題後再行刊登。
若投稿三個工作日後，未收到編輯檯回覆刊登通知，即為本次投稿不予刊登。






























 人資要知道，求職者也不要錯過：只會用 LinkedIn 還不夠，你真的知道怎麼寫 InMail 才有高回覆率嗎？




 Google的經典搜尋框居然要改掉了！繼FB之後，現在連Google都要來『喂資訊』了




 50 歲科技記者在新創公司的一年：整間公司都是奇葩庸才，卻還能做到估值 600 億台幣？




 好啦分你一杯羹！臉書測試「付費牆」讓用戶看新聞也要錢，媒體們還不快謝謝臉書主子














入門 AI 從「深度學習」開始，五本必讀的深度學習聖經書籍Posted on 2017/05/102017/05/10 
  雷鋒網 


 

【我們為什麼挑選這篇文章】本文作者 Daniel Jeffries 針對五本兼具理論與實務的書籍，推薦給不同需求、不同經驗的深度學習者，以期大家在買書前作為參考，依序閱讀。（責任編輯：楊侑陵）
（以下以 Daniel Jeffries 第一人稱撰寫）
多年來，由於實驗室研究和現實應用效果之間的鴻溝，少有人持續研究人工智能，AI 在很多領域停滯不前。然而近兩年，AI 在一些領域陸續有了重大突破，像是圖像識別、自動駕駛、Alpha Go 等等。許多八九十年代的算法，因為硬體速度慢和缺乏數據等原因而不再被使用。而現在，受眾多大數據和大規模並行芯片的支持，這些算法終於初見成效。
在過去的一年多時間裡，研究人員競相出版專著，以滿足讀者對深度學習知識的渴求。第一本關於深度學習的書已經上架，更多的將會在夏天或者明年年初陸續上架。我有幸提前拜讀了一些專著的初稿，這些書的最終出版讓人期待萬分。
接下來我將推薦一些深度學習書籍給大家。這些書可以引導大家如何學習人工智能，這對深度學習的快速理解有很大幫助。
理論和實踐相互兼顧 
我父親常說：「凡事均衡最好。」對此我深信不疑，且奉為圭臬。當然，我承認在周末或者在拉斯維加斯的時候，偶爾也會將其拋諸腦後。
我贊成理論與實踐要相互均衡。比如針對某個問題， 給一個明確的背景資訊，然後根據實例自己動手實踐。書本不要過多闡述理論知識，應該讓讀者邊學習邊查漏補缺。 設想有一本書，非常抽象，或者列舉大量的實例，卻不解釋問題的來龍去脈，你還看的下去嗎？
每個人都有自己的學習習慣，應該清楚地知道怎麼學，才能卓有成效。我始終認為花時間去買那些與我無用的書，特別容易錯過真正適合的。如果你喜歡通篇理論，那就不必繼續看我的推薦了。如果你喜歡看那些理論與實踐結合相宜得章的書，相信我推薦的書單會讓你會喜歡。
《Deep Learning》

第一本書是 Ian Goodfellow 的《Deep Learning》。作者在 Google Brain 和 OpenAI 上做出了突出貢獻。不少人認為這本書是深度學習領域的聖經，因為它是迄今為止唯一一本融合了前幾十年研究工作的鴻篇巨著。
不過，除非你有較好的數學基礎，否則不建議你從本書入門，因為讀起來挫敗感十足。 書中不僅有大量的公式，同時寫得比較枯燥乾澀。儘管 Goodfellow 希望能傳授讀者更多的知識，但該書讀起來卻比較乏味，不能引人入勝。
我估計明年首次參加大學深度學習課程的學生手中會擁有這本書，而其中許多人會因為這本書難懂而堅持不下去。這本書比較適合那些經過幾年相關工作後，仍想進一步掌握深度學習的從業者。對於擁有較多專業領域知識且正準備初次踏入 AI 行業的專業程序員而言，這也是一本比較全面的指南。
本書免費中文版請 點此 。
《Hands-On Machine Learning with Scikit-Learn and Tensorflow》

第二本是剛出版的《Hands-On Machine Learning with Scikit-Learn and Tensorflow》。雖然這本書也有大量的公式，不過  作者 Aurélien Géron 用簡單的方式詮釋了複雜的概念。全書寫得通俗易懂，可讀性很強，不過我也不強烈推薦。
《數學不好還學 AI》系列文章，是本書很好的補充，尤其是該系列的第五篇《用捲積神經網絡進行圖像識別》和第七篇《自然語言處理》。在我的印象中，這本書有詳盡的實例和相應代碼，兩者完美結合。我讀過在 Safari 的在線叢書初稿，當時許多部分沒有寫完，而且網站還將一些公式轉換的莫名其妙，但這並不影響我對內容的理解。
和其他優秀的修改稿一樣，最終版本相比初稿有了實質的躍進，完美詮釋了作者的觀點和採用的實例。本書內容的組織非常自然流暢，各種觀點都有清晰的實例證明，建議讀者看第一遍的時候，不要去管那些公式，以後需要時再去深入推敲。
《Deep Learning with Python》

第三本是《Deep Learning with Python》。作者是 Keras 框架的構建者 Francois Chollet。不過這本書還得過段時間才能出版。但透過 Manning 的 MEAP 系統可以閱讀前三章的內容，內容寫得很好，我極力推薦此書。
正如 Chollet 在編寫 Keras 框架時一樣  ，他神奇的將復雜概念簡單化，文中措詞巧妙，可讀性強。 即使是 AI 和深度學習中最具挑戰性的概念，他也同樣解釋的通俗易懂。讀了這本書我才真正理解什麼是張量。書中有大量不錯的實例，大家可以在他的 Github上看看。隨著越來越接近正式出版，這本書也越來越完整。請關註支持作者，且同時，盡量能先在 MEAP 上跟進閱讀，留言給作者，來讓這本書更完善。
《Deep Learning: A Practitioner’s Approach》

第四本是《Deep Learning: A Practitioner’s Approach》。該書主要使用 Java 的深度學習框架 DL4J。目前 AI 領域的研究大多數使用 Python 語言實現，不過隨著越來越多企業湧入機器學習領域，Java 的使用可能會逐漸增多。由於 Java 擁有龐大的生態系統，現在的大公司裡，它仍然是主要的開發工具。
這本書的讀者設定是深度學習的初學者。因此，如果你已經有一些深度學習的基本知識、並想進一步深入研究如何用 Java 實現深度學習的話，請直接跳過前面的例子。但是如果你沒什麼深度學習經驗，Java 也不太熟悉的話，那麼這本書值得你細細研讀。 尤其是第 4 章「出色的深度學習架構」，提供了一個可以幫你解決現實應用中架構問題的關鍵方法 。
雖然我不熟悉 Java 語言，但我把它分享給我的幾個寫程式的同事後，他們非常喜歡。在介紹深度學習上，書中的實例和書本的總體結構顯得非常專業。
《TensorFlow Machine Learning Cookbook》

最後推薦的是《TensorFlow Machine Learning Cookbook》。雖然書中的內容和編碼有一些錯別字，不過在自然語言處理等內容上，還是列舉了許多不錯的實例。和其他手冊書一樣，本書也偏重編碼，如果你不太瞭解捲積神經網絡的輸入輸出，你會被許多基本概念搞暈，買書前如果你已經看過其它的書，並且對書中的例子進行了實戰，那麼這本書可以給你提供更多的練習與實踐。 但請不要把此書當作入門書，也不建議單獨購買此書 。
有些書正在編寫中、有些書已經出版但還沒有機會拜讀的，將來有機會的話，我會繼續推薦的。
想更瞭解人工智慧協助轉型的方法？？5/24 中港台三地專家共同解密，要帶你瞭解人工智慧協助製造業轉型的最強案例！ https://goo.gl/Wvkmfj

—
（本文經合作夥伴雷鋒網授權轉載，並同意 TechOrange 編寫導讀與修訂標題，原文標題為 〈入門深度學習，讀對書很重要 〉。首圖來源：Jisc, CC licensed）
延伸閱讀
如果想靠寫程式吃飯，第一個學的絕對要是 JavaScript！
【工程師顫抖了】Google 開發出「會寫 AI 程式的 AI」，難道以後 Coding 的都不是人了嗎……
 向全世界的工程師請教書單：在 Stack Overflow 上最常被提到的 10 本書 
【免費自修資源】精選 8 本資料科學經典書單，讓你自學也能變大師 
2017 年程式語言最佳有潛力獎：Go、Dart 與 Perl，你認識幾種呢？


你對製作這些科技趨勢內容有興趣嗎？
想從 TO 讀者變成 TO 製作者嗎？
 對內容策展有無比興趣的你，快加入我們的編輯團隊吧！

TechOrange 社群編輯擴大徵才中 >>  詳細內容 

 意者請提供履歷自傳以及文字作品，寄至 jobs@fusionmedium.com
 來信主旨請註明：【應徵】TechOrange 職缺名稱：您的大名 

點關鍵字看更多相關文章：AI 人工智慧TensorFlow書單 


2.5 KShares





留言






 

















【麥肯錫重磅報告】沒玩 AI 就等著被淘汰！科技、電信、金融業 AI 投資發燒，揭露科技巨頭荷包賺滿滿的秘密




技術打臉馬雲無人超市！根本不是 AI 啟動，只是簡單的 RFID 應用




【快來看 AI 學走路！】Google AlphaGo 模擬人走路，滾帶跑各種淒慘摔法都摔不死他




自動販賣機也能打天下？可口可樂結合人工智慧 app，打造新奇體驗拚銷售




2017 年 15 個最好用 Python 庫，學習資料科學、機器學習絕對不容錯過




【微軟 X 德國 ThyssenKrupp】能夠預測哪裡壞了？智慧電梯自己叫維修工人




「你只能二選一，成為上帝還是無用階層?」《人類大歷史》作者談 21 世紀人工智慧最大危機




與一般民眾一起參加的無人公車試乘，我看見了台灣的超高科技接受度




未來 45 年內，哪些產業將消失？350 位人工智慧專家點名暢銷作家、外科醫生




Google 想買下全世界數據人才，為了求才你可能得付出多少代價？



 










橘子學院 




【企業專屬】Google Analytics 網站數據分析課程







【活動快訊】拒當數位行動時代的廢柴和魯蛇！8/10 第二屆「WHATs NEXT！移動到未來」輕鬆接軌 Mobile Internet


 


 

報橘週榜

【投書】台灣嘲笑中國上 FB 要翻牆？陸生回擊：台灣人根本目光如鼠


史上最殘酷的史丹佛雞湯文：你是如何從優秀的 19 歲年輕人，變成了平庸的 40 歲大人？


比爾蓋茲再度預言：新鮮人，這三個才是你未來該進的領域！


全球最大色情網站Pornhub全面加密，再也不怕A片瀏覽記錄被看到啦！


厭世不幹了！華盛頓巡邏機器人 K5 罷工，跳進水池內自殺


延燒 15 個月，「直播」還是泡沫化了：一場全中國都參與的流量遊戲趨近尾聲


【有圖】黑科技產生器！亞馬遜蓋水底倉庫，大家潛水領貨呀


Tesla 馬斯克親傳開啟知識天眼法！他用了個超神奇原理，變身無敵天才


C 語言已死？想取代資安漏洞寬如海溝的 C，你該用這個語言


北科大教授觀點：別弄錯了，oBike 和 Uber，已經是截然不同的兩種「共享」
 
 
 






























人工智慧時代來臨，必須學習的新技術
輕鬆學會「深度學習」：先學Keras再學TensorFlow

★成長最快領域：深度學習與類神經網路，是人工智慧" />



TensorFlow+Keras深度學習人工智慧實務應用 – TreeMall書城

















 


TreeMall　|　會員中心　|　加入會員　|　訂單查詢　|　購物車結帳　|　Q&A




書名
作者
出版社
書碼


































首頁 

				
					
					> 博客來書城
					
					
				
				
				> 電腦資訊
				
			








 




行銷企管





財經投資





文學





小說





社會人文





史地





法律/政治/軍事





哲學





傳記





電腦





應用科學





自然科學





語言學習





考試用書





心理勵志





宗教





醫療保健





休閒娛樂





觀光旅遊





飲食生活





漫畫





家庭百科/親子





童書/青少年叢書





文學小說





商業理財





藝術設計





人文史地





社會科學





自然科普





心理勵志





醫療保健





飲食





生活風格





旅遊





宗教命理





親子教養





童書/青少年文學





輕小說





漫畫





語言學習





考試用書





電腦資訊





專業/教科書/政府出版品

























TensorFlow+Keras深度學習人工智慧實務應用







作 者：林大貴


出版社：博碩


ISBN：9789864342167


出版日期：2017年06月09日



語言別：繁體中文



										原價：590 元    
										優惠價：78折 
										461元 


















內容簡介





人工智慧時代來臨，必須學習的新技術
輕鬆學會「深度學習」：先學Keras再學TensorFlow

★成長最快領域：深度學習與類神經網路，是人工智慧成長最快的領域，讓電腦更接近人類的思考。
★應用深入生活：手機語音助理、人臉識別、影像辨識、手寫辨識、醫學診斷、自然語言處理。
★實作快速上手：只需Python基礎，依照本書Step by Step學習，就可以輕鬆學會深度學習概念與應用。

TensorFlow功能強大、執行效率高、支援各種平臺，然而TensorFlow是低階的深度學習程式庫，學習門檻高。所以本書先介紹Keras，Keras是高階的深度學習程式庫（以TensorFlow作為後端引擎），對初學者學習門檻低，可以很容易地建立深度學習模型，並且進行訓練、預測。等讀者熟悉深度學習模型概念與應用後，再來學習TensorFlow就很輕鬆了。

【在Windows安裝TensorFlow 1.0＋Keras2.0】
對於初學者而言，在Windows安裝非常簡單容易上手。本書詳細步驟說明，如何在Windows作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【在Linux Ubuntu安裝TensorFlow 1.0＋Keras2.0】
因為Linux作業系統是大數據分析與機器學習很常用的平臺。本書詳細步驟說明，如何在Linux Ubuntu作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【使用GPU大幅加快深度學習訓練】
GPU的平行運算架構，可讓深度學習訓練比CPU快數十倍。您必須有Nvidia顯示卡。然後......(詳細內容請參閱本書)







運送及保固說明






										寄送時間：完成付款後 7   個工作天內送達(不含週六、日及國定假日。部份預購、特別訂製商品除外）
										送貨方式：透過宅配或是郵局送達，特殊商品將由原廠專人為您送達(註記於商品文案中)。消費者訂購之商品若經配送兩次無法送達，再經本公司以電話與E-mail均無法聯繫逾三天者，本公司將取消該筆訂單，並且全額退款。 
										送貨地址：請註意！本公司所提供的產品配送區域僅限於台灣本島且請勿為郵政信箱。
										售後服務：若產品本身瑕疵或運送過程導致新品瑕疵，到貨7日內可更換新品。
										關於退貨：依照消費者保護法規定，TreeMall購物消費者均享有商品到貨七天猶豫期之權益。但退回商品必須是全新狀態且完整包裝(保持商品、附件、包裝、廠商紙箱及所有附隨文件或資料之完整性)，否則恕不接受退訂。但針對商品性質特殊，一經拆除下列所載之包裝，即不接受退貨：

書籍、雜誌必須是全新狀態(包括但不限於已被讀者簽名、註記….等)與完整包裝(商品、附件、內外包裝、隨貨文件、贈品等)。
化妝品、清潔保養用品等耗損性商品之內裝部分，包括但不限於瓶蓋、封口、封膜等接觸商品內容之包裝部分。
個人用品之商品實際包裝部分。
軟體(如電腦軟體、程式等)、影音商品(如錄音帶、錄影帶、CD、VCD、DVD等)及相機之商品實際包裝部分（即除運送用之包裝外一切包裝）。
手藝商品(如縫紉、銀黏土)之實際包裝部分（即除運送用之包裝外一切包裝）。

										退換貨需知：為縮短您退貨及退款作業時間，可以先行至會員中心-訂單查詢填寫「退換貨申請」本網站即可以提供最快速的處理服務，或者亦可與TreeMall購物客服中心聯繫 TEL：0800-039-866 轉2或3，我們將有專人為您處理退換貨事宜。退回之商品必須是全新狀態且完整包裝 (包含發票、所有產品、附件、耗材、內外包裝、及所有附隨文件或資料之完整)，否則恕不接受退貨。 
純點兌換所提供之商品或服務均不可退還或更換其他商品（非不可抗力之因素損壞除外）。
										申請兌換、換購之商品或服務停止提供時，國泰世華銀行或神坊資訊有權變更商品或服務項目，另代以其他等值之類似商品或服務。  




































































		TreeMall由神坊資訊股份有限公司經營管理 非經本公司書面同意,請勿任意轉載網站內容 法務部調查局為穩定國內物價，避免不肖份子囤積商(貨)品獲利，特成立【舉報商品囤積免付費電話0800-007-007】， 盼民眾多加利用 TreeMall網路客服中心，本公司將於24小時內回覆或撥打客服電話：0809-060-699(此為商品諮詢專線，無法提供電話訂購)，電話客服時間每週一至週五早上9:30~晚上18:00。


神坊資訊│ 隱私權政策│ 合作廠商 │ 徵人啟事










華文網網路書店：TensorFlow+Keras深度學習人工智慧實務應用．電腦與網路/程式語言◎知．識．服．務．新．思．路◎


























 
春節重要公告



 



會員登入｜ 加入會員｜ 會員專區｜ 購物車｜培訓課程｜
                          王道增智會｜自資出版｜ 電子書城｜ 客服中心



























文學小說
商管創投
人文藝坊
勵志養生
進修學習
科學工程
家庭親子
玩樂天地



















書名
出版社
作者
isbn
編號




川普 | 霹靂62 | 核心競爭力 | 出書出版 | 易經占卜 | 英語會話 | 成交的秘密 | 專業培訓 | 








App程式設計入門：iPhone、iPad(附光碟)．Python+Spa．Hadoop+Spa









文學小說





文學｜小說





商管創投





財經投資｜行銷企管





人文藝坊





宗教、哲學
社會、人文、史地
藝術、美學｜電影戲劇





勵志養生





醫療、保健
料理、生活百科
教育、心理、勵志





進修學習





電腦與網路｜語言工具
雜誌、期刊｜軍政、法律
參考、考試、教科用書





科學工程





科學、自然｜工業、工程





家庭親子





家庭、親子、人際
青少年、童書





玩樂天地





旅遊、地圖｜休閒娛樂
漫畫、插圖｜限制級






 






    TensorFlow+Keras深度學習人工智慧實務應用作者：林大貴 分類：電腦與網路／程式語言叢書系列：單行本出版社：博碩文化出版日期：2017/6/9ISBN：9789864342167書籍編號：kk0444373頁數：384 定價：590 元一般會員價：79 折  466 元特別會員價：75 折  443 元書價若有異動，以出版社實際定價為準訂購後立即為您進貨訂購後立即為您進貨：目前無庫存量,讀者下訂後,開始進入調書程序,一般天數約為2-10工作日(不含例假日)。團購數最低為 15 本以上評價數：     (請將滑鼠移至星星處進行評價)  目前平均評價：文字連結 複製語法 TensorFlow+Keras深度學習人工智慧實務應用圖片連結 複製語法分享
  









•借力與整合的秘密

























內容簡介作者介紹書籍目錄
同類推薦






TensorFlow+Keras深度學習人工智慧實務應用

內容簡介

人工智慧時代來臨，必須學習的新技術
輕鬆學會「深度學習」：先學Keras再學TensorFlow

★成長最快領域：深度學習與類神經網路，是人工智慧成長最快的領域，讓電腦更接近人類的思考。
★應用深入生活：手機語音助理、人臉識別、影像辨識、手寫辨識、醫學診斷、自然語言處理。
★實作快速上手：只需Python基礎，依照本書Step by Step學習，就可以輕鬆學會深度學習概念與應用。

TensorFlow功能強大、執行效率高、支援各種平臺，然而TensorFlow是低階的深度學習程式庫，學習門檻高。所以本書先介紹Keras，Keras是高階的深度學習程式庫（以TensorFlow作為後端引擎），對初學者學習門檻低，可以很容易地建立深度學習模型，並且進行訓練、預測。等讀者熟悉深度學習模型概念與應用後，再來學習TensorFlow就很輕鬆了。

【在Windows安裝TensorFlow 1.0＋Keras2.0】
對於初學者而言，在Windows安裝非常簡單容易上手。本書詳細步驟說明，如何在Windows作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【在Linux Ubuntu安裝TensorFlow 1.0＋Keras2.0】
因為Linux作業系統是大數據分析與機器學習很常用的平臺。本書詳細步驟說明，如何在Linux Ubuntu作業系統上，安裝最新版的TensorFlow 1.0＋Keras2.0。

【使用GPU大幅加快深度學習訓練】
GPU的平行運算架構，可讓深度學習訓練比CPU快數十倍。您必須有Nvidia顯示卡。然後依照本書步驟說明，安裝Cuda、CudNN以及TensorFlow GPU版本，就可以使用GPU大幅加快深度學習訓練。

【MNIST手寫數字影像辨識，可辨識0~9的手寫數字】
以實際範例說明，如何使用Keras與TensorFlow建構MLP（多層感知器）、CNN（捲積神經網路）模型，可辨識0~9的手寫數字。

【CIFAR-10照片影像物體辨識，可辨識10種物體】
以實際範例說明，如何使用Keras建構CNN（捲積神經網路）模型，可辨識照片類別：飛機、汽車、鳥、貓、鹿、狗、青蛙、船、卡車。

【預測鐵達尼號旅客生存機率】
以實際範例說明，如何使用Keras建構MLP（多層感知器）模型、可以預測旅客及鐵達尼號電影男女主角生存機率，並且找出鐵達尼號其他旅客的感人故事。

【IMDb影評文字「自然語言處理」與「情緒分析」】
情緒分析的商業價值，在於透過文字分析，得知顧客對公司或產品的評價，以調整營運策略。本書以實際範例說明，如何運用Keras自然語言處理，並且建構MLP（多層感知器）、RNN（遞歸神經網路）、LSTM（長短期記憶）等模型，可以預測影評文字是正面或負面評價。





作者簡介 

林大貴

作者從事IT產業多年，系統設計、網站開發、數位行銷、商業智慧、大數據、機器學習等領域，具備豐富的實務經驗。目前從事大數據分析、機器學習、深度學習與人工智慧，相關的研究、教學與企業顧問。

【facebook粉絲團】 
https://www.facebook.com/TensorflowKeras/

【部落格】
http://tensorflowkeras.blogspot.tw/




目錄

CHAPTER01 人工智慧、機器學習、深度學習介紹
CHAPTER02 深度學習的原理
CHAPTER03 TensorFlow與Keras介紹
CHAPTER04 在Windows安裝TensorFlow與Keras
CHAPTER05 在Linux Ubuntu安裝TensorFlow與Keras
CHAPTER06 Keras MNIST手寫數字辨識資料集介紹
CHAPTER07 Keras多元感知器（MLP）辨識手寫數字
CHAPTER08 Keras捲積神經網路（CNN）辨識手寫數字
CHAPTER09 Keras Cifar-10影像辨識資料集介紹
CHAPTER10 Keras捲積神經網路（CNN）辨識Cifar-10影像
CHAPTER11 Keras鐵達尼號旅客資料集介紹
CHAPTER12 Keras多層感知器（MLP）預測鐵達尼號旅客生存機率
CHAPTER13 IMDb網路電影資料集與自然語言處理介紹
CHAPTER14 Keras建立MLP、RNN、LSTM模型，進行IMDb情緒分析
CHAPTER15 TensorFlow程式設計模式介紹
CHAPTER16 以TensorFlow張量運算模擬神經網路運作
CHAPTER17 TensorFlow Mnist手寫數字辨識資料集介紹
CHAPTER18 TensorFlow多層感知器MLP辨識手寫數字
CHAPTER19 TensorFlow捲積神經網路CNN辨識手寫數字
CHAPTER20 TensorFlow GPU版本安裝
CHAPTER21 使用GPU加快TensorFlow與Keras訓練
附錄A 本書範例程式下載與安裝說明





秋聲教你玩Pytho我是小小程式設計師─用JavascripLinux ShelR語言：金融演算法與深入理解Linux程打造可維護軟體｜編寫打造可維護軟體｜編寫Haskell的魔力寫程式前就該懂的演算



				
Q1：若我已報名付了1000元訂金，接下來我要做什麼？
A：感謝您報名，後續相關事情及服務我們會以e-mail和電話跟您聯絡. 您也可以主動來信(andy@book4u.com.tw)詢問。謝謝！

Q2：我本身並沒有產品和服務，也沒什麼實務上的經驗，那又能如何賺錢呢？這個課程真的有實際成效嗎？
A：我們這個課程共有11項贈品，其中第2項贈品便是資訊產品創造藍圖，它將教您如何用最簡潔而快速的方法創出屬於您自己的資訊產品。一般的課程大多是教您如何捕魚，但多數的成功致富者，其成功的關鍵卻不是仰賴捕魚技術，而是仰賴借力之術，因為唯有借力才能無中生有！才能快速成長！也才能快速致富！本課程3天共有三套樣版，其中一套樣版便是教您打造在沒有任何商品與服務，也沒有任何資源的情況下，就能快速借力致富的樣版！本課程二位講師都是白手起家，對於沒有實務經驗的新手會格外用心，請放心。

Q3：請問贈品中魚池矩陣直效聯盟VVIP是什麼？
A：這是一個強大的系統，而我們送的是體驗版，會提供你Email追客系統。國外有一種函授課程，亦即每幾天發一個課程給學員， 就是用EMAIL追客系統來實踐。EMAIL追客系統簡單的說就是今天有一個客戶在你的網站上購買產品或註冊，系統會自動回覆一封確認信，之後你可以自行設計發送內容和發信時間一次一對多發給客戶，而且每封信還會帶入客戶本身的姓名。讓客戶以為是我們專門為客戶所量身訂作的信件。所以Email追客系統可以讓你培養跟客戶的信任感，進而成交。

Q4：請問贈品中資訊產品創造藍圖是什麼？
A：這是一位叫林星?老師的課程，課程售價是9800元，現在免費送給你。此課程包含：
● 何謂資訊型產品？
● 資訊型產品的種類大解析！以及如何組織你的資訊型產品賺錢？
● 如何找到你的利基市場？
● 為什麼錯的利基市場，再好的產品與行銷也很難有效！
● 29個國外已經證實能讓你賺到錢的利基市場！
● 如何確保你做出來的產品客戶會很想購買？以及實際可執行的步驟！
● 創造資訊型產品計劃書
● 15個步驟建立你的資訊型產品事業
● E-mail精準行銷的10個法則
● 10個別人沒有告訴你的有效文案撰寫法則

Q5：請問贈品中自動財富系統 6片DVD是什麼？
A：此6片DVD定價3200元。內容為《借力淘金！最吸利的鈔級魚池賺錢術》作者之一王紫傑所錄製的DVD，內容為有關網路行銷的知識和技巧，非常豐富且實用，免費送給您。
			



 為了保障您的權益，新絲路網路書店所購買的商品均享有到貨七天的鑑賞期（含例假日）。退回之商品必須於鑑賞期內寄回（以郵戳或收執聯為憑），且商品必須是全新狀態與完整包裝(商品、附件、內外包裝、隨貨文件、贈品等)，否則恕不接受退貨。 








天瓏網路書店 | 深度學習系列書籍




























天瓏網路書店
全台最齊全
中英文電腦書專賣店







天瓏資訊圖書粉絲專頁




搜尋


資料科學
簡體書
Packt
微服務
工程師必讀經典
英文書新到貨
Python
Tensorflow
無瑕程式碼





    深度學習系列書籍
  




『人工智慧、深度學習正在穿透你我生活之中，人們也漸漸習慣這樣的生活』
身為資訊工程師的你們，駕馭這個浪潮了嗎？天瓏資訊帶著一系列優質好書陪伴您們學習、成長
此頁面為深度學習、Tensorflow 系列，繁體中文、簡體中文、原文英文書籍現貨供應，
新書出版、到貨將陸續更新於此，參考看看






活動書籍






 

      貴賓價: $730
  

深度學習、優化與識別 (Deep Learning,Optimization and Recognition)




 

      貴賓價: $336
  

深度學習理論與實踐




 

      售價: $2,907
  

Deep Learning: Practical Neural Networks with Java




78折
 
$590

      售價: $460
  

TensorFlow+Keras 深度學習人工智慧實務應用




78折
 
$360

      售價: $281
  

深度學習快速入門—使用 TensorFlow (Getting started with TensorFlow)




 

      售價: $1,197
  

Getting Started with TensorFlow (Paperback)




 

      售價: $1,881
  

Building Machine Learning Projects with TensorFlow (Paperback)




 

      售價: $1,710
  

Deep Learning with Keras




 

      售價: $1,710
  

Effective Amazon Machine Learning




 

      售價: $1,799
  

Deep Learning for Business with Python: A Very Gentle Introduction to Business Analytics Using Deep Neural Networks Paperback – October 27, 2016




 

      售價: $1,799
  

Deep Learning for Business with R: A Very Gentle Introduction To Business Analytics Using Deep Neural Networks Paperback




 

      貴賓價: $1,617
  

Deep Learning (Hardcover)




 

      售價: $1,368
  

Deep Learning with Hadoop (Paperback)




 

      售價: $1,710
  

 Deep Learning with TensorFlow (Paperback)




 

      售價: $1,881
  

TensorFlow Machine Learning Cookbook (Paperback)




 

      售價: $1,881
  

Python Deep Learning (Paperback)




 

      售價: $1,568
  

Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems




VIP 95折
 

      貴賓價: $3,420
  

Deep Learning for Medical Image Analysis




 

      貴賓價: $450
  

深度學習 : 21天實戰 Caffe




 

      貴賓價: $450
  

TensorFlow 實戰




 

      貴賓價: $450
  

Tensorflow：實戰Google深度學習框架




 

      貴賓價: $450
  

深度學習 : Caffe 之經典模型詳解與實戰




 

      貴賓價: $336
  

神經網絡與深度學習




 

      貴賓價: $450
  

解析深度學習: 語音識別實踐




 

      貴賓價: $450
  

深入淺出深度學習:原理剖析與Python實踐






1
2
下一頁






活動主題列表

資料科學
CRC好書上架
簡體中文書最新到貨
PACKT 最新到貨
Effective 系列書
微服務系列書
Ruddy老師的敏捷教室
程式設計必讀經典系列
Springer好書上架
英文書最新到貨
資料視覺化系列書
Python 系列書籍
深度學習系列書籍
無瑕的程式碼 超值合購
Maker小天地



特價書籍

Bootstrap系列76折起
松崗暢銷書展5折起








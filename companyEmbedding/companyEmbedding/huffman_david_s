

Bizapedia.com - Human Validation
















COMPANIESPEOPLEPRODUCTS/SERVICESTRADEMARKSADDRESSESPHONE BOOK




            Our system has detected a high number of page requests from your IP Address, please prove to us that you are a human to continue using our website.
        
























                            Please send me to the search subscription offer page after I successfully answer the word challenge.
                        









BIZAPEDIAPRO SEARCHPerform unlimited searches via ouradvanced search form withBizapedia Pro Search.FIND OUT MORE >BIZAPEDIAPRO DATABuild custom data listsfor one or many stateswith Bizapedia Pro Data.FIND OUT MORE >


Copyright © 2012-2017 · Bizapedia.com · All rights reserved.BlogPro SearchPro APIContact UsTerms of UsePrivacy PolicySitemapDesktop Version















Huffman coding - Wikipedia






















 






Huffman coding

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search





Huffman tree generated from the exact frequencies of the text "this is an example of a huffman tree". The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 (or 180) bits if 36 characters of 8 (or 5) bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)




Char
Freq
Code


space
7
111


a
4
010


e
4
000


f
3
1101


h
2
1010


i
2
1000


m
2
0111


n
2
0010


s
2
1011


t
2
0110


l
1
11001


o
1
00110


p
1
10011


r
1
11000


u
1
00111


x
1
10010


In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.



Contents


1 History
2 Terminology
3 Problem definition

3.1 Informal description
3.2 Formalized description
3.3 Example


4 Basic technique

4.1 Compression
4.2 Decompression


5 Main properties

5.1 Optimality


6 Variations

6.1 n-ary Huffman coding
6.2 Adaptive Huffman coding
6.3 Huffman template algorithm
6.4 Length-limited Huffman coding/minimum variance Huffman coding
6.5 Huffman coding with unequal letter costs
6.6 Optimal alphabetic binary trees (Hu–Tucker coding)
6.7 The canonical Huffman code


7 Applications
8 References
9 Bibliography
10 External links



History[edit]
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]
In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of Shannon-Fano coding.
Terminology[edit]
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition[edit]




Constructing a Huffman Tree


Informal description[edit]

Given
A set of symbols and their weights (usually proportional to probabilities).
Find
A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).

Formalized description[edit]
Input.
Alphabet 



A
=

{

a

1


,

a

2


,
⋯
,

a

n


}



{\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}

, which is the symbol alphabet of size 



n


{\displaystyle n}

.
Set 



W
=

{

w

1


,

w

2


,
⋯
,

w

n


}



{\displaystyle W=\left\{w_{1},w_{2},\cdots ,w_{n}\right\}}

, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. 




w

i


=

w
e
i
g
h
t


(

a

i


)

,
1
≤
i
≤
n


{\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}

.

Output.
Code 



C

(
A
,
W
)

=
(

c

1


,

c

2


,
⋯
,

c

n


)


{\displaystyle C\left(A,W\right)=(c_{1},c_{2},\cdots ,c_{n})}

, which is the tuple of (binary) codewords, where 




c

i




{\displaystyle c_{i}}

 is the codeword for 




a

i


,
1
≤
i
≤
n


{\displaystyle a_{i},1\leq i\leq n}

.

Goal.
Let 



L

(
C
)

=

∑

i
=
1


n




w

i


×

l
e
n
g
t
h


(

c

i


)




{\displaystyle L\left(C\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}}

 be the weighted path length of code 



C


{\displaystyle C}

. Condition: 



L

(
C
)

≤
L

(
T
)



{\displaystyle L\left(C\right)\leq L\left(T\right)}

 for any code 



T

(
A
,
W
)



{\displaystyle T\left(A,W\right)}

.
Example[edit]
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.


Input (A, W)
Symbol (ai)
a
b
c
d
e
Sum


Weights (wi)
0.10
0.15
0.30
0.16
0.29
= 1


Output C
Codewords (ci)
010
011
11
00
10
 


Codeword length (in bits)
(li)
3
3
2
2
2


Contribution to weighted path length
(li wi )
0.30
0.45
0.60
0.32
0.58
L(C) = 2.25


Optimality
Probability budget
(2−li)
1/8
1/8
1/4
1/4
1/4
= 1.00


Information content (in bits)
(−log2 wi) ≈
3.32
2.74
1.74
2.64
1.79
 


Contribution to entropy
(−wi log2 wi)
0.332
0.411
0.521
0.423
0.518
H(A) = 2.205


For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.
As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is





h
(

a

i


)
=

log

2


⁡


1

w

i




.


{\displaystyle h(a_{i})=\log _{2}{1 \over w_{i}}.}



The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:





H
(
A
)
=

∑


w

i


>
0



w

i


h
(

a

i


)
=

∑


w

i


>
0



w

i



log

2


⁡


1

w

i




=
−

∑


w

i


>
0



w

i



log

2


⁡


w

i



.


{\displaystyle H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.}



(Note: A symbol with zero probability has zero contribution to the entropy, since 




lim

w
→

0

+




w

log

2


⁡
w
=
0


{\displaystyle \lim _{w\to 0^{+}}w\log _{2}w=0}

 So for simplicity, symbols with zero probability can be left out of the formula above.)
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing 



L
(
C
)


{\displaystyle L(C)}

 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique[edit]
Compression[edit]




Visualisation of the use of Huffman coding to encode the message "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by increasing frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.






A source generates 4 different symbols 



{

a

1


,

a

2


,

a

3


,

a

4


}


{\displaystyle \{a_{1},a_{2},a_{3},a_{4}\}}

 with probability 



{
0.4
;
0.35
;
0.2
;
0.05
}


{\displaystyle \{0.4;0.35;0.2;0.05\}}

. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:


Symbol
Code


a1
0


a2
10


a3
110


a4
111


The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.


The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, 



n


{\displaystyle n}

. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to 



n


{\displaystyle n}

 leaf nodes and 



n
−
1


{\displaystyle n-1}

 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:

Create a leaf node for each symbol and add it to the priority queue.
While there is more than one node in the queue:

Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.


The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n−1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

Start with as many leaves as there are symbols.
Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
While there is more than one node in the queues:

Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
Enqueue the new node into the rear of the second queue.


The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Decompression[edit]
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just 



B

2

B




{\displaystyle B2^{B}}

 bits of information (where 



B


{\displaystyle B}

 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties[edit]
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed. This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality[edit]

See also Arithmetic coding#Huffman coding

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. The primary trade-off is higher computational complexity. Also, arithmetic coding was historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for this alternative to Huffman coding have passed into the public domain as the early patents have expired.
Huffman coding still has advantages: it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. Combining symbols together ("blocking") can be used to make Huffman coding theoretically approach the entropy limit, i.e., optimal compression. However, blocking arbitrarily large groups of symbols is impractical, as the complexity of a Huffman code is linear in the number of possibilities to be encoded, a number that is exponential in the size of a block. A practical form of blocking, in widespread use, is run-length encoding. This technique encodes counts (runs) of repeated symbols. For the simple case of Bernoulli processes, Golomb coding is optimal among run-length codes, a fact proved via the techniques of Huffman coding.[4] A similar approach is taken by fax machines using modified Huffman coding.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations[edit]
Many variations of Huffman coding exist,[5] some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time.
n-ary Huffman coding[edit]
The n-ary Huffman algorithm uses the {0, 1, ... , n − 1} alphabet to encode message and build an n-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (n equals 2) codes, except that the n least probable symbols are taken together, instead of just the 2 least probable. Note that for n greater than 2, not all sets of source words can properly form an n-ary tree for Huffman coding. In these cases, additional 0-probability place holders must be added. This is because the tree must form an n to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo n-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding[edit]
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, which is more flexible and has better compression.
Huffman template algorithm[edit]
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing 




max

i



[

w

i


+

l
e
n
g
t
h


(

c

i


)

]



{\displaystyle \max _{i}\left[w_{i}+\mathrm {length} \left(c_{i}\right)\right]}

, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance Huffman coding[edit]
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is 



O
(
n
L
)


{\displaystyle O(nL)}

, where 



L


{\displaystyle L}

 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs[edit]
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is N digits will always have a cost of N, no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
Huffman coding with unequal letter costs is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding, though it has been solved by Karp whose solution has been refined for the case of integer costs by Golin.
Optimal alphabetic binary trees (Hu–Tucker coding)[edit]
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, 



A
=

{
a
,
b
,
c
}



{\displaystyle A=\left\{a,b,c\right\}}

 could not be assigned code 



H

(
A
,
C
)

=

{
00
,
1
,
01
}



{\displaystyle H\left(A,C\right)=\left\{00,1,01\right\}}

, but instead should be assigned either 



H

(
A
,
C
)

=

{
00
,
01
,
1
}



{\displaystyle H\left(A,C\right)=\left\{00,01,1\right\}}

 or 



H

(
A
,
C
)

=

{
0
,
10
,
11
}



{\displaystyle H\left(A,C\right)=\left\{0,10,11\right\}}

. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem,[6] which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code[edit]
Main article: Canonical Huffman code
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the canonical Huffman code and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is 



{
000
,
001
,
01
,
10
,
11
}


{\displaystyle \{000,001,01,10,11\}}

, which, having the same codeword lengths as the original solution, is also optimal. But in canonical Huffman code, the result is 



{
110
,
111
,
00
,
01
,
10
}


{\displaystyle \{110,111,00,01,10\}}

.
Applications[edit]
Arithmetic coding and Huffman coding produce equivalent results — achieving entropy — when every symbol has a probability of the form 1/2k. In other circumstances, arithmetic coding can offer better compression than Huffman coding because — intuitively — its "code words" can have effectively non-integer bit lengths, whereas code words in prefix codes such as Huffman codes can only have an integer number of bits. Therefore, a code word of length k only optimally matches a symbol of probability 1/2k and other probabilities are not represented optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol. This difference is especially striking for small alphabet sizes.
Prefix codes nevertheless remain in wide use because of their simplicity, high speed, and lack of patent coverage. They are often used as a "back-end" to other compression methods. DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by the use of prefix codes; these are often called "Huffman codes" even though most applications use pre-defined variable-length codes rather than codes designed using Huffman's algorithm.
References[edit]



Wikimedia Commons has media related to Huffman coding.





^ Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
^ Van Leeuwen, Jan (1976). "On the construction of Huffman trees" (PDF). ICALP: 382–410. Retrieved 20 February 2014. 
^ Huffman, Ken (1991). "Profile: David A. Huffman: Encoding the “Neatness” of Ones and Zeroes". Scientific American: 54–58. 
^ Gallager, R.G.; van Voorhis, D.C. (1975). "Optimal source codes for geometrically distributed integer alphabets". IEEE Transactions on Information Theory. 21 (2): 228–230. 
^ Abrahams, J. (1997-06-11). Written at Arlington, VA, USA. Division of Mathematics, Computer & Information Sciences, Office of Naval Research (ONR). "Code and Parse Trees for Lossless Source Encoding". Compression and Complexity of Sequences 1997 Proceedings. Salerno: IEEE: 145–171. ISBN 0-8186-8132-2. doi:10.1109/SEQUEN.1997.666911. Retrieved 2016-02-09. 
^ Hu, T. C.; Tucker, A. C. (1971). "Optimal Computer Search Trees and Variable-Length Alphabetical Codes". SIAM Journal on Applied Mathematics. 21 (4): 514. JSTOR 2099603. doi:10.1137/0121057. 



Bibliography[edit]

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp. 385–392.

External links[edit]

Huffman coding in various languages on Rosetta Code







v
t
e


Data compression methods



Lossless




Entropy type



Unary
Arithmetic
Asymmetric Numeral Systems
Golomb
Huffman

Adaptive
Canonical
Modified


Range
Shannon
Shannon–Fano
Shannon–Fano–Elias
Tunstall
Universal

Exp-Golomb
Fibonacci
Gamma
Levenshtein







Dictionary type



Byte pair encoding
DEFLATE
Snappy
Lempel–Ziv

LZ77 / LZ78 (LZ1 / LZ2)
LZJB
LZMA
LZO
LZRW
LZS
LZSS
LZW
LZWL
LZX
LZ4
Brotli
Statistical
Zstd







Other types



BWT
CTW
Delta
DMC
MTF
PAQ
PPM
RLE








Audio




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Companding
Convolution
Dynamic range
Latency
Nyquist–Shannon theorem
Sampling
Sound quality
Speech coding
Sub-band coding





Codec parts



A-law
μ-law
ACELP
ADPCM
CELP
DPCM
Fourier transform
LPC

LAR
LSP


MDCT
Psychoacoustic model
WLPC








Image




Concepts



Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image





Methods



Chain code
DCT
EZW
Fractal
KLT
LP
RLE
SPIHT
Wavelet








Video




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality





Codec parts



Lapped transform
DCT
Deblocking filter
Motion compensation








Theory



Entropy
Kolmogorov complexity
Lossy
Quantization
Rate–distortion
Redundancy
Timeline of information theory








 Compression formats
 Compression software (codecs)










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Huffman_coding&oldid=792304118"					
Categories: 1952 in computer scienceLossless compression algorithmsBinary trees 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoעבריתქართულიMagyarമലയാളംNederlands日本語Norsk bokmålPolskiPortuguêsРусскийСрпски / srpskiSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 25 July 2017, at 19:16.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 









Huffman coding - Wikipedia






















 






Huffman coding

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search





Huffman tree generated from the exact frequencies of the text "this is an example of a huffman tree". The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 (or 180) bits if 36 characters of 8 (or 5) bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)




Char
Freq
Code


space
7
111


a
4
010


e
4
000


f
3
1101


h
2
1010


i
2
1000


m
2
0111


n
2
0010


s
2
1011


t
2
0110


l
1
11001


o
1
00110


p
1
10011


r
1
11000


u
1
00111


x
1
10010


In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.



Contents


1 History
2 Terminology
3 Problem definition

3.1 Informal description
3.2 Formalized description
3.3 Example


4 Basic technique

4.1 Compression
4.2 Decompression


5 Main properties

5.1 Optimality


6 Variations

6.1 n-ary Huffman coding
6.2 Adaptive Huffman coding
6.3 Huffman template algorithm
6.4 Length-limited Huffman coding/minimum variance Huffman coding
6.5 Huffman coding with unequal letter costs
6.6 Optimal alphabetic binary trees (Hu–Tucker coding)
6.7 The canonical Huffman code


7 Applications
8 References
9 Bibliography
10 External links



History[edit]
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]
In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of Shannon-Fano coding.
Terminology[edit]
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition[edit]




Constructing a Huffman Tree


Informal description[edit]

Given
A set of symbols and their weights (usually proportional to probabilities).
Find
A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).

Formalized description[edit]
Input.
Alphabet 



A
=

{

a

1


,

a

2


,
⋯
,

a

n


}



{\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}

, which is the symbol alphabet of size 



n


{\displaystyle n}

.
Set 



W
=

{

w

1


,

w

2


,
⋯
,

w

n


}



{\displaystyle W=\left\{w_{1},w_{2},\cdots ,w_{n}\right\}}

, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. 




w

i


=

w
e
i
g
h
t


(

a

i


)

,
1
≤
i
≤
n


{\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}

.

Output.
Code 



C

(
A
,
W
)

=
(

c

1


,

c

2


,
⋯
,

c

n


)


{\displaystyle C\left(A,W\right)=(c_{1},c_{2},\cdots ,c_{n})}

, which is the tuple of (binary) codewords, where 




c

i




{\displaystyle c_{i}}

 is the codeword for 




a

i


,
1
≤
i
≤
n


{\displaystyle a_{i},1\leq i\leq n}

.

Goal.
Let 



L

(
C
)

=

∑

i
=
1


n




w

i


×

l
e
n
g
t
h


(

c

i


)




{\displaystyle L\left(C\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}}

 be the weighted path length of code 



C


{\displaystyle C}

. Condition: 



L

(
C
)

≤
L

(
T
)



{\displaystyle L\left(C\right)\leq L\left(T\right)}

 for any code 



T

(
A
,
W
)



{\displaystyle T\left(A,W\right)}

.
Example[edit]
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.


Input (A, W)
Symbol (ai)
a
b
c
d
e
Sum


Weights (wi)
0.10
0.15
0.30
0.16
0.29
= 1


Output C
Codewords (ci)
010
011
11
00
10
 


Codeword length (in bits)
(li)
3
3
2
2
2


Contribution to weighted path length
(li wi )
0.30
0.45
0.60
0.32
0.58
L(C) = 2.25


Optimality
Probability budget
(2−li)
1/8
1/8
1/4
1/4
1/4
= 1.00


Information content (in bits)
(−log2 wi) ≈
3.32
2.74
1.74
2.64
1.79
 


Contribution to entropy
(−wi log2 wi)
0.332
0.411
0.521
0.423
0.518
H(A) = 2.205


For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.
As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is





h
(

a

i


)
=

log

2


⁡


1

w

i




.


{\displaystyle h(a_{i})=\log _{2}{1 \over w_{i}}.}



The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:





H
(
A
)
=

∑


w

i


>
0



w

i


h
(

a

i


)
=

∑


w

i


>
0



w

i



log

2


⁡


1

w

i




=
−

∑


w

i


>
0



w

i



log

2


⁡


w

i



.


{\displaystyle H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.}



(Note: A symbol with zero probability has zero contribution to the entropy, since 




lim

w
→

0

+




w

log

2


⁡
w
=
0


{\displaystyle \lim _{w\to 0^{+}}w\log _{2}w=0}

 So for simplicity, symbols with zero probability can be left out of the formula above.)
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing 



L
(
C
)


{\displaystyle L(C)}

 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique[edit]
Compression[edit]




Visualisation of the use of Huffman coding to encode the message "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by increasing frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.






A source generates 4 different symbols 



{

a

1


,

a

2


,

a

3


,

a

4


}


{\displaystyle \{a_{1},a_{2},a_{3},a_{4}\}}

 with probability 



{
0.4
;
0.35
;
0.2
;
0.05
}


{\displaystyle \{0.4;0.35;0.2;0.05\}}

. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:


Symbol
Code


a1
0


a2
10


a3
110


a4
111


The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.


The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, 



n


{\displaystyle n}

. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to 



n


{\displaystyle n}

 leaf nodes and 



n
−
1


{\displaystyle n-1}

 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:

Create a leaf node for each symbol and add it to the priority queue.
While there is more than one node in the queue:

Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.


The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n−1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

Start with as many leaves as there are symbols.
Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
While there is more than one node in the queues:

Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
Enqueue the new node into the rear of the second queue.


The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Decompression[edit]
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just 



B

2

B




{\displaystyle B2^{B}}

 bits of information (where 



B


{\displaystyle B}

 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties[edit]
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed. This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality[edit]

See also Arithmetic coding#Huffman coding

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. The primary trade-off is higher computational complexity. Also, arithmetic coding was historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for this alternative to Huffman coding have passed into the public domain as the early patents have expired.
Huffman coding still has advantages: it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. Combining symbols together ("blocking") can be used to make Huffman coding theoretically approach the entropy limit, i.e., optimal compression. However, blocking arbitrarily large groups of symbols is impractical, as the complexity of a Huffman code is linear in the number of possibilities to be encoded, a number that is exponential in the size of a block. A practical form of blocking, in widespread use, is run-length encoding. This technique encodes counts (runs) of repeated symbols. For the simple case of Bernoulli processes, Golomb coding is optimal among run-length codes, a fact proved via the techniques of Huffman coding.[4] A similar approach is taken by fax machines using modified Huffman coding.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations[edit]
Many variations of Huffman coding exist,[5] some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time.
n-ary Huffman coding[edit]
The n-ary Huffman algorithm uses the {0, 1, ... , n − 1} alphabet to encode message and build an n-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (n equals 2) codes, except that the n least probable symbols are taken together, instead of just the 2 least probable. Note that for n greater than 2, not all sets of source words can properly form an n-ary tree for Huffman coding. In these cases, additional 0-probability place holders must be added. This is because the tree must form an n to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo n-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding[edit]
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, which is more flexible and has better compression.
Huffman template algorithm[edit]
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing 




max

i



[

w

i


+

l
e
n
g
t
h


(

c

i


)

]



{\displaystyle \max _{i}\left[w_{i}+\mathrm {length} \left(c_{i}\right)\right]}

, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance Huffman coding[edit]
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is 



O
(
n
L
)


{\displaystyle O(nL)}

, where 



L


{\displaystyle L}

 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs[edit]
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is N digits will always have a cost of N, no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
Huffman coding with unequal letter costs is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding, though it has been solved by Karp whose solution has been refined for the case of integer costs by Golin.
Optimal alphabetic binary trees (Hu–Tucker coding)[edit]
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, 



A
=

{
a
,
b
,
c
}



{\displaystyle A=\left\{a,b,c\right\}}

 could not be assigned code 



H

(
A
,
C
)

=

{
00
,
1
,
01
}



{\displaystyle H\left(A,C\right)=\left\{00,1,01\right\}}

, but instead should be assigned either 



H

(
A
,
C
)

=

{
00
,
01
,
1
}



{\displaystyle H\left(A,C\right)=\left\{00,01,1\right\}}

 or 



H

(
A
,
C
)

=

{
0
,
10
,
11
}



{\displaystyle H\left(A,C\right)=\left\{0,10,11\right\}}

. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem,[6] which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code[edit]
Main article: Canonical Huffman code
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the canonical Huffman code and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is 



{
000
,
001
,
01
,
10
,
11
}


{\displaystyle \{000,001,01,10,11\}}

, which, having the same codeword lengths as the original solution, is also optimal. But in canonical Huffman code, the result is 



{
110
,
111
,
00
,
01
,
10
}


{\displaystyle \{110,111,00,01,10\}}

.
Applications[edit]
Arithmetic coding and Huffman coding produce equivalent results — achieving entropy — when every symbol has a probability of the form 1/2k. In other circumstances, arithmetic coding can offer better compression than Huffman coding because — intuitively — its "code words" can have effectively non-integer bit lengths, whereas code words in prefix codes such as Huffman codes can only have an integer number of bits. Therefore, a code word of length k only optimally matches a symbol of probability 1/2k and other probabilities are not represented optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol. This difference is especially striking for small alphabet sizes.
Prefix codes nevertheless remain in wide use because of their simplicity, high speed, and lack of patent coverage. They are often used as a "back-end" to other compression methods. DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by the use of prefix codes; these are often called "Huffman codes" even though most applications use pre-defined variable-length codes rather than codes designed using Huffman's algorithm.
References[edit]



Wikimedia Commons has media related to Huffman coding.





^ Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
^ Van Leeuwen, Jan (1976). "On the construction of Huffman trees" (PDF). ICALP: 382–410. Retrieved 20 February 2014. 
^ Huffman, Ken (1991). "Profile: David A. Huffman: Encoding the “Neatness” of Ones and Zeroes". Scientific American: 54–58. 
^ Gallager, R.G.; van Voorhis, D.C. (1975). "Optimal source codes for geometrically distributed integer alphabets". IEEE Transactions on Information Theory. 21 (2): 228–230. 
^ Abrahams, J. (1997-06-11). Written at Arlington, VA, USA. Division of Mathematics, Computer & Information Sciences, Office of Naval Research (ONR). "Code and Parse Trees for Lossless Source Encoding". Compression and Complexity of Sequences 1997 Proceedings. Salerno: IEEE: 145–171. ISBN 0-8186-8132-2. doi:10.1109/SEQUEN.1997.666911. Retrieved 2016-02-09. 
^ Hu, T. C.; Tucker, A. C. (1971). "Optimal Computer Search Trees and Variable-Length Alphabetical Codes". SIAM Journal on Applied Mathematics. 21 (4): 514. JSTOR 2099603. doi:10.1137/0121057. 



Bibliography[edit]

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp. 385–392.

External links[edit]

Huffman coding in various languages on Rosetta Code







v
t
e


Data compression methods



Lossless




Entropy type



Unary
Arithmetic
Asymmetric Numeral Systems
Golomb
Huffman

Adaptive
Canonical
Modified


Range
Shannon
Shannon–Fano
Shannon–Fano–Elias
Tunstall
Universal

Exp-Golomb
Fibonacci
Gamma
Levenshtein







Dictionary type



Byte pair encoding
DEFLATE
Snappy
Lempel–Ziv

LZ77 / LZ78 (LZ1 / LZ2)
LZJB
LZMA
LZO
LZRW
LZS
LZSS
LZW
LZWL
LZX
LZ4
Brotli
Statistical
Zstd







Other types



BWT
CTW
Delta
DMC
MTF
PAQ
PPM
RLE








Audio




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Companding
Convolution
Dynamic range
Latency
Nyquist–Shannon theorem
Sampling
Sound quality
Speech coding
Sub-band coding





Codec parts



A-law
μ-law
ACELP
ADPCM
CELP
DPCM
Fourier transform
LPC

LAR
LSP


MDCT
Psychoacoustic model
WLPC








Image




Concepts



Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image





Methods



Chain code
DCT
EZW
Fractal
KLT
LP
RLE
SPIHT
Wavelet








Video




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality





Codec parts



Lapped transform
DCT
Deblocking filter
Motion compensation








Theory



Entropy
Kolmogorov complexity
Lossy
Quantization
Rate–distortion
Redundancy
Timeline of information theory








 Compression formats
 Compression software (codecs)










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Huffman_coding&oldid=792304118"					
Categories: 1952 in computer scienceLossless compression algorithmsBinary trees 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoעבריתქართულიMagyarമലയാളംNederlands日本語Norsk bokmålPolskiPortuguêsРусскийСрпски / srpskiSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 25 July 2017, at 19:16.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 









Huffman coding - Wikipedia






















 






Huffman coding

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search





Huffman tree generated from the exact frequencies of the text "this is an example of a huffman tree". The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 (or 180) bits if 36 characters of 8 (or 5) bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)




Char
Freq
Code


space
7
111


a
4
010


e
4
000


f
3
1101


h
2
1010


i
2
1000


m
2
0111


n
2
0010


s
2
1011


t
2
0110


l
1
11001


o
1
00110


p
1
10011


r
1
11000


u
1
00111


x
1
10010


In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.



Contents


1 History
2 Terminology
3 Problem definition

3.1 Informal description
3.2 Formalized description
3.3 Example


4 Basic technique

4.1 Compression
4.2 Decompression


5 Main properties

5.1 Optimality


6 Variations

6.1 n-ary Huffman coding
6.2 Adaptive Huffman coding
6.3 Huffman template algorithm
6.4 Length-limited Huffman coding/minimum variance Huffman coding
6.5 Huffman coding with unequal letter costs
6.6 Optimal alphabetic binary trees (Hu–Tucker coding)
6.7 The canonical Huffman code


7 Applications
8 References
9 Bibliography
10 External links



History[edit]
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]
In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of Shannon-Fano coding.
Terminology[edit]
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition[edit]




Constructing a Huffman Tree


Informal description[edit]

Given
A set of symbols and their weights (usually proportional to probabilities).
Find
A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).

Formalized description[edit]
Input.
Alphabet 



A
=

{

a

1


,

a

2


,
⋯
,

a

n


}



{\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}

, which is the symbol alphabet of size 



n


{\displaystyle n}

.
Set 



W
=

{

w

1


,

w

2


,
⋯
,

w

n


}



{\displaystyle W=\left\{w_{1},w_{2},\cdots ,w_{n}\right\}}

, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. 




w

i


=

w
e
i
g
h
t


(

a

i


)

,
1
≤
i
≤
n


{\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}

.

Output.
Code 



C

(
A
,
W
)

=
(

c

1


,

c

2


,
⋯
,

c

n


)


{\displaystyle C\left(A,W\right)=(c_{1},c_{2},\cdots ,c_{n})}

, which is the tuple of (binary) codewords, where 




c

i




{\displaystyle c_{i}}

 is the codeword for 




a

i


,
1
≤
i
≤
n


{\displaystyle a_{i},1\leq i\leq n}

.

Goal.
Let 



L

(
C
)

=

∑

i
=
1


n




w

i


×

l
e
n
g
t
h


(

c

i


)




{\displaystyle L\left(C\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}}

 be the weighted path length of code 



C


{\displaystyle C}

. Condition: 



L

(
C
)

≤
L

(
T
)



{\displaystyle L\left(C\right)\leq L\left(T\right)}

 for any code 



T

(
A
,
W
)



{\displaystyle T\left(A,W\right)}

.
Example[edit]
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.


Input (A, W)
Symbol (ai)
a
b
c
d
e
Sum


Weights (wi)
0.10
0.15
0.30
0.16
0.29
= 1


Output C
Codewords (ci)
010
011
11
00
10
 


Codeword length (in bits)
(li)
3
3
2
2
2


Contribution to weighted path length
(li wi )
0.30
0.45
0.60
0.32
0.58
L(C) = 2.25


Optimality
Probability budget
(2−li)
1/8
1/8
1/4
1/4
1/4
= 1.00


Information content (in bits)
(−log2 wi) ≈
3.32
2.74
1.74
2.64
1.79
 


Contribution to entropy
(−wi log2 wi)
0.332
0.411
0.521
0.423
0.518
H(A) = 2.205


For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.
As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is





h
(

a

i


)
=

log

2


⁡


1

w

i




.


{\displaystyle h(a_{i})=\log _{2}{1 \over w_{i}}.}



The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:





H
(
A
)
=

∑


w

i


>
0



w

i


h
(

a

i


)
=

∑


w

i


>
0



w

i



log

2


⁡


1

w

i




=
−

∑


w

i


>
0



w

i



log

2


⁡


w

i



.


{\displaystyle H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.}



(Note: A symbol with zero probability has zero contribution to the entropy, since 




lim

w
→

0

+




w

log

2


⁡
w
=
0


{\displaystyle \lim _{w\to 0^{+}}w\log _{2}w=0}

 So for simplicity, symbols with zero probability can be left out of the formula above.)
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing 



L
(
C
)


{\displaystyle L(C)}

 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique[edit]
Compression[edit]




Visualisation of the use of Huffman coding to encode the message "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by increasing frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.






A source generates 4 different symbols 



{

a

1


,

a

2


,

a

3


,

a

4


}


{\displaystyle \{a_{1},a_{2},a_{3},a_{4}\}}

 with probability 



{
0.4
;
0.35
;
0.2
;
0.05
}


{\displaystyle \{0.4;0.35;0.2;0.05\}}

. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:


Symbol
Code


a1
0


a2
10


a3
110


a4
111


The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.


The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, 



n


{\displaystyle n}

. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to 



n


{\displaystyle n}

 leaf nodes and 



n
−
1


{\displaystyle n-1}

 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:

Create a leaf node for each symbol and add it to the priority queue.
While there is more than one node in the queue:

Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.


The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n−1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

Start with as many leaves as there are symbols.
Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
While there is more than one node in the queues:

Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
Enqueue the new node into the rear of the second queue.


The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Decompression[edit]
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just 



B

2

B




{\displaystyle B2^{B}}

 bits of information (where 



B


{\displaystyle B}

 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties[edit]
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed. This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality[edit]

See also Arithmetic coding#Huffman coding

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. The primary trade-off is higher computational complexity. Also, arithmetic coding was historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for this alternative to Huffman coding have passed into the public domain as the early patents have expired.
Huffman coding still has advantages: it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. Combining symbols together ("blocking") can be used to make Huffman coding theoretically approach the entropy limit, i.e., optimal compression. However, blocking arbitrarily large groups of symbols is impractical, as the complexity of a Huffman code is linear in the number of possibilities to be encoded, a number that is exponential in the size of a block. A practical form of blocking, in widespread use, is run-length encoding. This technique encodes counts (runs) of repeated symbols. For the simple case of Bernoulli processes, Golomb coding is optimal among run-length codes, a fact proved via the techniques of Huffman coding.[4] A similar approach is taken by fax machines using modified Huffman coding.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations[edit]
Many variations of Huffman coding exist,[5] some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time.
n-ary Huffman coding[edit]
The n-ary Huffman algorithm uses the {0, 1, ... , n − 1} alphabet to encode message and build an n-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (n equals 2) codes, except that the n least probable symbols are taken together, instead of just the 2 least probable. Note that for n greater than 2, not all sets of source words can properly form an n-ary tree for Huffman coding. In these cases, additional 0-probability place holders must be added. This is because the tree must form an n to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo n-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding[edit]
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, which is more flexible and has better compression.
Huffman template algorithm[edit]
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing 




max

i



[

w

i


+

l
e
n
g
t
h


(

c

i


)

]



{\displaystyle \max _{i}\left[w_{i}+\mathrm {length} \left(c_{i}\right)\right]}

, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance Huffman coding[edit]
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is 



O
(
n
L
)


{\displaystyle O(nL)}

, where 



L


{\displaystyle L}

 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs[edit]
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is N digits will always have a cost of N, no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
Huffman coding with unequal letter costs is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding, though it has been solved by Karp whose solution has been refined for the case of integer costs by Golin.
Optimal alphabetic binary trees (Hu–Tucker coding)[edit]
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, 



A
=

{
a
,
b
,
c
}



{\displaystyle A=\left\{a,b,c\right\}}

 could not be assigned code 



H

(
A
,
C
)

=

{
00
,
1
,
01
}



{\displaystyle H\left(A,C\right)=\left\{00,1,01\right\}}

, but instead should be assigned either 



H

(
A
,
C
)

=

{
00
,
01
,
1
}



{\displaystyle H\left(A,C\right)=\left\{00,01,1\right\}}

 or 



H

(
A
,
C
)

=

{
0
,
10
,
11
}



{\displaystyle H\left(A,C\right)=\left\{0,10,11\right\}}

. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem,[6] which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code[edit]
Main article: Canonical Huffman code
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the canonical Huffman code and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is 



{
000
,
001
,
01
,
10
,
11
}


{\displaystyle \{000,001,01,10,11\}}

, which, having the same codeword lengths as the original solution, is also optimal. But in canonical Huffman code, the result is 



{
110
,
111
,
00
,
01
,
10
}


{\displaystyle \{110,111,00,01,10\}}

.
Applications[edit]
Arithmetic coding and Huffman coding produce equivalent results — achieving entropy — when every symbol has a probability of the form 1/2k. In other circumstances, arithmetic coding can offer better compression than Huffman coding because — intuitively — its "code words" can have effectively non-integer bit lengths, whereas code words in prefix codes such as Huffman codes can only have an integer number of bits. Therefore, a code word of length k only optimally matches a symbol of probability 1/2k and other probabilities are not represented optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol. This difference is especially striking for small alphabet sizes.
Prefix codes nevertheless remain in wide use because of their simplicity, high speed, and lack of patent coverage. They are often used as a "back-end" to other compression methods. DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by the use of prefix codes; these are often called "Huffman codes" even though most applications use pre-defined variable-length codes rather than codes designed using Huffman's algorithm.
References[edit]



Wikimedia Commons has media related to Huffman coding.





^ Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
^ Van Leeuwen, Jan (1976). "On the construction of Huffman trees" (PDF). ICALP: 382–410. Retrieved 20 February 2014. 
^ Huffman, Ken (1991). "Profile: David A. Huffman: Encoding the “Neatness” of Ones and Zeroes". Scientific American: 54–58. 
^ Gallager, R.G.; van Voorhis, D.C. (1975). "Optimal source codes for geometrically distributed integer alphabets". IEEE Transactions on Information Theory. 21 (2): 228–230. 
^ Abrahams, J. (1997-06-11). Written at Arlington, VA, USA. Division of Mathematics, Computer & Information Sciences, Office of Naval Research (ONR). "Code and Parse Trees for Lossless Source Encoding". Compression and Complexity of Sequences 1997 Proceedings. Salerno: IEEE: 145–171. ISBN 0-8186-8132-2. doi:10.1109/SEQUEN.1997.666911. Retrieved 2016-02-09. 
^ Hu, T. C.; Tucker, A. C. (1971). "Optimal Computer Search Trees and Variable-Length Alphabetical Codes". SIAM Journal on Applied Mathematics. 21 (4): 514. JSTOR 2099603. doi:10.1137/0121057. 



Bibliography[edit]

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp. 385–392.

External links[edit]

Huffman coding in various languages on Rosetta Code







v
t
e


Data compression methods



Lossless




Entropy type



Unary
Arithmetic
Asymmetric Numeral Systems
Golomb
Huffman

Adaptive
Canonical
Modified


Range
Shannon
Shannon–Fano
Shannon–Fano–Elias
Tunstall
Universal

Exp-Golomb
Fibonacci
Gamma
Levenshtein







Dictionary type



Byte pair encoding
DEFLATE
Snappy
Lempel–Ziv

LZ77 / LZ78 (LZ1 / LZ2)
LZJB
LZMA
LZO
LZRW
LZS
LZSS
LZW
LZWL
LZX
LZ4
Brotli
Statistical
Zstd







Other types



BWT
CTW
Delta
DMC
MTF
PAQ
PPM
RLE








Audio




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Companding
Convolution
Dynamic range
Latency
Nyquist–Shannon theorem
Sampling
Sound quality
Speech coding
Sub-band coding





Codec parts



A-law
μ-law
ACELP
ADPCM
CELP
DPCM
Fourier transform
LPC

LAR
LSP


MDCT
Psychoacoustic model
WLPC








Image




Concepts



Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image





Methods



Chain code
DCT
EZW
Fractal
KLT
LP
RLE
SPIHT
Wavelet








Video




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality





Codec parts



Lapped transform
DCT
Deblocking filter
Motion compensation








Theory



Entropy
Kolmogorov complexity
Lossy
Quantization
Rate–distortion
Redundancy
Timeline of information theory








 Compression formats
 Compression software (codecs)










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Huffman_coding&oldid=792304118"					
Categories: 1952 in computer scienceLossless compression algorithmsBinary trees 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoעבריתქართულიMagyarമലയാളംNederlands日本語Norsk bokmålPolskiPortuguêsРусскийСрпски / srpskiSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 25 July 2017, at 19:16.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 









David S Huffman - Westlake, OH | Intelius



























Sign In



We found David S Huffman in Westlake, OH


David S Huffman

                                                                           Intelius found that David S Huffman  is  a male between 60 and 60 years old from Westlake, OH.  We have connected them to
                5 addresses,
                4 phones,
                and 3 relatives or associates.
         





Also Known As

David A Huffman


Get Report Now

Age

David S Huffman is in his 60s

David Has Lived In

Westlake, OH
Cleveland, OH

David's Relatives

Joyce Huffman
Charise Winter
Jennifer Huffman







David S Huffman



Zodiac SignAquarius



GenderMale



Get Report Now










Want to know more about David? Get a comprehensive background report, find full phone numbers, and other contact information when available, all from billions of available public records.
            Continue below for more details about David, or use our people search engine to find others.
Get Background Check on David S Huffman
Get a Criminal Check on David S Huffman
Get a Public Record Report on David S Huffman
Get a People Search Report on David S Huffman


David S Huffman's Contact Information
Known Cities Lived In
Find out where David S Huffman has lived as well as David S Huffman's phone numbers and email addresses.




David S Huffman Has Lived in 1 States
Ohio Address for David S Huffman


1912 C******* R* 

Westlake, OH


Has Lived In

Westlake, OH
Cleveland, OH


Get Full Address Report










Phone Numbers Associated with David S Huffman

(440) ***-**** - Westlake, OH 
(440) ***-**** - Westlake, OH 
(440) ***-**** - Westlake, OH 


Get Full Phone Report



Email Addresses Associated with David S Huffman

d***********n@***.com


Get Email Report




David S Huffman's Education Information
Known Schools Attended
Learn about David S Huffman's academic history.  Find out which schools David S Huffman attended, the dates attended as well as the degrees David S Huffman received.
            The following data is not guaranteed for accuracy and should not be used for employment, insurance, credit eligibility, or for any other purpose covered under the Fair Credit Reporting Act.


David S Huffman Has Attended 1 School
J. F. Rhodes H.S. 1970 – 1974                              


David S Huffman's Social Network and Potential Email Matches
Find out potential social network profiles and potential email usernamed for David S Huffman


David S Huffman's known Social Networks And Potential Email Matches

Find all of David S Huffman's Social Network Profiles

Get Full Report

Search Social Networks
Including Facebook, LinkedIn, MySpace, Google, Twitter, Yahoo, Jigsaw, ZoomInfo and more



Potential Email Matches
David Huffman
Username Matches

                  DavidHuffman
                  HuffmanDavid
                  David.Huffman
                  Huffman.David
                  David_Huffman
                  Huffman_David
                  David-Huffman
                  Huffman-David
                  DHuffman
               


Popular Email Services

@gmail.com
@aol.com
@yahoo.com
@comcast.net
@hotmail.com
@msn.com
@rocketmail.com
@att.net
@sbcglobal.net
@ymail.com
@facebook.com
@mail.com
@bellsouth.net
@live.com
@earthlink.net
@cox.net
@prodigy.net
@me.com
@peoplepc.com
@juno.com
@rediffmail.com
@mindspring.com
@comcast.com
@charter.net
@outlook.com




All trademarks, product names, company names or logos on this page are the property of their respective owners.
Related People Searches
D Huffman







         Intelius is a leading provider of public data about people and their connections to others. Intelius does not provide consumer reports and is not a consumer reporting agency as defined by the Fair Credit Reporting Act (FCRA). This site should not be used to determine an individual’s eligibility for credit, insurance, employment, housing or any other purpose covered by the FCRA. For employment or tenant screening services, please visit our partner Talentwise.
      

About Us
Site Map
About Our Reports
Blog
Help
Contact Us

© 2003 – 2017 PeopleConnect, Inc. d/b/a Intelius. All Rights Reserved.
         Privacy Policy - UPDATED
Terms of Service - UPDATED






What is the FCRA?
The Fair Credit Reporting Act ("FCRA") is a federal law that promotes the accuracy, fairness and privacy of information in the files of consumer reporting agencies.
Intelius does not provide consumer reports and is not a consumer reporting agency as defined by the FCRA. Intelius reports cannot be used for background checks related to consumer credit, insurance, employment, housing or any other purpose
                  prohibited under the FCRA.
You may not use any information obtained from Intelius for any purpose covered by the FCRA.
Find out more about the FCRA here.

















Huffman David S DDS Dentist in Humble, TX



















 
Directory




Specialists
Cardiologists
Chiropractors
Dermatologists
ENT Doctors
Eye Doctors
Family Doctors



Get Listed»

Internists
Massage
Naturopaths
OBGYN
Oncologists
Pediatricians



Physical Therapists
Podiatrists
Surgeons
Urologists
Search by Name
More Specialists > 


Healthy Living
Dietitians
Fitness
Health Food
Salons
Spas
Yoga


Dental Health
Dentists
Oral Surgeons
Orthodontists



Providers A-Z
#
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z



Mental Health
Counselors
Psychiatrists
Psychologists


Senior Care
Assisted Living
Hospice
Nursing Homes


Other
Animal Hospitals
Hospitals
Medical Supplies
Pharmacies
Veterinarians




Wellness A-Z





Health
Allergies
Conditions
Diagnostics
Therapies


Lifestyle
Beauty
Diet
Fitness
Health


Nutrition
Food
Herbs
Recipes
Vitamins


Wellness Features
What Is Wellness?
Wellness News
Featured Articles


Wellness Extras
Discount Health Benefits
Product Sponsors
Wellness Store


Insurance
Health Insurance
Life Insurance





Community





Interact
Blogs
Goals
Members
Podcasts


Contribute
Write For Us
List Your Business
Publishing Rules
Join


Featured Contributors
Featured Authors
Featured Providers





Shop






















Sign Up Free|Log In













            Directory
            


            > 
        
            Providers
            


            > 
        
            Dental Care
            


            > 
        
            Dentist
            


            > 
        
            TX
            


            > 
        
            Humble
            


            > 
        
            Huffman David S DDS
            
























Profile
Reviews (22)
Phone Numbers & Directions





Huffman David S DDS



        Nearby Dentists





5

James Watson, D.D.S.            
Houston, TX





5

Bimal Mehta, D.D.S.            
Houston, TX





5

Stephen Wolters, D.D.S.            
Houston, TX



 

Sandhya Kondapaneni, DDS            
Houston, TX




Get Featured on Wellness.com

>  Learn More
















Huffman David S DDS

4
(22 Reviews)


> 
Get Phone Number & Directions



7157 Atascocita Road


Humble, TX 77346








Update Profile


Report Incorrect Info













Nearby Specialists - Call Now




(281) 895-1168
Made Ya Smile - Eldridge Lakes





(936) 657-4528
Made Ya Smile - The Woodlands



sponsored


About Huffman David S DDS
Huffman David S DDS is a Dentist facility at 7157 Atascocita Road in Humble, TX.



Primary Specialty
Dentist
















Map and Directions

7157 Atascocita Road, Humble, TX  77346













Services
Huffman David S DDS is a dentist/dental office located in Humble, TX. A dentist is trained to diagnose, treat, and prevent diseases of the gums, teeth, and jaw. Please call Huffman David S DDS at (281) 852-7874 to schedule an appointment in Humble, TX or get more information.




Consumer Feedback





            (22
 Reviews)        
4





Service







Environment







Expertise







Staff







Recommended







Value







View All  22  Reviews
Add a Review


Recent Reviews



5


>  Report this review



I love Dr. Huffman and his staff.                 






                by momma
xxx.xxx.78.226

June 04, 2015







1


>  Report this review



They answered some of my questions but I could have used more information.  Their staff was downright rude to me they didn't go out of their way to help me.  I feel like my appointment was pointless. None of my questions were answered, nothing was accomplished, and I felt worse when I left.  Office was easy to find. 






                by seh
xxx.xxx.179.191

August 16, 2012





>  View All Reviews




Recent Polls



5


>  Report this review



                            Will this Dental Care provider try to get you an appointment ASAP if you have an emergency?
                        

                            Yes
                        

                            Was the Dental Care provider able to take and develop x-rays in their office? 
                        

                            Yes
                        

                            Was the music playing while you waited or during your visit pleasant to you?
                        

                            Very
                        

                            Did this provider seem up-to-date with the current advancements in their field?
                        

                            Definitely
                        

                            How would you compare this provider to others in his/her field that you have visited?
                        

                            The best
                        






                by Jennifer
xxx.xxx.0.127

May 28, 2014







1.5


>  Report this review



                            Did this provider leave you unattended for an extended period of time?
                        

                            Yes, I was completely ignored for a long time!
                        

                            Was this provider argumentative or easily angered?
                        

                            Somewhat, they didn't want to listen to almost anything I said
                        

                            Were the staff members able to answer your questions?
                        

                            Every time I asked the staff a question, they referred me to someone else
                        

                            Did you spend a lot of time in the waiting room at this provider's office?
                        

                            I waited forever for my appointment!
                        

                            Does this provider promptly return your phone calls?
                        

                            Absolutely not! My phone calls aren't returned at all!
                        






                by Anonymous
xxx.xxx.223.18

October 09, 2013





>  View All Polls





Add a Review













Popular
Chiropractors
Eye Doctors
Massage
Nutritionists
Weight Loss
Wellness Store

Facilities
Animal Hospitals
Hospitals
Pharmacies
Veterinarians


Fitness & Beauty
Cosmetic
Fitness
Gyms
Physical Therapy
Salons
Spas
Yoga

Dental Care
Dentists
Orthodontists


Stores
Health Food
Medical Supplies

Senior Care
Assisted Living
Home Care
Hospice
Nursing Homes

Insurance
Health Insurance
Life Insurance


Doctors

Cardiologists
Dermatologists
ENT Doctors
Family Doctors
Internists


OBGYN
Oncologists
Pediatricians
Podiatrists
Surgeons
Urologists



Mental Health
Psychiatrists
Psychologists

Counseling
Counselors
Marriage/Family

Community
Members
Posts
Goals
Podcasts
Recipes


Wellness.com
About Us
Contact Us
Privacy Policy
Advertise
FAQ
Publishing Rules
What is Wellness
Write for Wellness

Provider Program
Featured Providers
Become Featured




Providers A-Z
#
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z



Feel our  on







    Wellness.com does not provide medical advice, diagnosis or treatment nor do we verify or endorse any specific business or professional 
    listed on the site. Wellness.com does not verify the accuracy or efficacy of user generated content, reviews, ratings or any published 
    content on the site. Use of this website constitutes acceptance of the Terms of Use.


©2017 Wellness®.com is a registered trademark of Wellness.com, Inc.  
















                  Ripoff Report | Hoosier Home Maintenance Complaint Review Indianapolis, Indiana                    Ripoff Report | Complaints Reviews Scams Lawsuits Frauds Reported. File your review. Consumers educating consumers.™     By consumers, for consumers...    Don't let them get away with it!® Let the truth be known!™   
            FILE A REPORT
               Review Latest Reports Advanced Search Browse Categories        Total Visits since 1998: 8,823,000,000+ Estimated money Consumers saved since 1998: $15,449,000,000+ Reports filed: 1,995,000+            Update a Report   Programs & Services    VIP Arbitration Corporate Advocacy Program – Verified Status Guardian+ Program – Verified Status Verified Status Explained      Help & FAQs    FAQs – General Questions Legal Issues Contact Us      Consumer Resources   Verified Business Directory   Legal Directory   Consumers Say Thank You   In the Media   Ripoff Report Investigates     Repair your reputation the right way Corporate Advocacy Program     Reputation Management Corporate Advocacy Program This is the best way to manage and repair your business reputation. Hiding negative complaints is only a Band-Aid. Consumers want to see how a business took care of business. All business will get complaints. How those businesses take care of those complaints is what separates good businesses from bad businesses. Consumers love to do business with someone that can admit mistakes and state how they made improvements.  Corporate Advocacy Business Remediation and Customer Satisfaction Program. A program that benefits the consumer, assures them of complete satisfaction and confidence when doing business with a member business.   Get Started       Register or  Login
                Your  Account
                         
        Home > Reports > Services > Home Improvements > Hoosier Home Maintenance > Hoosier Home Maintenance David S Huffman This So Called Company Has Taken Over 35,000 00 Of Our Money He Is A Liar And A Theif Not To Be Trusted Indianapolis, Indiana       
        Ripoff Report protects consumers first amendment right to free speech              Report: #631822     Report - Rebuttal - Arbitrate  Arbitrate  Remove Reports?
						No! Better yet! Arbitrate to set the record straight!
					    Complaint Review: Hoosier Home Maintenance      Related Reports     EDitorial Comments  Political Signs Must Go!    EDitorial Comment ED Magedson – Founder
        Ripoff Report
        Ripoff Report Investigates!  FACE CREAMRipoff! Likely in a Mall near you...       WARNING! 
        HAVE YOU BEEN A VICTIM OF"FACE CREAM FRAUD"? 
        Consumers Reported Ripoffs byMall predators
         
            Kiosk hucksters offeringfree skin cream trials...Only thing... it's NOT free!  
                WHAT IS YOUR STORY?
                ARE YOU A VICTIM?
                FORMER EMPLOYEE?   
                WE WANT TO HEAR IT!
             
                Contact Our Team Now:
             investigate@ripoffreport.com       Featured Ripoff Reports      View More RecentFeatured Reports   View past featured reports   Ripoff Report in the Media     
					Ripoff Report on CBS 19
				    
					Ripoff Report on CBS 19 - Global Marketing Alliance
				    
					Ripoff Report on ABC 15 - Smart Shopper
				    
					Ripoff Report - Girls Gone Wild
				    
					Ripoff Report on Fox 11 - Car Repair
				    See more videos   
        What the BBB has done to consumers for over 100 years is one of the many reasons why Ripoff Report was created.
        BBB: What consumers need to know
     
    20/20 exposes the real BBB
      Submitted:  Mon, August 16, 2010   Updated:  Mon, August 16, 2010    Reported By:
									    James Green/ Lisa Green —
	                                    Indianapolis Indiana
	                                    United States of America									         Hoosier Home Maintenance  7421 Acton Rd 
				    							Indianapolis, Indiana								      		 United States of America     Phone: 317 374-3118 Web: fordulator05@comcast.net  Category: Home Improvements            Hoosier Home Maintenance David S Huffman This so called company has taken over 35,000.00 of our money. He is a liar and a theif.  Not to be trusted!!!!!!!!!!!!!!!!!! Indianapolis, Indiana   Print this Report   Email this Report         Tweet     REBUTTAL BOX™ | Respond to this Report!     Add Rebuttal to this Report What's this?  Are you an owner, employee or ex-employee with either negative or positive information about the company or individual, or can you provide "insider information" on this company?    Arbitrate & Set Record Straight     File New Report What's this?  Are you also a victim of the same company or individual? Want Justice? File a Rip-off Report, help other consumers to be educated and don't let them get away with it!    Repair Your Reputation What's this?  Got Reports filed against you? Resolve the issues and rebuild trust through our Corporate Advocacy Program. Corporate Advocacy Program: The best way to manage and repair your business reputation. Hiding negative complaints is only a Band-Aid. Consumers want to see how businesses take care of business. All businesses will get complaints. How those businesses take care of those complaints is what separates good businesses from bad businesses.        0Author 0Consumer 0Employee/Owner       ← Is this Ripoff Report
    								About you?  Ripoff Report
    								A business' first
    								line of defense on the Internet.  
    								If your business is
    								willing to make a
    								commitment to
    								customer satisfaction Click here now..   
    								Does your business have a bad reputation?
    								Fix it the right way. Corporate Advocacy Program™   SEO Reputation Management at its best!    
										We hired Hoosier Home Maintenance to reside our home in James Hardie Siding and a second contract to add a master bedroom and master bath on to our home.  He required 1/2 down on each contract.  He was recommended by Brizzendine Builders.  He started the James Hardie siding and it was apparent he did not know what he was doing.  He wasted several thousand dollars worth of siding.  The city of Indianapolis came out and put a stop work order on him.  They said it was the worst James Hardie siding that they had ever seen.  So we fired him the siding Job.  Then he just quit answering his phone.  We hired a attorney. He  has over 35,000.00 of our money.  He told our attorney he would bring us 10,000.00 of our money. Then give us more later.  That never happened.  Do not trust this Company.  He now has an F on Angie's List.  A D- from the BBB. I have also turned him into the Attorney General of Indiana.  And right now I am working with the contractor licensing board.  I just do not want someone else to go through what we have.  I guess he was in finacial trouble and hid it well.  NOT TO BE TRUSTED!!!!!!!!!!!!!!!!!!!!  Not to mention this guy can lie!!!!!!!!!!!!!									    This report was posted on Ripoff Report on 08/16/2010 11:19 AM and is a permanent record located here:
									http://www.ripoffreport.com/reports/hoosier-home-maintenance/indianapolis-indiana-46259/hoosier-home-maintenance-david-s-huffman-this-so-called-company-has-taken-over-3500000-o-631822.
									The posting time indicated is Arizona local time. Arizona does not observe daylight savings so the post time may be Mountain or Pacific depending on the time of year. Ripoff Report has an exclusive license to this report. It may not be copied without the written permission of Ripoff Report. READ: Foreign websites steal our content   
										Click Here to read other Ripoff Reports on Hoosier Home Maintenance									    Search for additional reports If you would like to see more Rip-off Reports on this company/individual, search here:  Search Search Tips  In order to assure the best results in your search:  Keep the name short & simple, and try different variations of the name. Do not include ".com", "S", "Inc.", "Corp", or "LLC" at the end of the Company name. Use only the first/main part of a name to get best results. Only search one name at a time if Company has many AKA's.      
									Report & Rebuttal
								  Respond to this report! File a Rebuttal What's this?  Are you an owner, employee or ex-employee with either negative or positive information about the company or individual, or can you provide "insider information" on this company?    Also a victim? File a Report What's this?  Are you also a victim of the same company or individual? Want Justice? File a Rip-off Report, help other consumers to be educated and don't let them get away with it!    Repair Your Reputation! Get Started What's this?  Got Reports filed against you? Resolve the issues and rebuild trust through our Corporate Advocacy Program. Corporate Advocacy Program: The best way to manage and repair your business reputation. Hiding negative complaints is only a Band-Aid. Consumers want to see how businesses take care of business. All businesses will get complaints. How those businesses take care of those complaints is what separates good businesses from bad businesses.    Arbitrate  Remove Reports?
										No! Better yet! Arbitrate to set the record straight!
									                                                                   Ripoff Report RecommendsZipBooks Accounting Software
        and ZipBooks for Invoice Templates     
      Advertisers above have met our
       strict standards for business conduct.
                                     Home File a Report Consumer Resources Search Top Trends Link to Ripoff Report Customer Support for Technical Issues General Questions and Suggestions   Privacy Policy Terms of Service FAQ About Us Why Ripoff Report will not release author information! Go to Mobile Version of Ripoff Report   Thank You Emails! Corporate Advocacy Program: How to repair your business reputation. Ed Magedson - Ripoff Report Founder   Want to sue Ripoff Report? Donate to our Efforts BadBusinessBureau.com Media Requests Contact Us   
          Copyright © 1998-2017, Ripoff Report. All rights reserved.
               X                



Huffman coding - Wikipedia






















 






Huffman coding

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search





Huffman tree generated from the exact frequencies of the text "this is an example of a huffman tree". The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 (or 180) bits if 36 characters of 8 (or 5) bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)




Char
Freq
Code


space
7
111


a
4
010


e
4
000


f
3
1101


h
2
1010


i
2
1000


m
2
0111


n
2
0010


s
2
1011


t
2
0110


l
1
11001


o
1
00110


p
1
10011


r
1
11000


u
1
00111


x
1
10010


In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.



Contents


1 History
2 Terminology
3 Problem definition

3.1 Informal description
3.2 Formalized description
3.3 Example


4 Basic technique

4.1 Compression
4.2 Decompression


5 Main properties

5.1 Optimality


6 Variations

6.1 n-ary Huffman coding
6.2 Adaptive Huffman coding
6.3 Huffman template algorithm
6.4 Length-limited Huffman coding/minimum variance Huffman coding
6.5 Huffman coding with unequal letter costs
6.6 Optimal alphabetic binary trees (Hu–Tucker coding)
6.7 The canonical Huffman code


7 Applications
8 References
9 Bibliography
10 External links



History[edit]
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]
In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of Shannon-Fano coding.
Terminology[edit]
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition[edit]




Constructing a Huffman Tree


Informal description[edit]

Given
A set of symbols and their weights (usually proportional to probabilities).
Find
A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).

Formalized description[edit]
Input.
Alphabet 



A
=

{

a

1


,

a

2


,
⋯
,

a

n


}



{\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}

, which is the symbol alphabet of size 



n


{\displaystyle n}

.
Set 



W
=

{

w

1


,

w

2


,
⋯
,

w

n


}



{\displaystyle W=\left\{w_{1},w_{2},\cdots ,w_{n}\right\}}

, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. 




w

i


=

w
e
i
g
h
t


(

a

i


)

,
1
≤
i
≤
n


{\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}

.

Output.
Code 



C

(
A
,
W
)

=
(

c

1


,

c

2


,
⋯
,

c

n


)


{\displaystyle C\left(A,W\right)=(c_{1},c_{2},\cdots ,c_{n})}

, which is the tuple of (binary) codewords, where 




c

i




{\displaystyle c_{i}}

 is the codeword for 




a

i


,
1
≤
i
≤
n


{\displaystyle a_{i},1\leq i\leq n}

.

Goal.
Let 



L

(
C
)

=

∑

i
=
1


n




w

i


×

l
e
n
g
t
h


(

c

i


)




{\displaystyle L\left(C\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}}

 be the weighted path length of code 



C


{\displaystyle C}

. Condition: 



L

(
C
)

≤
L

(
T
)



{\displaystyle L\left(C\right)\leq L\left(T\right)}

 for any code 



T

(
A
,
W
)



{\displaystyle T\left(A,W\right)}

.
Example[edit]
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.


Input (A, W)
Symbol (ai)
a
b
c
d
e
Sum


Weights (wi)
0.10
0.15
0.30
0.16
0.29
= 1


Output C
Codewords (ci)
010
011
11
00
10
 


Codeword length (in bits)
(li)
3
3
2
2
2


Contribution to weighted path length
(li wi )
0.30
0.45
0.60
0.32
0.58
L(C) = 2.25


Optimality
Probability budget
(2−li)
1/8
1/8
1/4
1/4
1/4
= 1.00


Information content (in bits)
(−log2 wi) ≈
3.32
2.74
1.74
2.64
1.79
 


Contribution to entropy
(−wi log2 wi)
0.332
0.411
0.521
0.423
0.518
H(A) = 2.205


For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.
As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is





h
(

a

i


)
=

log

2


⁡


1

w

i




.


{\displaystyle h(a_{i})=\log _{2}{1 \over w_{i}}.}



The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:





H
(
A
)
=

∑


w

i


>
0



w

i


h
(

a

i


)
=

∑


w

i


>
0



w

i



log

2


⁡


1

w

i




=
−

∑


w

i


>
0



w

i



log

2


⁡


w

i



.


{\displaystyle H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.}



(Note: A symbol with zero probability has zero contribution to the entropy, since 




lim

w
→

0

+




w

log

2


⁡
w
=
0


{\displaystyle \lim _{w\to 0^{+}}w\log _{2}w=0}

 So for simplicity, symbols with zero probability can be left out of the formula above.)
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing 



L
(
C
)


{\displaystyle L(C)}

 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique[edit]
Compression[edit]




Visualisation of the use of Huffman coding to encode the message "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by increasing frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.






A source generates 4 different symbols 



{

a

1


,

a

2


,

a

3


,

a

4


}


{\displaystyle \{a_{1},a_{2},a_{3},a_{4}\}}

 with probability 



{
0.4
;
0.35
;
0.2
;
0.05
}


{\displaystyle \{0.4;0.35;0.2;0.05\}}

. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:


Symbol
Code


a1
0


a2
10


a3
110


a4
111


The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.


The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, 



n


{\displaystyle n}

. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to 



n


{\displaystyle n}

 leaf nodes and 



n
−
1


{\displaystyle n-1}

 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:

Create a leaf node for each symbol and add it to the priority queue.
While there is more than one node in the queue:

Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.


The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n−1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

Start with as many leaves as there are symbols.
Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
While there is more than one node in the queues:

Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
Enqueue the new node into the rear of the second queue.


The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Decompression[edit]
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just 



B

2

B




{\displaystyle B2^{B}}

 bits of information (where 



B


{\displaystyle B}

 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties[edit]
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed. This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality[edit]

See also Arithmetic coding#Huffman coding

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. The primary trade-off is higher computational complexity. Also, arithmetic coding was historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for this alternative to Huffman coding have passed into the public domain as the early patents have expired.
Huffman coding still has advantages: it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. Combining symbols together ("blocking") can be used to make Huffman coding theoretically approach the entropy limit, i.e., optimal compression. However, blocking arbitrarily large groups of symbols is impractical, as the complexity of a Huffman code is linear in the number of possibilities to be encoded, a number that is exponential in the size of a block. A practical form of blocking, in widespread use, is run-length encoding. This technique encodes counts (runs) of repeated symbols. For the simple case of Bernoulli processes, Golomb coding is optimal among run-length codes, a fact proved via the techniques of Huffman coding.[4] A similar approach is taken by fax machines using modified Huffman coding.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations[edit]
Many variations of Huffman coding exist,[5] some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time.
n-ary Huffman coding[edit]
The n-ary Huffman algorithm uses the {0, 1, ... , n − 1} alphabet to encode message and build an n-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (n equals 2) codes, except that the n least probable symbols are taken together, instead of just the 2 least probable. Note that for n greater than 2, not all sets of source words can properly form an n-ary tree for Huffman coding. In these cases, additional 0-probability place holders must be added. This is because the tree must form an n to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo n-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding[edit]
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, which is more flexible and has better compression.
Huffman template algorithm[edit]
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing 




max

i



[

w

i


+

l
e
n
g
t
h


(

c

i


)

]



{\displaystyle \max _{i}\left[w_{i}+\mathrm {length} \left(c_{i}\right)\right]}

, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance Huffman coding[edit]
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is 



O
(
n
L
)


{\displaystyle O(nL)}

, where 



L


{\displaystyle L}

 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs[edit]
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is N digits will always have a cost of N, no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
Huffman coding with unequal letter costs is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding, though it has been solved by Karp whose solution has been refined for the case of integer costs by Golin.
Optimal alphabetic binary trees (Hu–Tucker coding)[edit]
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, 



A
=

{
a
,
b
,
c
}



{\displaystyle A=\left\{a,b,c\right\}}

 could not be assigned code 



H

(
A
,
C
)

=

{
00
,
1
,
01
}



{\displaystyle H\left(A,C\right)=\left\{00,1,01\right\}}

, but instead should be assigned either 



H

(
A
,
C
)

=

{
00
,
01
,
1
}



{\displaystyle H\left(A,C\right)=\left\{00,01,1\right\}}

 or 



H

(
A
,
C
)

=

{
0
,
10
,
11
}



{\displaystyle H\left(A,C\right)=\left\{0,10,11\right\}}

. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem,[6] which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code[edit]
Main article: Canonical Huffman code
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the canonical Huffman code and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is 



{
000
,
001
,
01
,
10
,
11
}


{\displaystyle \{000,001,01,10,11\}}

, which, having the same codeword lengths as the original solution, is also optimal. But in canonical Huffman code, the result is 



{
110
,
111
,
00
,
01
,
10
}


{\displaystyle \{110,111,00,01,10\}}

.
Applications[edit]
Arithmetic coding and Huffman coding produce equivalent results — achieving entropy — when every symbol has a probability of the form 1/2k. In other circumstances, arithmetic coding can offer better compression than Huffman coding because — intuitively — its "code words" can have effectively non-integer bit lengths, whereas code words in prefix codes such as Huffman codes can only have an integer number of bits. Therefore, a code word of length k only optimally matches a symbol of probability 1/2k and other probabilities are not represented optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol. This difference is especially striking for small alphabet sizes.
Prefix codes nevertheless remain in wide use because of their simplicity, high speed, and lack of patent coverage. They are often used as a "back-end" to other compression methods. DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by the use of prefix codes; these are often called "Huffman codes" even though most applications use pre-defined variable-length codes rather than codes designed using Huffman's algorithm.
References[edit]



Wikimedia Commons has media related to Huffman coding.





^ Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
^ Van Leeuwen, Jan (1976). "On the construction of Huffman trees" (PDF). ICALP: 382–410. Retrieved 20 February 2014. 
^ Huffman, Ken (1991). "Profile: David A. Huffman: Encoding the “Neatness” of Ones and Zeroes". Scientific American: 54–58. 
^ Gallager, R.G.; van Voorhis, D.C. (1975). "Optimal source codes for geometrically distributed integer alphabets". IEEE Transactions on Information Theory. 21 (2): 228–230. 
^ Abrahams, J. (1997-06-11). Written at Arlington, VA, USA. Division of Mathematics, Computer & Information Sciences, Office of Naval Research (ONR). "Code and Parse Trees for Lossless Source Encoding". Compression and Complexity of Sequences 1997 Proceedings. Salerno: IEEE: 145–171. ISBN 0-8186-8132-2. doi:10.1109/SEQUEN.1997.666911. Retrieved 2016-02-09. 
^ Hu, T. C.; Tucker, A. C. (1971). "Optimal Computer Search Trees and Variable-Length Alphabetical Codes". SIAM Journal on Applied Mathematics. 21 (4): 514. JSTOR 2099603. doi:10.1137/0121057. 



Bibliography[edit]

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp. 385–392.

External links[edit]

Huffman coding in various languages on Rosetta Code







v
t
e


Data compression methods



Lossless




Entropy type



Unary
Arithmetic
Asymmetric Numeral Systems
Golomb
Huffman

Adaptive
Canonical
Modified


Range
Shannon
Shannon–Fano
Shannon–Fano–Elias
Tunstall
Universal

Exp-Golomb
Fibonacci
Gamma
Levenshtein







Dictionary type



Byte pair encoding
DEFLATE
Snappy
Lempel–Ziv

LZ77 / LZ78 (LZ1 / LZ2)
LZJB
LZMA
LZO
LZRW
LZS
LZSS
LZW
LZWL
LZX
LZ4
Brotli
Statistical
Zstd







Other types



BWT
CTW
Delta
DMC
MTF
PAQ
PPM
RLE








Audio




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Companding
Convolution
Dynamic range
Latency
Nyquist–Shannon theorem
Sampling
Sound quality
Speech coding
Sub-band coding





Codec parts



A-law
μ-law
ACELP
ADPCM
CELP
DPCM
Fourier transform
LPC

LAR
LSP


MDCT
Psychoacoustic model
WLPC








Image




Concepts



Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image





Methods



Chain code
DCT
EZW
Fractal
KLT
LP
RLE
SPIHT
Wavelet








Video




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality





Codec parts



Lapped transform
DCT
Deblocking filter
Motion compensation








Theory



Entropy
Kolmogorov complexity
Lossy
Quantization
Rate–distortion
Redundancy
Timeline of information theory








 Compression formats
 Compression software (codecs)










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Huffman_coding&oldid=792304118"					
Categories: 1952 in computer scienceLossless compression algorithmsBinary trees 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoעבריתქართულიMagyarമലയാളംNederlands日本語Norsk bokmålPolskiPortuguêsРусскийСрпски / srpskiSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 25 July 2017, at 19:16.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 









Huffman coding - Wikipedia






















 






Huffman coding

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search





Huffman tree generated from the exact frequencies of the text "this is an example of a huffman tree". The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 (or 180) bits if 36 characters of 8 (or 5) bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)




Char
Freq
Code


space
7
111


a
4
010


e
4
000


f
3
1101


h
2
1010


i
2
1000


m
2
0111


n
2
0010


s
2
1011


t
2
0110


l
1
11001


o
1
00110


p
1
10011


r
1
11000


u
1
00111


x
1
10010


In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.



Contents


1 History
2 Terminology
3 Problem definition

3.1 Informal description
3.2 Formalized description
3.3 Example


4 Basic technique

4.1 Compression
4.2 Decompression


5 Main properties

5.1 Optimality


6 Variations

6.1 n-ary Huffman coding
6.2 Adaptive Huffman coding
6.3 Huffman template algorithm
6.4 Length-limited Huffman coding/minimum variance Huffman coding
6.5 Huffman coding with unequal letter costs
6.6 Optimal alphabetic binary trees (Hu–Tucker coding)
6.7 The canonical Huffman code


7 Applications
8 References
9 Bibliography
10 External links



History[edit]
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]
In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of Shannon-Fano coding.
Terminology[edit]
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition[edit]




Constructing a Huffman Tree


Informal description[edit]

Given
A set of symbols and their weights (usually proportional to probabilities).
Find
A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).

Formalized description[edit]
Input.
Alphabet 



A
=

{

a

1


,

a

2


,
⋯
,

a

n


}



{\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}

, which is the symbol alphabet of size 



n


{\displaystyle n}

.
Set 



W
=

{

w

1


,

w

2


,
⋯
,

w

n


}



{\displaystyle W=\left\{w_{1},w_{2},\cdots ,w_{n}\right\}}

, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. 




w

i


=

w
e
i
g
h
t


(

a

i


)

,
1
≤
i
≤
n


{\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}

.

Output.
Code 



C

(
A
,
W
)

=
(

c

1


,

c

2


,
⋯
,

c

n


)


{\displaystyle C\left(A,W\right)=(c_{1},c_{2},\cdots ,c_{n})}

, which is the tuple of (binary) codewords, where 




c

i




{\displaystyle c_{i}}

 is the codeword for 




a

i


,
1
≤
i
≤
n


{\displaystyle a_{i},1\leq i\leq n}

.

Goal.
Let 



L

(
C
)

=

∑

i
=
1


n




w

i


×

l
e
n
g
t
h


(

c

i


)




{\displaystyle L\left(C\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}}

 be the weighted path length of code 



C


{\displaystyle C}

. Condition: 



L

(
C
)

≤
L

(
T
)



{\displaystyle L\left(C\right)\leq L\left(T\right)}

 for any code 



T

(
A
,
W
)



{\displaystyle T\left(A,W\right)}

.
Example[edit]
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.


Input (A, W)
Symbol (ai)
a
b
c
d
e
Sum


Weights (wi)
0.10
0.15
0.30
0.16
0.29
= 1


Output C
Codewords (ci)
010
011
11
00
10
 


Codeword length (in bits)
(li)
3
3
2
2
2


Contribution to weighted path length
(li wi )
0.30
0.45
0.60
0.32
0.58
L(C) = 2.25


Optimality
Probability budget
(2−li)
1/8
1/8
1/4
1/4
1/4
= 1.00


Information content (in bits)
(−log2 wi) ≈
3.32
2.74
1.74
2.64
1.79
 


Contribution to entropy
(−wi log2 wi)
0.332
0.411
0.521
0.423
0.518
H(A) = 2.205


For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.
As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is





h
(

a

i


)
=

log

2


⁡


1

w

i




.


{\displaystyle h(a_{i})=\log _{2}{1 \over w_{i}}.}



The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:





H
(
A
)
=

∑


w

i


>
0



w

i


h
(

a

i


)
=

∑


w

i


>
0



w

i



log

2


⁡


1

w

i




=
−

∑


w

i


>
0



w

i



log

2


⁡


w

i



.


{\displaystyle H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.}



(Note: A symbol with zero probability has zero contribution to the entropy, since 




lim

w
→

0

+




w

log

2


⁡
w
=
0


{\displaystyle \lim _{w\to 0^{+}}w\log _{2}w=0}

 So for simplicity, symbols with zero probability can be left out of the formula above.)
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing 



L
(
C
)


{\displaystyle L(C)}

 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique[edit]
Compression[edit]




Visualisation of the use of Huffman coding to encode the message "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by increasing frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.






A source generates 4 different symbols 



{

a

1


,

a

2


,

a

3


,

a

4


}


{\displaystyle \{a_{1},a_{2},a_{3},a_{4}\}}

 with probability 



{
0.4
;
0.35
;
0.2
;
0.05
}


{\displaystyle \{0.4;0.35;0.2;0.05\}}

. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:


Symbol
Code


a1
0


a2
10


a3
110


a4
111


The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.


The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, 



n


{\displaystyle n}

. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to 



n


{\displaystyle n}

 leaf nodes and 



n
−
1


{\displaystyle n-1}

 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:

Create a leaf node for each symbol and add it to the priority queue.
While there is more than one node in the queue:

Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.


The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n−1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

Start with as many leaves as there are symbols.
Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
While there is more than one node in the queues:

Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
Enqueue the new node into the rear of the second queue.


The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Decompression[edit]
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just 



B

2

B




{\displaystyle B2^{B}}

 bits of information (where 



B


{\displaystyle B}

 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties[edit]
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed. This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality[edit]

See also Arithmetic coding#Huffman coding

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. The primary trade-off is higher computational complexity. Also, arithmetic coding was historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for this alternative to Huffman coding have passed into the public domain as the early patents have expired.
Huffman coding still has advantages: it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. Combining symbols together ("blocking") can be used to make Huffman coding theoretically approach the entropy limit, i.e., optimal compression. However, blocking arbitrarily large groups of symbols is impractical, as the complexity of a Huffman code is linear in the number of possibilities to be encoded, a number that is exponential in the size of a block. A practical form of blocking, in widespread use, is run-length encoding. This technique encodes counts (runs) of repeated symbols. For the simple case of Bernoulli processes, Golomb coding is optimal among run-length codes, a fact proved via the techniques of Huffman coding.[4] A similar approach is taken by fax machines using modified Huffman coding.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations[edit]
Many variations of Huffman coding exist,[5] some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time.
n-ary Huffman coding[edit]
The n-ary Huffman algorithm uses the {0, 1, ... , n − 1} alphabet to encode message and build an n-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (n equals 2) codes, except that the n least probable symbols are taken together, instead of just the 2 least probable. Note that for n greater than 2, not all sets of source words can properly form an n-ary tree for Huffman coding. In these cases, additional 0-probability place holders must be added. This is because the tree must form an n to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo n-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding[edit]
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, which is more flexible and has better compression.
Huffman template algorithm[edit]
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing 




max

i



[

w

i


+

l
e
n
g
t
h


(

c

i


)

]



{\displaystyle \max _{i}\left[w_{i}+\mathrm {length} \left(c_{i}\right)\right]}

, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance Huffman coding[edit]
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is 



O
(
n
L
)


{\displaystyle O(nL)}

, where 



L


{\displaystyle L}

 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs[edit]
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is N digits will always have a cost of N, no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
Huffman coding with unequal letter costs is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding, though it has been solved by Karp whose solution has been refined for the case of integer costs by Golin.
Optimal alphabetic binary trees (Hu–Tucker coding)[edit]
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, 



A
=

{
a
,
b
,
c
}



{\displaystyle A=\left\{a,b,c\right\}}

 could not be assigned code 



H

(
A
,
C
)

=

{
00
,
1
,
01
}



{\displaystyle H\left(A,C\right)=\left\{00,1,01\right\}}

, but instead should be assigned either 



H

(
A
,
C
)

=

{
00
,
01
,
1
}



{\displaystyle H\left(A,C\right)=\left\{00,01,1\right\}}

 or 



H

(
A
,
C
)

=

{
0
,
10
,
11
}



{\displaystyle H\left(A,C\right)=\left\{0,10,11\right\}}

. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem,[6] which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code[edit]
Main article: Canonical Huffman code
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the canonical Huffman code and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is 



{
000
,
001
,
01
,
10
,
11
}


{\displaystyle \{000,001,01,10,11\}}

, which, having the same codeword lengths as the original solution, is also optimal. But in canonical Huffman code, the result is 



{
110
,
111
,
00
,
01
,
10
}


{\displaystyle \{110,111,00,01,10\}}

.
Applications[edit]
Arithmetic coding and Huffman coding produce equivalent results — achieving entropy — when every symbol has a probability of the form 1/2k. In other circumstances, arithmetic coding can offer better compression than Huffman coding because — intuitively — its "code words" can have effectively non-integer bit lengths, whereas code words in prefix codes such as Huffman codes can only have an integer number of bits. Therefore, a code word of length k only optimally matches a symbol of probability 1/2k and other probabilities are not represented optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol. This difference is especially striking for small alphabet sizes.
Prefix codes nevertheless remain in wide use because of their simplicity, high speed, and lack of patent coverage. They are often used as a "back-end" to other compression methods. DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by the use of prefix codes; these are often called "Huffman codes" even though most applications use pre-defined variable-length codes rather than codes designed using Huffman's algorithm.
References[edit]



Wikimedia Commons has media related to Huffman coding.





^ Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
^ Van Leeuwen, Jan (1976). "On the construction of Huffman trees" (PDF). ICALP: 382–410. Retrieved 20 February 2014. 
^ Huffman, Ken (1991). "Profile: David A. Huffman: Encoding the “Neatness” of Ones and Zeroes". Scientific American: 54–58. 
^ Gallager, R.G.; van Voorhis, D.C. (1975). "Optimal source codes for geometrically distributed integer alphabets". IEEE Transactions on Information Theory. 21 (2): 228–230. 
^ Abrahams, J. (1997-06-11). Written at Arlington, VA, USA. Division of Mathematics, Computer & Information Sciences, Office of Naval Research (ONR). "Code and Parse Trees for Lossless Source Encoding". Compression and Complexity of Sequences 1997 Proceedings. Salerno: IEEE: 145–171. ISBN 0-8186-8132-2. doi:10.1109/SEQUEN.1997.666911. Retrieved 2016-02-09. 
^ Hu, T. C.; Tucker, A. C. (1971). "Optimal Computer Search Trees and Variable-Length Alphabetical Codes". SIAM Journal on Applied Mathematics. 21 (4): 514. JSTOR 2099603. doi:10.1137/0121057. 



Bibliography[edit]

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp. 385–392.

External links[edit]

Huffman coding in various languages on Rosetta Code







v
t
e


Data compression methods



Lossless




Entropy type



Unary
Arithmetic
Asymmetric Numeral Systems
Golomb
Huffman

Adaptive
Canonical
Modified


Range
Shannon
Shannon–Fano
Shannon–Fano–Elias
Tunstall
Universal

Exp-Golomb
Fibonacci
Gamma
Levenshtein







Dictionary type



Byte pair encoding
DEFLATE
Snappy
Lempel–Ziv

LZ77 / LZ78 (LZ1 / LZ2)
LZJB
LZMA
LZO
LZRW
LZS
LZSS
LZW
LZWL
LZX
LZ4
Brotli
Statistical
Zstd







Other types



BWT
CTW
Delta
DMC
MTF
PAQ
PPM
RLE








Audio




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Companding
Convolution
Dynamic range
Latency
Nyquist–Shannon theorem
Sampling
Sound quality
Speech coding
Sub-band coding





Codec parts



A-law
μ-law
ACELP
ADPCM
CELP
DPCM
Fourier transform
LPC

LAR
LSP


MDCT
Psychoacoustic model
WLPC








Image




Concepts



Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image





Methods



Chain code
DCT
EZW
Fractal
KLT
LP
RLE
SPIHT
Wavelet








Video




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality





Codec parts



Lapped transform
DCT
Deblocking filter
Motion compensation








Theory



Entropy
Kolmogorov complexity
Lossy
Quantization
Rate–distortion
Redundancy
Timeline of information theory








 Compression formats
 Compression software (codecs)










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Huffman_coding&oldid=792304118"					
Categories: 1952 in computer scienceLossless compression algorithmsBinary trees 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoעבריתქართულიMagyarമലയാളംNederlands日本語Norsk bokmålPolskiPortuguêsРусскийСрпски / srpskiSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 25 July 2017, at 19:16.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 









Huffman coding - Wikipedia






















 






Huffman coding

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search





Huffman tree generated from the exact frequencies of the text "this is an example of a huffman tree". The frequencies and codes of each character are below. Encoding the sentence with this code requires 135 bits, as opposed to 288 (or 180) bits if 36 characters of 8 (or 5) bits were used. (This assumes that the code tree structure is known to the decoder and thus does not need to be counted as part of the transmitted information.)




Char
Freq
Code


space
7
111


a
4
010


e
4
000


f
3
1101


h
2
1010


i
2
1000


m
2
0111


n
2
0010


s
2
1011


t
2
0110


l
1
11001


o
1
00110


p
1
10011


r
1
11000


u
1
00111


x
1
10010


In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.



Contents


1 History
2 Terminology
3 Problem definition

3.1 Informal description
3.2 Formalized description
3.3 Example


4 Basic technique

4.1 Compression
4.2 Decompression


5 Main properties

5.1 Optimality


6 Variations

6.1 n-ary Huffman coding
6.2 Adaptive Huffman coding
6.3 Huffman template algorithm
6.4 Length-limited Huffman coding/minimum variance Huffman coding
6.5 Huffman coding with unequal letter costs
6.6 Optimal alphabetic binary trees (Hu–Tucker coding)
6.7 The canonical Huffman code


7 Applications
8 References
9 Bibliography
10 External links



History[edit]
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]
In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of Shannon-Fano coding.
Terminology[edit]
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.
Problem definition[edit]




Constructing a Huffman Tree


Informal description[edit]

Given
A set of symbols and their weights (usually proportional to probabilities).
Find
A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).

Formalized description[edit]
Input.
Alphabet 



A
=

{

a

1


,

a

2


,
⋯
,

a

n


}



{\displaystyle A=\left\{a_{1},a_{2},\cdots ,a_{n}\right\}}

, which is the symbol alphabet of size 



n


{\displaystyle n}

.
Set 



W
=

{

w

1


,

w

2


,
⋯
,

w

n


}



{\displaystyle W=\left\{w_{1},w_{2},\cdots ,w_{n}\right\}}

, which is the set of the (positive) symbol weights (usually proportional to probabilities), i.e. 




w

i


=

w
e
i
g
h
t


(

a

i


)

,
1
≤
i
≤
n


{\displaystyle w_{i}=\mathrm {weight} \left(a_{i}\right),1\leq i\leq n}

.

Output.
Code 



C

(
A
,
W
)

=
(

c

1


,

c

2


,
⋯
,

c

n


)


{\displaystyle C\left(A,W\right)=(c_{1},c_{2},\cdots ,c_{n})}

, which is the tuple of (binary) codewords, where 




c

i




{\displaystyle c_{i}}

 is the codeword for 




a

i


,
1
≤
i
≤
n


{\displaystyle a_{i},1\leq i\leq n}

.

Goal.
Let 



L

(
C
)

=

∑

i
=
1


n




w

i


×

l
e
n
g
t
h


(

c

i


)




{\displaystyle L\left(C\right)=\sum _{i=1}^{n}{w_{i}\times \mathrm {length} \left(c_{i}\right)}}

 be the weighted path length of code 



C


{\displaystyle C}

. Condition: 



L

(
C
)

≤
L

(
T
)



{\displaystyle L\left(C\right)\leq L\left(T\right)}

 for any code 



T

(
A
,
W
)



{\displaystyle T\left(A,W\right)}

.
Example[edit]
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.


Input (A, W)
Symbol (ai)
a
b
c
d
e
Sum


Weights (wi)
0.10
0.15
0.30
0.16
0.29
= 1


Output C
Codewords (ci)
010
011
11
00
10
 


Codeword length (in bits)
(li)
3
3
2
2
2


Contribution to weighted path length
(li wi )
0.30
0.45
0.60
0.32
0.58
L(C) = 2.25


Optimality
Probability budget
(2−li)
1/8
1/8
1/4
1/4
1/4
= 1.00


Information content (in bits)
(−log2 wi) ≈
3.32
2.74
1.74
2.64
1.79
 


Contribution to entropy
(−wi log2 wi)
0.332
0.411
0.521
0.423
0.518
H(A) = 2.205


For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.
As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is





h
(

a

i


)
=

log

2


⁡


1

w

i




.


{\displaystyle h(a_{i})=\log _{2}{1 \over w_{i}}.}



The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:





H
(
A
)
=

∑


w

i


>
0



w

i


h
(

a

i


)
=

∑


w

i


>
0



w

i



log

2


⁡


1

w

i




=
−

∑


w

i


>
0



w

i



log

2


⁡


w

i



.


{\displaystyle H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.}



(Note: A symbol with zero probability has zero contribution to the entropy, since 




lim

w
→

0

+




w

log

2


⁡
w
=
0


{\displaystyle \lim _{w\to 0^{+}}w\log _{2}w=0}

 So for simplicity, symbols with zero probability can be left out of the formula above.)
As a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible for the given alphabet with associated weights. In this example, the weighted average codeword length is 2.25 bits per symbol, only slightly larger than the calculated entropy of 2.205 bits per symbol. So not only is this code optimal in the sense that no other feasible code performs better, but it is very close to the theoretical limit established by Shannon.
In general, a Huffman code need not be unique. Thus the set of Huffman codes for a given probability distribution is a non-empty subset of the codes minimizing 



L
(
C
)


{\displaystyle L(C)}

 for that probability distribution. (However, for each minimizing codeword length assignment, there exists at least one Huffman code with those lengths.)
Basic technique[edit]
Compression[edit]




Visualisation of the use of Huffman coding to encode the message "A_DEAD_DAD_CEDED_A_BAD_BABE_A_BEADED_ABACA_BED". In steps 2 to 6, the letters are sorted by increasing frequency, and the least frequent two at each step are combined and reinserted into the list, and a partial tree is constructed. The final tree in step 6 is traversed to generate the dictionary in step 7. Step 8 uses it to encode the message.






A source generates 4 different symbols 



{

a

1


,

a

2


,

a

3


,

a

4


}


{\displaystyle \{a_{1},a_{2},a_{3},a_{4}\}}

 with probability 



{
0.4
;
0.35
;
0.2
;
0.05
}


{\displaystyle \{0.4;0.35;0.2;0.05\}}

. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:


Symbol
Code


a1
0


a2
10


a3
110


a4
111


The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.


The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, 



n


{\displaystyle n}

. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to 



n


{\displaystyle n}

 leaf nodes and 



n
−
1


{\displaystyle n-1}

 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths.
The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree.
The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority:

Create a leaf node for each symbol and add it to the priority queue.
While there is more than one node in the queue:

Remove the two nodes of highest priority (lowest probability) from the queue
Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes' probabilities.
Add the new node to the queue.


The remaining node is the root node and the tree is complete.

Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2n−1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols.
If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues:

Start with as many leaves as there are symbols.
Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue).
While there is more than one node in the queues:

Dequeue the two nodes with the lowest weight by examining the fronts of both queues.
Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight.
Enqueue the new node into the rear of the second queue.


The remaining node is the root node; the tree has now been generated.

Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity.
In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large.
It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code.
Decompression[edit]
Generally speaking, the process of decompression is simply a matter of translating the stream of prefix codes to individual byte values, usually by traversing the Huffman tree node by node as each bit is read from the input stream (reaching a leaf node necessarily terminates the search for that particular byte value). Before this can take place, however, the Huffman tree must be somehow reconstructed. In the simplest case, where character frequencies are fairly predictable, the tree can be preconstructed (and even statistically adjusted on each compression cycle) and thus reused every time, at the expense of at least some measure of compression efficiency. Otherwise, the information to reconstruct the tree must be sent a priori. A naive approach might be to prepend the frequency count of each character to the compression stream. Unfortunately, the overhead in such a case could amount to several kilobytes, so this method has little practical use. If the data is compressed using canonical encoding, the compression model can be precisely reconstructed with just 



B

2

B




{\displaystyle B2^{B}}

 bits of information (where 



B


{\displaystyle B}

 is the number of bits per symbol). Another method is to simply prepend the Huffman tree, bit by bit, to the output stream. For example, assuming that the value of 0 represents a parent node and 1 a leaf node, whenever the latter is encountered the tree building routine simply reads the next 8 bits to determine the character value of that particular leaf. The process continues recursively until the last leaf node is reached; at that point, the Huffman tree will thus be faithfully reconstructed. The overhead using such a method ranges from roughly 2 to 320 bytes (assuming an 8-bit alphabet). Many other techniques are possible as well. In any case, since the compressed data can include unused "trailing bits" the decompressor must be able to determine when to stop producing output. This can be accomplished by either transmitting the length of the decompressed data along with the compression model or by defining a special code symbol to signify the end of input (the latter method can adversely affect code length optimality, however).
Main properties[edit]
The probabilities used can be generic ones for the application domain that are based on average experience, or they can be the actual frequencies found in the text being compressed. This requires that a frequency table must be stored with the compressed text. See the Decompression section above for more information about the various techniques employed for this purpose.
Optimality[edit]

See also Arithmetic coding#Huffman coding

Although Huffman's original algorithm is optimal for a symbol-by-symbol coding (i.e., a stream of unrelated symbols) with a known input probability distribution, it is not optimal when the symbol-by-symbol restriction is dropped, or when the probability mass functions are unknown. Also, if symbols are not independent and identically distributed, a single code may be insufficient for optimality. Other methods such as arithmetic coding often have better compression capability: Both of these methods can combine an arbitrary number of symbols for more efficient coding, and generally adapt to the actual input statistics, useful when input probabilities are not precisely known or vary significantly within the stream. The primary trade-off is higher computational complexity. Also, arithmetic coding was historically a subject of some concern over patent issues. However, as of mid-2010, the most commonly used techniques for this alternative to Huffman coding have passed into the public domain as the early patents have expired.
Huffman coding still has advantages: it can be used adaptively, accommodating unknown, changing, or context-dependent probabilities. In the case of known independent and identically distributed random variables, combining symbols ("blocking") reduces inefficiency in a way that approaches optimality as the number of symbols combined increases. Huffman coding is optimal when each input symbol is a known independent and identically distributed random variable having a probability that is the inverse of a power of two.
Prefix codes tend to have inefficiency on small alphabets, where probabilities often fall between these optimal points. The worst case for Huffman coding can happen when the probability of a symbol exceeds 2−1 = 0.5, making the upper limit of inefficiency unbounded. Combining symbols together ("blocking") can be used to make Huffman coding theoretically approach the entropy limit, i.e., optimal compression. However, blocking arbitrarily large groups of symbols is impractical, as the complexity of a Huffman code is linear in the number of possibilities to be encoded, a number that is exponential in the size of a block. A practical form of blocking, in widespread use, is run-length encoding. This technique encodes counts (runs) of repeated symbols. For the simple case of Bernoulli processes, Golomb coding is optimal among run-length codes, a fact proved via the techniques of Huffman coding.[4] A similar approach is taken by fax machines using modified Huffman coding.
For a set of symbols with a uniform probability distribution and a number of members which is a power of two, Huffman coding is equivalent to simple binary block encoding, e.g., ASCII coding. This reflects the fact that compression is not possible with such an input.
Variations[edit]
Many variations of Huffman coding exist,[5] some of which use a Huffman-like algorithm, and others of which find optimal prefix codes (while, for example, putting different restrictions on the output). Note that, in the latter case, the method need not be Huffman-like, and, indeed, need not even be polynomial time.
n-ary Huffman coding[edit]
The n-ary Huffman algorithm uses the {0, 1, ... , n − 1} alphabet to encode message and build an n-ary tree. This approach was considered by Huffman in his original paper. The same algorithm applies as for binary (n equals 2) codes, except that the n least probable symbols are taken together, instead of just the 2 least probable. Note that for n greater than 2, not all sets of source words can properly form an n-ary tree for Huffman coding. In these cases, additional 0-probability place holders must be added. This is because the tree must form an n to 1 contractor; for binary coding, this is a 2 to 1 contractor, and any sized set can form such a contractor. If the number of source words is congruent to 1 modulo n-1, then the set of source words will form a proper Huffman tree.
Adaptive Huffman coding[edit]
A variation called adaptive Huffman coding involves calculating the probabilities dynamically based on recent actual frequencies in the sequence of source symbols, and changing the coding tree structure to match the updated probability estimates. It is used rarely in practice, since the cost of updating the tree makes it slower than optimized adaptive arithmetic coding, which is more flexible and has better compression.
Huffman template algorithm[edit]
Most often, the weights used in implementations of Huffman coding represent numeric probabilities, but the algorithm given above does not require this; it requires only that the weights form a totally ordered commutative monoid, meaning a way to order weights and to add them. The Huffman template algorithm enables one to use any kind of weights (costs, frequencies, pairs of weights, non-numerical weights) and one of many combining methods (not just addition). Such algorithms can solve other minimization problems, such as minimizing 




max

i



[

w

i


+

l
e
n
g
t
h


(

c

i


)

]



{\displaystyle \max _{i}\left[w_{i}+\mathrm {length} \left(c_{i}\right)\right]}

, a problem first applied to circuit design.
Length-limited Huffman coding/minimum variance Huffman coding[edit]
Length-limited Huffman coding is a variant where the goal is still to achieve a minimum weighted path length, but there is an additional restriction that the length of each codeword must be less than a given constant. The package-merge algorithm solves this problem with a simple greedy approach very similar to that used by Huffman's algorithm. Its time complexity is 



O
(
n
L
)


{\displaystyle O(nL)}

, where 



L


{\displaystyle L}

 is the maximum length of a codeword. No algorithm is known to solve this problem in linear or linearithmic time, unlike the presorted and unsorted conventional Huffman problems, respectively.
Huffman coding with unequal letter costs[edit]
In the standard Huffman coding problem, it is assumed that each symbol in the set that the code words are constructed from has an equal cost to transmit: a code word whose length is N digits will always have a cost of N, no matter how many of those digits are 0s, how many are 1s, etc. When working under this assumption, minimizing the total cost of the message and minimizing the total number of digits are the same thing.
Huffman coding with unequal letter costs is the generalization without this assumption: the letters of the encoding alphabet may have non-uniform lengths, due to characteristics of the transmission medium. An example is the encoding alphabet of Morse code, where a 'dash' takes longer to send than a 'dot', and therefore the cost of a dash in transmission time is higher. The goal is still to minimize the weighted average codeword length, but it is no longer sufficient just to minimize the number of symbols used by the message. No algorithm is known to solve this in the same manner or with the same efficiency as conventional Huffman coding, though it has been solved by Karp whose solution has been refined for the case of integer costs by Golin.
Optimal alphabetic binary trees (Hu–Tucker coding)[edit]
In the standard Huffman coding problem, it is assumed that any codeword can correspond to any input symbol. In the alphabetic version, the alphabetic order of inputs and outputs must be identical. Thus, for example, 



A
=

{
a
,
b
,
c
}



{\displaystyle A=\left\{a,b,c\right\}}

 could not be assigned code 



H

(
A
,
C
)

=

{
00
,
1
,
01
}



{\displaystyle H\left(A,C\right)=\left\{00,1,01\right\}}

, but instead should be assigned either 



H

(
A
,
C
)

=

{
00
,
01
,
1
}



{\displaystyle H\left(A,C\right)=\left\{00,01,1\right\}}

 or 



H

(
A
,
C
)

=

{
0
,
10
,
11
}



{\displaystyle H\left(A,C\right)=\left\{0,10,11\right\}}

. This is also known as the Hu–Tucker problem, after T. C. Hu and Alan Tucker, the authors of the paper presenting the first linearithmic solution to this optimal binary alphabetic problem,[6] which has some similarities to Huffman algorithm, but is not a variation of this algorithm. These optimal alphabetic binary trees are often used as binary search trees.
The canonical Huffman code[edit]
Main article: Canonical Huffman code
If weights corresponding to the alphabetically ordered inputs are in numerical order, the Huffman code has the same lengths as the optimal alphabetic code, which can be found from calculating these lengths, rendering Hu–Tucker coding unnecessary. The code resulting from numerically (re-)ordered input is sometimes called the canonical Huffman code and is often the code used in practice, due to ease of encoding/decoding. The technique for finding this code is sometimes called Huffman-Shannon-Fano coding, since it is optimal like Huffman coding, but alphabetic in weight probability, like Shannon-Fano coding. The Huffman-Shannon-Fano code corresponding to the example is 



{
000
,
001
,
01
,
10
,
11
}


{\displaystyle \{000,001,01,10,11\}}

, which, having the same codeword lengths as the original solution, is also optimal. But in canonical Huffman code, the result is 



{
110
,
111
,
00
,
01
,
10
}


{\displaystyle \{110,111,00,01,10\}}

.
Applications[edit]
Arithmetic coding and Huffman coding produce equivalent results — achieving entropy — when every symbol has a probability of the form 1/2k. In other circumstances, arithmetic coding can offer better compression than Huffman coding because — intuitively — its "code words" can have effectively non-integer bit lengths, whereas code words in prefix codes such as Huffman codes can only have an integer number of bits. Therefore, a code word of length k only optimally matches a symbol of probability 1/2k and other probabilities are not represented optimally; whereas the code word length in arithmetic coding can be made to exactly match the true probability of the symbol. This difference is especially striking for small alphabet sizes.
Prefix codes nevertheless remain in wide use because of their simplicity, high speed, and lack of patent coverage. They are often used as a "back-end" to other compression methods. DEFLATE (PKZIP's algorithm) and multimedia codecs such as JPEG and MP3 have a front-end model and quantization followed by the use of prefix codes; these are often called "Huffman codes" even though most applications use pre-defined variable-length codes rather than codes designed using Huffman's algorithm.
References[edit]



Wikimedia Commons has media related to Huffman coding.





^ Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes" (PDF). Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898. 
^ Van Leeuwen, Jan (1976). "On the construction of Huffman trees" (PDF). ICALP: 382–410. Retrieved 20 February 2014. 
^ Huffman, Ken (1991). "Profile: David A. Huffman: Encoding the “Neatness” of Ones and Zeroes". Scientific American: 54–58. 
^ Gallager, R.G.; van Voorhis, D.C. (1975). "Optimal source codes for geometrically distributed integer alphabets". IEEE Transactions on Information Theory. 21 (2): 228–230. 
^ Abrahams, J. (1997-06-11). Written at Arlington, VA, USA. Division of Mathematics, Computer & Information Sciences, Office of Naval Research (ONR). "Code and Parse Trees for Lossless Source Encoding". Compression and Complexity of Sequences 1997 Proceedings. Salerno: IEEE: 145–171. ISBN 0-8186-8132-2. doi:10.1109/SEQUEN.1997.666911. Retrieved 2016-02-09. 
^ Hu, T. C.; Tucker, A. C. (1971). "Optimal Computer Search Trees and Variable-Length Alphabetical Codes". SIAM Journal on Applied Mathematics. 21 (4): 514. JSTOR 2099603. doi:10.1137/0121057. 



Bibliography[edit]

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 16.3, pp. 385–392.

External links[edit]

Huffman coding in various languages on Rosetta Code







v
t
e


Data compression methods



Lossless




Entropy type



Unary
Arithmetic
Asymmetric Numeral Systems
Golomb
Huffman

Adaptive
Canonical
Modified


Range
Shannon
Shannon–Fano
Shannon–Fano–Elias
Tunstall
Universal

Exp-Golomb
Fibonacci
Gamma
Levenshtein







Dictionary type



Byte pair encoding
DEFLATE
Snappy
Lempel–Ziv

LZ77 / LZ78 (LZ1 / LZ2)
LZJB
LZMA
LZO
LZRW
LZS
LZSS
LZW
LZWL
LZX
LZ4
Brotli
Statistical
Zstd







Other types



BWT
CTW
Delta
DMC
MTF
PAQ
PPM
RLE








Audio




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Companding
Convolution
Dynamic range
Latency
Nyquist–Shannon theorem
Sampling
Sound quality
Speech coding
Sub-band coding





Codec parts



A-law
μ-law
ACELP
ADPCM
CELP
DPCM
Fourier transform
LPC

LAR
LSP


MDCT
Psychoacoustic model
WLPC








Image




Concepts



Chroma subsampling
Coding tree unit
Color space
Compression artifact
Image resolution
Macroblock
Pixel
PSNR
Quantization
Standard test image





Methods



Chain code
DCT
EZW
Fractal
KLT
LP
RLE
SPIHT
Wavelet








Video




Concepts



Bit rate

average (ABR)
constant (CBR)
variable (VBR)


Display resolution
Frame
Frame rate
Frame types
Interlace
Video characteristics
Video quality





Codec parts



Lapped transform
DCT
Deblocking filter
Motion compensation








Theory



Entropy
Kolmogorov complexity
Lossy
Quantization
Rate–distortion
Redundancy
Timeline of information theory








 Compression formats
 Compression software (codecs)










 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Huffman_coding&oldid=792304118"					
Categories: 1952 in computer scienceLossless compression algorithmsBinary trees 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةAzərbaycancaCatalàČeštinaDanskDeutschEestiΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoעבריתქართულიMagyarമലയാളംNederlands日本語Norsk bokmålPolskiPortuguêsРусскийСрпски / srpskiSuomiSvenskaไทยTürkçeУкраїнськаTiếng Việt中文 
Edit links 





 This page was last edited on 25 July 2017, at 19:16.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 








David Huffman's Travel Photo Tips - Huffman, David - 9781440499517 | HPB









HPBYour favorite local bookstore.Everywhere.ClickSign up / Log inHelp0Your cartLooks like there are no items in here.  Start shopping now and add treasures to your cart. If you have items saved in your cart, sign in to your account to pick up where you left off. Sign in to your accountBrowse Best SellersBooks Popular subjectsYoung Adult FictionChildren's FictionReligionHistoryBiography & AutobiographySee All CategoriesCustomer favoritesBestsellersNew ReleasesBox SetsStaff PicksHPB BlogSuperbuysUnder $10Under $5Under $4Under $3Under $2Under $1Must HaveThe Hottest Book This WeekShop BestsellersMovies & TV Popular subjectsFamilyWesternsTelevision: HBOAction/AdventureTelevision: SeriesDramaTelevision: BBCComedyChildren's VideoCustomer favoritesBest selling moviesNew releasesBlu RayBooks turned into moviesKids and familyAnimeBoxed SetsSuperbuysUnder $5Under $4Under $3Under $2Under $1Good for a LaughThe Top Comedy of the WeekShop ComediesMusic Popular subjectsSoul/R&BPopular MusicFictionJuvenile Grades 4-6 Ages 9-11Country & WesternJazz MusicAudio Books on Tape-CDRomance/ModernDiet & NutritionCustomer favoritesBest selling musicNew releasesBest selling vinylKids and familyMovie soundtracksBoxed setsSuperbuysUnder $5Under $4Under $3Under $2Under $1Teen under $5Kids under $5New TunesThe Top Album of the WeekShop New MusicTextbooks Popular subjectsInternational EditionsSell Us Your TextbooksHomeschoolStudy GuidesCliffs NotesCareerRare Finds BooksSigned BooksFirst EditionsBoxed SetsBrowse Rare FindsMoviesCollectors EditionsBoxed SetsVinylPre 19501950-19541955-19591960-19641965-19691970-NowSuperbuysRare under $10Rare under $5New treasures on our shelvesStart collecting todayShop Rare FindsGift Cards Our StorySign up for our mailing list  and  save! JOIN THE LISTClickFIND A STOREDavid Huffman's Travel Photo Tips: Custom Volumeby Huffman, DavidTake better travel photos with your camera.  Choose and use a new camera, from simple to professional models.  How to use the controls and settings for better photos.  Tips for traveling with cameras and taking interesting pictures.  Using your computer to correct, enhance and share your pictures.  This new volume II expands on the first volume with help choosing and using digital single lens reflex cameras, lenses, flash attachments and computer software.  This is a "must read" for anyone interested in better photos with their camera.ColorCondition: --HPB condition ratingsNew: Item is brand new, unused and unmarked, in flawless condition.Fine/Like New (F): No defects, little usage. May show remainder marks. Older books may show minor flaws.Very Good (VG): Shows some signs of wear and is no longer fresh. Attractive. Used textbooks do not come with supplemental materials.Good (G): Average used book with all pages present. Possible loose bindings, highlighting, cocked spine or torn dust jackets. Used textbooks do not come with supplemental materials.Fair (FR): Obviously well-worn, but no text pages missing. May be without endpapers or title page. Markings do not interfere with readability. Used textbooks do not come with supplemental materials.Poor (P): All text is legible but may be soiled and have binding defects. Reading copies and binding copies fall into this category. Used textbooks do not come with supplemental materials.Conditions GuideFormat: PaperbackSold by: --Language: EnglishPublisher: CreatespaceISBN-13: 9781440499517ISBN: 1440499519Publication Year: 2009HPB pick - Out of stockLoading...Loading marketplace...HPB condition ratingsNew: Item is brand new, unused and unmarked, in flawless condition.Fine/Like New (F): No defects, little usage. May show remainder marks. Older books may show minor flaws.Very Good (VG): Shows some signs of wear and is no longer fresh. Attractive. Used textbooks do not come with supplemental materials.Good (G): Average used book with all pages present. Possible loose bindings, highlighting, cocked spine or torn dust jackets. Used textbooks do not come with supplemental materials.Fair (FR): Obviously well-worn, but no text pages missing. May be without endpapers or title page. Markings do not interfere with readability. Used textbooks do not come with supplemental materials.Poor (P): All text is legible but may be soiled and have binding defects. Reading copies and binding copies fall into this category. Used textbooks do not come with supplemental materials.Conditions GuideHPB condition ratingsNew: Mint condition or still sealed (SS). Absolutely perfect in every way. New.Fine/Like New (EX): No defects, little sign of use, well cared for. Plays perfectly. Close to new. Not necessarily sealed or unused, but close. Could be an unopened promotional or cut item. Sometimes called: mint-minus.Very Good (VG): Will show some signs that it was played and otherwise handled by a previous owner who took good care of it.Good (G): Attractive and well cared for, but no longer fresh. Minor signs of wear, scuffing or scratching, but will play almost perfectly. For vinyl: barely detectable crackles or pops.Fair (FR): This item is in okay condition. For vinyl: good is not so good and the record may have low level crackles or pops when playing. CD: one or more tracks may skip.Poor (P): Obviously well-worn and handled. Most vinyl collectors will not buy good or below, but some tracks on CD or vinyl will play.Conditions GuideHPB condition ratingsNew: This movie is unopened and brand new.Fine/Like New (EX): Near new. No defects, little sign of use. Plays perfectly. Not necessarily sealed or unused, but close. No skipping; no fuzzy or snowy frames in VHS.Very Good (VG): Attractive and well cared for but no longer fresh. Minor signs of wear, but will play almost perfectly. For VHS: barely detectable distortion or very few fuzzy or snowy frames.Good (G): This item is in okay condition and basically works well. There may be some minor distortion on VHS tape; slight scratching or wear on DVD.Fair (FR): Basically plays, but may be obviously well-worn with some scratching or tape distortion.Poor (P): Disc or tape is intact, but may be scratched or stretched. There may be skips or distortion or product defects.Conditions Guide×

Books, eBooks & Audio / Christian Living / Spirituality / Spiritual Growth / Lessons from Biblical Figures - Christianbook.com









































Hear about sales, receive special offers & more.You can unsubscribe at any time.


Enter email address














My Account

Wishlist

Help

Email Signup


Cart (0)  items
Checkout

Welcome to Christianbook.com! Sign in or create an account 




Search by title, author, isbn, etc.







Dollar Days- Shop $10 or Less





Tumblers, Water Bottles, & More





Fab Fridays







Search within

All Products
Books
eBook
Music
MP3 Downloads
Accompaniment
Bibles
Homeschool
Gift & Home
DVDs
Christian Living
Children
Fiction
Software







Search



Bibles
VBS
Church
Bible Study
Books
eBooks
Fiction
Homeschool
Music
Kids
Toys
Gifts
DVD
Bargain



BestsellersPrayerChristian LivingSpanish ProductsMP3sVBSChurch SuppliesBible CoversFamilyGift CardsCatalogMembers














Browse

All Products

Books, eBooks & Audio&times

Christian Living&times

Spirituality&times

Spiritual Growth&times

Lessons from Biblical Figures&times (891)















Refine by

Occasion▼▲ChristmasEasterLentAudience▼▲TeensWomenMenMothersCouplesSeekers / New BelieversLeadersPastorsSinglesSmall GroupsYoung AdultBinding▼▲HardcoverPaperbackTrade PaperbackFeatures▼▲Discussion QuestionsStudy GuideStudy QuestionsFormat▼▲Large PrintUnabridgedLanguage▼▲EnglishFrenchSpanishMedia Type▼▲Audio CDeBookMP3MP3 CDSeries▼▲ABCEGPRTGuides & Workbooks▼▲Participant's GuidesStand Alone WorkbookSupplemental Study GuideSupplemental WorkbooksDRM▼▲DRM FreeDRM Protected
Author / Artist▼▲ A B C D E F G H I J K L M N O P R S T U V W Y ZPublisher▼▲ 1 4 A B C D E F G H I J K L M N O P R S T U V W X ZTop Rated▼▲1+ Stars2+ Stars3+ Stars4+ Stars5 starsPrice▼▲$0-$5$5-$10$10-$25$25-$50$50-$100Discount▼▲10%+ Off20%+ Off30%+ Off40%+ Off50%+ Off60%+ Off70%+ Off80%+ Off90%+ Off


Advanced Search Links

Advanced Search
Commentary Search
Bible Finder
Homeschool Finder
Song Search
Bible Study Search











                Bestseller
             
 


 
 


                refine
             





                25
             
 


                50
             






 
 


 




< Back
Refine
X


Shop By Category


All Products                    
Books, eBooks & AudioChristian LivingSpiritualitySpiritual GrowthLessons from Biblical Figures 


All Products:
                        
Books, eBooks & AudioChristian LivingSpiritualitySpiritual GrowthLessons from Biblical Figures 


 
 
Refine By


Occasion                    



Occasion:
                        





                                Christmas                            

 


                                Easter                            

 


                                Lent                            

 
 
Audience                    



Audience:
                        





                                Teens                            

 


                                Women                            

 


                                Men                            

 


                                Mothers                            

 


                                Couples                            

 


                                Seekers/New Believers                            

 


                                Leaders                            

 


                                Pastors                            

 


                                Singles                            

 


                                Small Groups                            

 


                                Young Adult                            

 
 
Binding                    



Binding:
                        





                                Hardcover                            

 


                                Paperback                            

 


                                Trade Paperback                            

 
 
Features                    



Features:
                        





                                Discussion Questions                            

 


                                Study Guide                            

 


                                Study Questions                            

 
 
Format                    



Format:
                        





                                Large Print                            

 


                                Unabridged                            

 
 
Language                    



Language:
                        





                                English                            

 


                                French                            

 


                                Spanish                            

 
 
Media Type                    



Media Type:
                        





                                Audio CD                            

 


                                eBook                            

 


                                MP3                            

 


                                MP3 CD                            

 
 
Series                    



Series:
                        





                                A                            

 


                                B                            

 


                                C                            

 


                                E                            

 


                                G                            

 


                                P                            

 


                                R                            

 


                                T                            

 
 
Guides & Workbooks                    



Guides & Workbooks:
                        





                                Participant's Guides                            

 


                                Stand Alone Workbook                            

 


                                Supplemental Study Guide                            

 


                                Supplemental Workbooks                            

 
 
DRM                    



DRM:
                        





                                DRM Free                            

 


                                DRM Protected                            

 
 
Author/Artist                    



Author/Artist:
                        





                                 A                            

 


                                 B                            

 


                                 C                            

 


                                 D                            

 


                                 E                            

 


                                 F                            

 


                                 G                            

 


                                 H                            

 


                                 I                            

 


                                 J                            

 


                                 K                            

 


                                 L                            

 


                                 M                            

 


                                 N                            

 


                                 O                            

 


                                 P                            

 


                                 R                            

 


                                 S                            

 


                                 T                            

 


                                 U                            

 


                                 V                            

 


                                 W                            

 


                                 Y                            

 


                                 Z                            

 
 
Publisher                    



Publisher:
                        





                                 1                            

 


                                 4                            

 


                                 A                            

 


                                 B                            

 


                                 C                            

 


                                 D                            

 


                                 E                            

 


                                 F                            

 


                                 G                            

 


                                 H                            

 


                                 I                            

 


                                 J                            

 


                                 K                            

 


                                 L                            

 


                                 M                            

 


                                 N                            

 


                                 O                            

 


                                 P                            

 


                                 R                            

 


                                 S                            

 


                                 T                            

 


                                 U                            

 


                                 V                            

 


                                 W                            

 


                                 X                            

 


                                 Z                            

 
 
Top Rated                    



Top Rated:
                        





                                1+ Stars                            

 


                                2+ Stars                            

 


                                3+ Stars                            

 


                                4+ Stars                            

 


                                5 stars                            

 
 
Price                    



Price:
                        





                                $0-$5                            

 


                                $5-$10                            

 


                                $10-$25                            

 


                                $25-$50                            

 


                                $50-$100                            

 
 
Discount                    



Discount:
                        





                                10%+ Off                            

 


                                20%+ Off                            

 


                                30%+ Off                            

 


                                40%+ Off                            

 


                                50%+ Off                            

 


                                60%+ Off                            

 


                                70%+ Off                            

 


                                80%+ Off                            

 


                                90%+ Off                            

 
 






                    Bestseller                

 


                    Price: High to Low                

 


                    Price: Low to High                

 


                    Publication Date                

 


                    Customer Rating                

 


                    Savings (%)                

 






Lessons from Biblical Figures


 



showing 1 - 25 of 891 results 


 

Add To Cart
Add To Wishlist  
Twelve Extraordinary Women: How God Shaped Women of the Bible and What He Wants to Do with You
John MacArthur John MacArthur Thomas Nelson / 2008 / Trade Paperback 
$8.99
Retail: $16.99 Save 47%
($8.00)  4.5 Stars Out Of 5 92 Reviews Availability: In Stock Stock No: WW280285  

Add To Cart
Add To Wishlist  
Facing Your Giants: God Still Does the Impossible
Max Lucado Max Lucado Thomas Nelson / 2013 / Trade Paperback 
$7.79
Retail: $15.99 Save 51%
($8.20)  5 Stars Out Of 5 35 Reviews Availability: In Stock Stock No: WW947492  

Add To Cart
Add To Wishlist  
Twelve Ordinary Men
John MacArthur John MacArthur Thomas Nelson / 2006 / Trade Paperback 
$8.99
Retail: $16.99 Save 47%
($8.00)  5 Stars Out Of 5 67 Reviews Availability: In Stock Stock No: WW88244  

Add To Cart
Add To Wishlist  
Colliding with Destiny: Finding Hope in the Legacy of Ruth
Sarah Jakes Sarah Jakes Bethany House / 2014 / Hardcover 
$3.99
Retail: $24.99 Save 84%
($21.00)  4.5 Stars Out Of 5 9 Reviews Availability: In Stock Stock No: WW212116  

Add To Cart
Add To Wishlist  
Finding Spiritual Whitespace: Awakening Your Soul to Rest
Bonnie Gray Bonnie Gray Revell / 2014 / Trade Paperback 
$0.99
Retail: $14.00 Save 93%
($13.01)  5 Stars Out Of 5 18 Reviews Availability: In Stock Stock No: WW721794  eBOOK

More Info
Add To Wishlist  
Losers Like Us: Redefining Discipleship after Epic Failure / Digital original - eBook
Daniel Hochhalter Daniel Hochhalter David C. Cook / 2014 / ePub 
$6.39
Retail: $7.99 Save 20%
($1.60)  5 Stars Out Of 5 2 Reviews Availability: In Stock Stock No: WW64369EB  

Add To Cart
Add To Wishlist  
Really Bad Girls of the Bible, updated and repackaged
Liz Curtis Higgs Liz Curtis Higgs WaterBrook / 2016 / Trade Paperback 
$8.79
Retail: $15.99 Save 45%
($7.20)  5 Stars Out Of 5 19 Reviews Availability: In Stock Stock No: WW428615  

Add To Cart
Add To Wishlist  
The Girl's Still Got It: Take a Walk with Ruth and the God Who Rocked Her World
Liz Curtis Higgs Liz Curtis Higgs WaterBrook / 2012 / Trade Paperback 
$8.49
Retail: $15.99 Save 47%
($7.50)  5 Stars Out Of 5 51 Reviews Availability: In Stock Stock No: WW564484 Video  

Add To Cart
Add To Wishlist  
Are You Fit for Life?
Jack Graham Jack Graham Crossway / 2007 / Hardcover 
$13.79
Retail: $17.99 Save 23%
($4.20)  Availability: In Stock Stock No: WW49125  

Add To Cart
Add To Wishlist  
Slightly Bad Girls of the Bible: Flawed Women Loved by a Flawless God
Liz Curtis Higgs Liz Curtis Higgs WaterBrook / 2007 / Trade Paperback 
$9.99
Retail: $14.99 Save 33%
($5.00)  5 Stars Out Of 5 22 Reviews Availability: In Stock Stock No: WW072125  

Add To Cart
Add To Wishlist  
Chase the Lion: If Your Dream Doesn't Scare You, It's Too Small
Mark Batterson Mark Batterson Multnomah Books / 2016 / Hardcover 
$13.49
Retail: $19.99 Save 33%
($6.50)  5 Stars Out Of 5 17 Reviews Availability: In Stock Stock No: WW428851 Video  

Add To Cart
Add To Wishlist  
Twelve Unlikely Heroes - Slightly Imperfect
John MacArthur John MacArthur Thomas Nelson / 2014 / Trade Paperback 
$5.99
Retail: $15.99 Save 63%
($10.00)  Availability: In Stock Stock No: WW206117DA  

Add To Cart
Add To Wishlist  
Job: A Man of Heroic Endurance
Charles R. Swindoll Charles R. Swindoll Thomas Nelson / 2009 / Trade Paperback 
$10.99
Retail: $15.99 Save 31%
($5.00)  5 Stars Out Of 5 7 Reviews Availability: In Stock Stock No: WW202508  

Add To Cart
Add To Wishlist  
Esther: A Woman of Strength & Dignity
Charles R. Swindoll Charles R. Swindoll Thomas Nelson / 2008 / Trade Paperback 
$9.29
Retail: $15.99 Save 42%
($6.70)  5 Stars Out Of 5 2 Reviews Availability: In Stock Stock No: WW202232  

Add To Cart
Add To Wishlist  
Average Joe: God's Extraordinary Calling to Ordinary Men
Troy Meeder Troy Meeder Multnomah Books / 2011 / Trade Paperback 
$0.99
Retail: $13.99 Save 93%
($13.00)  4 Stars Out Of 5 13 Reviews Availability: In Stock Stock No: WW423078 Video  

Add To Cart
Add To Wishlist  
Paul: A Man of Grace and Grit
Charles R. Swindoll Charles R. Swindoll Thomas Nelson / 2009 / Trade Paperback 
$9.29
Retail: $15.99 Save 42%
($6.70)  5 Stars Out Of 5 5 Reviews Availability: In Stock Stock No: WW202591  

Add To Cart
Add To Wishlist  
Watershed Moments: Turning Points That Change the Course of Our Lives
Gari Meacham Gari Meacham Zondervan / 2013 / Trade Paperback 
$2.99
Retail: $14.99 Save 80%
($12.00)  4.5 Stars Out Of 5 2 Reviews Availability: In Stock Stock No: WW308669  

Add To Cart
Add To Wishlist  
Jesus: The Greatest Life of All
Charles R. Swindoll Charles R. Swindoll Thomas Nelson / 2009 / Trade Paperback 
$10.99
Retail: $15.99 Save 31%
($5.00)  5 Stars Out Of 5 24 Reviews Availability: In Stock Stock No: WW202584  

Add To Cart
Add To Wishlist  
Great Lives: Joseph: A Man of Integrity and Forgiveness
Charles R. Swindoll Charles R. Swindoll Thomas Nelson / 2008 / Trade Paperback 
$9.29
Retail: $15.99 Save 42%
($6.70)  Availability: In Stock Stock No: WW280339  

Add To Cart
Add To Wishlist  
The Apostle: A Life of Paul
John Pollock John Pollock David C. Cook / 2011 / Trade Paperback 
$9.99
Retail: $16.99 Save 41%
($7.00)  5 Stars Out Of 5 1 Reviews Availability: In Stock Stock No: WW405731  

Add To Cart
Add To Wishlist  
Twelve Unlikely Heroes
John MacArthur John MacArthur Thomas Nelson / 2014 / Trade Paperback 
$9.29
Retail: $15.99 Save 42%
($6.70)  4.5 Stars Out Of 5 30 Reviews Availability: In Stock Stock No: WW206117 Video  

Add To Cart
Add To Wishlist  
Beautiful Outlaw: Experiencing the Playful, Disruptive, Extravagant Personality of Jesus
John Eldredge John Eldredge FaithWords / 2013 / Trade Paperback 
$9.49
Retail: $16.00 Save 41%
($6.51)  4.5 Stars Out Of 5 46 Reviews Availability: In Stock Stock No: WW525706  

Add To Cart
Add To Wishlist  
Waiting on God: What to Do When God Does Nothing
Wayne Stiles Wayne Stiles Baker Books / 2015 / Trade Paperback 
$2.99
Retail: $16.00 Save 81%
($13.01)  5 Stars Out Of 5 5 Reviews Availability: In Stock Stock No: WW018459  

Add To Cart
Add To Wishlist  
In the Eye of the Storm, repackaged
Max Lucado Max Lucado Thomas Nelson / 2012 / Trade Paperback 
$8.29
Retail: $15.99 Save 48%
($7.70)  Availability: In Stock Stock No: WW947322  

Add To Cart
Add To Wishlist  
The Way of the Dragon or the Way of the Lamb
Jamin Goggin, Kyle Strobel Jamin Goggin, Kyle Strobel Thomas Nelson / 2017 / Trade Paperback 
$10.99
Retail: $16.99 Save 35%
($6.00)  5 Stars Out Of 5 5 Reviews Availability: In Stock Stock No: WW022358 




Displaying items 1-25 of 891

Page 1 of 36
12345 Next
|Last 








Sign Up To ReceiveExclusive Email OffersYou can unsubscribe at any time




Sign Up To Receive Exclusive Email OffersYou can unsubscribe at any time





Sign Up







Connect With Us








 Account

Checkout
Account
Order History
Wishlist
Cart
Subscribe Email
Unsubscribe Email


 Shipping & Returns

Shipping & Handling Rates
Easy Returns






 Services

Affiliate Program
Gift Cards
Membership
Self-Publishing


 Catalog

Catalog Quick Shop
Online Catalogs
Request our Catalogs







 Community

Social Media
Prayer Wall
Author Profiles


 About Us

Company Info.
Job Opportunities


 Help

Customer Service
FAQ






 Shops

Bibles
Homeschool
Bargains
VBS
Christian Living
Toys
Gifts
Kids
DVDs
Fiction
Music
Academics







Desktop Site










 1-800-CHRISTIAN1-800-247-4784(Outside the United Statesand Canada Call: 978-977-5000)




 Email Us


140 Summit St. Peabody, MA 01960





Tax Information for VT and LA Residents
Conditions of Use
Privacy Notice
© 2017 Christianbook.com, LLC
* 65 *



























David's Mighty Men:  John Huffman: 9781625097194 - Christianbook.com























































Hear about sales, receive special offers & more.You can unsubscribe at any time.


Enter email address














My Account

Wishlist

Help

Email Signup


Cart (0)  items
Checkout

Welcome to Christianbook.com! Sign in or create an account 




Search by title, author, isbn, etc.







Dollar Days- Shop $10 or Less





Tumblers, Water Bottles, & More





Fab Fridays







Search within

All Products
Books
eBook
Music
MP3 Downloads
Accompaniment
Bibles
Homeschool
Gift & Home
DVDs
Christian Living
Children
Fiction
Software







Search



Bibles
VBS
Church
Bible Study
Books
eBooks
Fiction
Homeschool
Music
Kids
Toys
Gifts
DVD
Bargain



BestsellersPrayerChristian LivingSpanish ProductsMP3sVBSChurch SuppliesBible CoversFamilyGift CardsCatalogMembers










David's Mighty Men
 By: John Huffman




 









Buy Item
$12.59 Retail:  $13.99  Save 10% ($1.40) In Stock Stock No: WW097194 Xulon Press / 2013 / Paperback 
Quantity:


 1 2 3 4 5 6 7 8 9 10+ 

 






            Add To Cart
            Add To Cart
 












            Add To Wishlist
            Add To Wishlist



























Quantity:


 1 2 3 4 5 6 7 8 9 10+ 

 






            Add To Cart
            Add To Cart

  











                Wishlist
                Wishlist






David's Mighty Men By: John Huffman
Xulon Press / 2013 / Paperback Write a Review

In StockStock No: WW097194



 


Browse

All Products

Books, eBooks & Audio&times

Christian Living&times

Spirituality&times

Spiritual Growth&times

Lessons from Biblical Figures&times (891)















Refine by

Occasion▼▲ChristmasEasterLentAudience▼▲TeensWomenMenMothersCouplesSeekers / New BelieversLeadersPastorsSinglesSmall GroupsYoung AdultBinding▼▲HardcoverPaperbackTrade PaperbackFeatures▼▲Discussion QuestionsStudy GuideStudy QuestionsFormat▼▲Large PrintUnabridgedLanguage▼▲EnglishFrenchSpanishMedia Type▼▲Audio CDeBookMP3MP3 CDSeries▼▲ABCEGPRTGuides & Workbooks▼▲Participant's GuidesStand Alone WorkbookSupplemental Study GuideSupplemental WorkbooksDRM▼▲DRM FreeDRM Protected
Author / Artist▼▲ A B C D E F G H I J K L M N O P R S T U V W Y ZPublisher▼▲ 1 4 A B C D E F G H I J K L M N O P R S T U V W X ZTop Rated▼▲1+ Stars2+ Stars3+ Stars4+ Stars5 starsPrice▼▲$0-$5$5-$10$10-$25$25-$50$50-$100Discount▼▲10%+ Off20%+ Off30%+ Off40%+ Off50%+ Off60%+ Off70%+ Off80%+ Off90%+ Off


Advanced Search Links

Advanced Search
Commentary Search
Bible Finder
Homeschool Finder
Song Search
Bible Study Search



 Product Close-up
This product is not available for expedited shipping.* This product is available for shipment only to the USA.





          Others Also Purchased (15)
       



Add To Cart







▼▲Others Also Purchased (15)
    


Add To Cart




Description Availability Price
Add Include


 
 






Interpreting the Bible & the Constitution
                

In Stock 
$81.25




Add To Cart
 $81.25 

 
 






Genesis, Volume 1: ArmorQuest
                

In Stock 
$4.49
Retail: $7.99 



Add To Cart
 $4.49 

 
 






The Creedal Imperative
                

In Stock 
$12.99
Retail: $16.99 



Add To Cart
 $12.99 

 
 






The Dynamic Heart in Daily Life: Connecting Christ to Human Experience
                

In Stock 
$15.99
Retail: $19.99 



Add To Cart
 $15.99 

 
 






The Remaining--Graphic Novel
                

In Stock 
$8.49
Retail: $16.99 



Add To Cart
 $8.49 

 
 






Feasts of the Bible: Participant Guide  - Slightly Imperfect
                

In Stock 
$4.79
Retail: $9.99 



Add To Cart
 $4.79 

 
 






A Friend of the King: David and His Mighty Men
                

In Stock 
$12.70




Add To Cart
 $12.70 

 
 






Becoming a Mighty Man of God
                

In Stock 
$13.49
Retail: $14.99 



Add To Cart
 $13.49 

 
 






No Other Gods
                

In Stock 
$15.29
Retail: $16.99 



Add To Cart
 $15.29 

 
 






Sensing Jesus: Life and Ministry as a Human Being - eBook
                

In Stock 
$9.89
Retail: $15.99 



Add To Cart
 0 $9.89 

 
 






David: Favored Friend of God
                

In Stock 
$10.99
Retail: $14.99 



Add To Cart
 $10.99 

 
 






Christ, Baptism and the Lord's Supper: Recovering the Sacraments for Evangelical Worship
                

In Stock 
$16.49
Retail: $23.00 



Add To Cart
 $16.49 

 
 






Jesus Is: Find a New Way to Be Human
                

In Stock 
$8.99
Retail: $16.99 



Add To Cart
 $8.99 

 
 






Political Church: The Local Assembly as Embassy of Christ's Rule
                

In Stock 
$24.99
Retail: $40.00 



Add To Cart
 $24.99 

 
 






A Heart for God: Learning from David Through the Tough Choices of Life
                

In Stock 
$18.00
Retail: $20.00 



Add To Cart
 $18.00 

 







View All 15 Products 

 

Add To Cart









Product Information▼▲
Format: PaperbackNumber of Pages: 154Vendor: Xulon PressPublication Date: 2013Dimensions: 9.02 X 5.98 X 0.36 (inches)ISBN: 1625097190ISBN-13: 9781625097194

Other Customers Also Purchased


 

Add To Cart
Add To Wishlist  
Interpreting the Bible & the Constitution
Jaroslav Pelikan Jaroslav Pelikan Yale University Press / Hardcover 
$81.25
  

Add To Cart
Add To Wishlist  
Genesis, Volume 1: ArmorQuest
Ben Avery, Sherwin Schwartzrock Ben Avery, Sherwin Schwartzrock Osmango Distribution / 2006 / Trade Paperback 
$4.49
Retail: $7.99 Save 44%
($3.50)   

Add To Cart
Add To Wishlist  
The Creedal Imperative
Carl R. Trueman Carl R. Trueman Crossway / 2012 / Trade Paperback 
$12.99
Retail: $16.99 Save 24%
($4.00)  4.5 Stars Out Of 5 3 Reviews  

Add To Cart
Add To Wishlist  
The Dynamic Heart in Daily Life: Connecting Christ to Human Experience
Jeremy Pierre Jeremy Pierre New Growth Press / 2016 / Trade Paperback 
$15.99
Retail: $19.99 Save 20%
($4.00)  5 Stars Out Of 5 1 Reviews 


Related Products


 

Add To Cart
Add To Wishlist  
For the Love of God, Volume 1
D.A. Carson D.A. Carson Crossway / 2006 / Trade Paperback 
$13.49
Retail: $23.99 Save 44%
($10.50)  4.5 Stars Out Of 5 3 Reviews  

Add To Cart
Add To Wishlist  
Rethinking Retirement: Finishing Life for the Glory of Christ Unabridged Audiobook on CD
John Piper & Arthur Morey (Narrator) John Piper & Arthur Morey (Narrator) christianaudio / Compact disc 
$4.99
Retail: $5.98 Save 17%
($0.99)   

Add To Cart
Add To Wishlist  
Of Other Worlds
C.S. Lewis C.S. Lewis HarperOne / 2017 / Trade Paperback 
$7.79
Retail: $13.99 Save 44%
($6.20)   

Add To Cart
Add To Wishlist  
Bearing False Witness: Debunking Centuries of Anti-Catholic History
Rodney Stark Rodney Stark Templeton Press / 2016 / Hardcover 
$25.16
Retail: $27.95 Save 10%
($2.79)  


Publisher's Description▼▲
This book takes the reader on a journey into the rocky and desolate wilderness of Judea to catch a glimpse of the "mighty men" of David. I Chronicles 11 and II Samuel 23 are the two passages in Scripture that describe these men in detail. But these warriors are also followed through the narrative of David's reign, where names like Abishai and Benaiah appear in numerous accounts, giving us great insight into their courage and loyalty throughout a stormy reign marked by various foreign invasions, court intrigues, and violent insurrections. The modern world needs such men: men like the "three mighties" who "brake through the host of the Philistines" to deliver a vessel of water to their beloved commander, men like Abishai who said to his uncle, David, when he asked for a volunteer for a perilous mission, "I will go down with thee," men like Benaiah, who "slew a lion in a pit in a snowy day," men like Eleazar who stood alone against a Philistine onslaught, slaying the enemies of the Lord until his hand "clave unto the sword," men like the Zebulunites who "could keep rank" and were "expert in war," and men like the Issacharites who had "understanding of the times." This book highlights 12 essential character qualities of David's mighty men that are desperately needed today. It is the prayer of the writer of these pages that the Lord might raise up a generation of Christians who will determine, with all their hearts, to follow the example of these stalwart warriors of the Old Testament.


Product Reviews▼▲
Be the first to write a review!


Write Review




Ask a Question▼▲

Have a question about this product? Ask us here.

Find Related Products▼▲
Books, eBooks & Audio >> Christian Living >> Spirituality >> Spiritual Growth >> Lessons from Biblical Figures

Author/Artist Review▼▲
I'm the author/artist and I want to review David's Mighty Men.

Back&times
Ask a Question


What would you like to know about this product? Please enter your name, your email and your question regarding the product in the fields below, and we'll answer you in the next 24-48 hours.
If you need immediate assistance regarding this product or any other, please call 1-800-CHRISTIAN to speak directly with a customer service representative.



Name:



Email:



Question:




Submit

Edit

Confirm









Sign Up To ReceiveExclusive Email OffersYou can unsubscribe at any time




Sign Up To Receive Exclusive Email OffersYou can unsubscribe at any time





Sign Up







Connect With Us








 Account

Checkout
Account
Order History
Wishlist
Cart
Subscribe Email
Unsubscribe Email


 Shipping & Returns

Shipping & Handling Rates
Easy Returns






 Services

Affiliate Program
Gift Cards
Membership
Self-Publishing


 Catalog

Catalog Quick Shop
Online Catalogs
Request our Catalogs







 Community

Social Media
Prayer Wall
Author Profiles


 About Us

Company Info.
Job Opportunities


 Help

Customer Service
FAQ






 Shops

Bibles
Homeschool
Bargains
VBS
Christian Living
Toys
Gifts
Kids
DVDs
Fiction
Music
Academics







Desktop Site










 1-800-CHRISTIAN1-800-247-4784(Outside the United Statesand Canada Call: 978-977-5000)




 Email Us


140 Summit St. Peabody, MA 01960





Tax Information for VT and LA Residents
Conditions of Use
Privacy Notice
© 2017 Christianbook.com, LLC
* 51 *

































Dr. David Huffman, MD - Chattanooga, TN - Endocrinology, Diabetes & Metabolism & Internal Medicine | Healthgrades.com









































HealthgradesDr. David Huffman, MD(42)Login MenuSearch Doctors, Hospitals, Specialties or ProceduresCloseHealthgradesSearchSearchNearSearchDr. David Huffman, MDSaveEndocrinology, Diabetes & Metabolism  | Male(42) Leave a ReviewDavid M Huffman MD 5616 Brainerd Rd Chattanooga,  TN 37411 Get Directions (423) 265-3561View Insurance Accepted  Suggest an editDr. David Huffman, MD is an endocrinology, diabetes & metabolism doctor who practices in Chattanooga, TN.  He is 61 years old and has been practicing for 35 years.  Dr. Huffman is affiliated with Memorial Hospital.ADVERTISEMENT






 




Learn about this doctorDr. Huffman's Care PhilosophyWe offer advanced, individualized, empathetic care for people with diabetes mellitus and other metabolic disorders, by people who live with those problems. Our providers and staff have 200 years of collective experience with insulin pump therapy. We share a comprehensive understanding of advanced insulin treatment, and education is our primary concern. We produce original research and participate in international research programs in diabetes and metabolic disorders.Dr. Huffman's Experience0ExperienceMatchExperience MatchSee how Dr. Huffman's experience matches your preferences.1SpecialtiesFor the best healthcare for your needs, choose a doctor who specializes in your medical condition.2Board CertificationsBoard certification indicates that a doctor is highly qualified in the medical field in which they practice.49Conditions TreatedCheck to see that this provider treats your medical condition.6Procedures PerformedCheck to see that this provider performs the procedure that you need.Background CheckCheck to see if your provider has any malpractices, board actions, or sanctions.EducationLearn more about where this provider went to medical school, residency, and more.Awards & RecognitionView information about special awards and recognition for this provider.Languages SpokenCheck to see what languages this provider and/or staff speak.Memberships & Professional AffiliationsView memberships and affiliations associated with this provider.Overall Patient Satisfaction42responsesLikelihood of recommending Dr. Huffman to family and friends is 4 out of 5Have you seen Dr. Huffman?Leave a ReviewSee all 219 Endocrinology, Diabetes & Metabolism Specialists in Chattanooga, TN 37411Dr. George McLean, MDInternal Medicine(21)Dr. David Kunz, MDEndocrinology, Diabetes & Metabolism(40)Dr. Nicole Nicome, MDInternal Medicine(29)Dr. Paul Bernard, MDPediatric Endocrinology(20)Dr. Dianne Roland, MDEndocrinology, Diabetes & Metabolism(17)Dr. Zachary Kistka, MDEndocrinology, Diabetes & Metabolism(16)View MoreVisiting Dr. HuffmanDr. Huffman accepts 12 insurance carriersView all insurance carriers Dr. Huffman is now accepting new patients BRAINARD Office5616 Brainerd RdChattanooga, TN 37411(423) 265-3561Get DirectionsAffiliated HospitalsHospital QualityCheck out the quality of care at the 1 hospitals where Dr. Huffman has admitting privileges.Memorial Hospital2525 Desales AveChattanooga, TN 37404Hospital Awards0Overall Patient ExperienceView Hospital ProfileDr. Huffman's ReviewsLikelihood of recommending Dr. Huffman to family and friends4.0 based on 42 total reviews (8 with comments)Leave a reviewDr. Huffman's PerformanceTrustworthinessVERY GOODExplains condition(s) wellVERY GOODAnswers QuestionsVERY GOODTime well spentVERY GOODOffice & Staff PerformanceSchedulingVERY GOODOffice environmentEXCELLENTStaff friendlinessVERY GOODAverage wait time 16 to 30 minutesPatient Comments (8)15Clermont, GA | Mar 04, 2017I was under the care of Dr Huffman for almost 2 yrs. During this time, he did a whole lot of experimenting but came up with no answers. The receptionist really needs to be replaced! She changed my appt and didn't consult with me. My husband drove me over an hr. for her to say I changed the appt. I still had the voice mail "she" left confirming that days appt. To make it worse, she told me the Dr wasn't even there. I heard him talking, looked up and saw him. Needless to say that was my last appt.55-Julia in Cleveland, TN | Feb 23, 2017I really like the staff. Everyone seems very helpful. Jamie is amazing! I have not had to wait long at all. I have received more information about my diabetes than any where else I have been. 15-James McKnight in Signal Mountain | Feb 21, 2017The waiting is unacceptable.  Over an hour and a half for a follow up.  I cancelled my appointment and advised the receptionist I would not reschedule.  I would seek someone that can keep up with his schedule. Very poor practice for a Doctor.55-David in Chattanooga,Tn | Jan 24, 2017A doctor that actually cares although wait times are sometimes longer than normal but its worth it.55-Mrs.Wilhite in Dalton, GA | May 12, 2016i would highly recommend Dr. Huffman he is very experienced at what he do. He make sure that he checks for all possibilities before he treats the problem. The wait times are longer than  i want to wait but its ok. His office staff is always very nice.Dr. Huffman was a blessing to me!!!55-TN Playwright in Chattanooga, TN | Mar 07, 2016Dr. Huffman is very knowledgable in his field.  You may wait a little extra time to get in to see him, but once he enters the room he stays until you have asked all your questions.  You never feel like he's rushing.  He makes you feel like you are the only patient in the office.  His office staff has been very accommodating making appointments for me and following up on my phone calls. I'm an RN and have seen many MD's over my 38 year career.  This guy is top of the MD line!!25-Agnes A. in Cleveland,  TN | Feb 16, 2016Staff could be more helpful and mire empathetic, in particular front desk staff.  Clinicians seem to be fine but front desk acts as if patients are bothering them.  Wait times are well over an hour just to get to the exam room then another 15 to 30 mins wait for doctor.  15-unhappy customer in Chattanooga, TN | Jan 20, 2016Appt to see if premenpause causing migraines.I waited 2 hours.He asked me a few questions and took a look over my body sent me to get blood drawn.2nd appt he went over results.Which I asked ?s & he kept speaking in Dr terms.Said for me to take diet pill so my problems would go away.which no it wont could have made me have a syndrome.Then in the dr. notes found out he said so many lies about what he went over with me and also about what I said.  Do not go here. It can cost your life.More CommentsHave you seen Dr. David Huffman, MD?Leave a Review.Are you Dr. Huffman?Make it easy for patients to share their feedback. Also manage your personalized profile!Post your responseCompare Dr. HuffmanCompare this DoctorView other doctors with similar experience to find the right doctor for you.Dr. George McLean, MDInternal Medicine2051B Hamill Rd Ste 401Hixson, TN 37343Patient Satisfaction(21)Distance7.9  miles awayDr. David Kunz, MDEndocrinology, Diabetes & Metabolism1655 Lebanon Rd Ste ALawrenceville, GA 30043Patient Satisfaction(40)Distance97  miles awayDr. Nicole Nicome, MDInternal Medicine7580 Springbox Dr Ste 250Fairburn, GA 30213Patient Satisfaction(29)Distance100.3  miles awayDr. Paul Bernard, MDPediatric Endocrinology10700 Medlock Bridge Rd Ste 102Duluth, GA 30097Patient Satisfaction(20)Distance89  miles awayDr. Dianne Roland, MDEndocrinology, Diabetes & Metabolism2700 Westside Dr NW # 203Cleveland, TN 37312Patient Satisfaction(17)Distance22.7  miles awayDr. Zachary Kistka, MDEndocrinology, Diabetes & Metabolism1272 Garrison DrMurfreesboro, TN 37129Patient Satisfaction(16)Distance90.1  miles awayDr. Philip Berry, MDEndocrinology, Diabetes & Metabolism550 Peachtree St NE Ste 1010Atlanta, GA 30308Patient Satisfaction(5)Distance98  miles awayDr. Rohini Kasturi, MDEndocrinology, Diabetes & Metabolism127 N Oak AveCookeville, TN 38501Patient Satisfaction(16)Distance81.5  miles awayDr. Harini Jalagani, MDEndocrinology, Diabetes & Metabolism962 Joe Frank Harris Pkwy SE Ste 207Cartersville, GA 30120Patient Satisfaction(22)Distance60.9  miles awayDr. James Rone, MDEndocrinology, Diabetes & Metabolism1272 Garrison DrMurfreesboro, TN 37129Patient Satisfaction(6)Distance90.1  miles awayView all 10 matchesSee all diabetes, metabolism & endocrinologists in Chattanooga, TN×Dr. Huffman's Care PhilosophyWe offer advanced, individualized, empathetic care for people with diabetes mellitus and other metabolic disorders, by people who live with those problems. Our providers and staff have 200 years of collective experience with insulin pump therapy. We share a comprehensive understanding of advanced insulin treatment, and education is our primary concern. We produce original research and participate in international research programs in diabetes and metabolic disorders.×Dr. Huffman's experience matches your search based on the following criteria: Based on total number of patients treated over the last 12 months Specializes in Endocrinology, Diabetes & Metabolism Board certified in Diabetes, Metabolism & Endocrinology and Internal Medicine No malpractice claims found No sanctions found No board actions found×Awards & RecognitionAwards & HonorsHealthgrades Honor RollWhat is a recognized doctor Healthgrades Recognized Doctor designation identifies leading doctors who:Are board certified.Have not had their license surrendered or revoked since Healthgrades started collecting data in 2000.Have no malpractice judgments, adverse arbitration awards, or monetary settlements for the last five years in the states in which Healthgrades can collect malpractice data.Are free of state or federal disciplinary actions (sanctions) for the last five years.Healthgrades updates the Recognized Doctor list quarterly based on board certification data. Healthgrades also receives sanction and malpractice data throughout the year, depending on how frequently the state medical boards release updates. We remove a newly sanctioned doctor from the Recognized Doctor list as soon as we receive the information. However, it is important to note that malpractice information is publically available in only 14 states.Media & PublicationsDr. Huffman has no media or publications listed.Background Check 0 Malpractice ClaimsNo malpractice history found for Tennessee.What is medical malpracticeWhat is medical malpractice?Medical malpractice is issued when negligence by a doctor causes injury to a patient. For example, a doctor may improperly diagnose, treat or medicate outside the standard of medical care. The three types of malpractice are: a settlement, an arbitration award, or a judgment.If my doctor has malpractice history, does that mean he or she is a poor-quality doctor?If your doctor has a malpractice claim, evaluate the information and determine if the action could potentially impact the quality of care you receive. Claim settlements and arbitration awards may occur for a variety of reasons, which should not necessarily reflect negatively on the doctor's professional competence or conduct.You may want to use this information to start a discussion with the doctor about his or her history and specific ability to provide healthcare for you.How far back does Healthgrades malpractice history go?Healthgrades reports details of a doctor’s malpractice history when the doctor has at least one closed medical malpractice claim within the last five years, even if he or she no longer practices in that state.For which states does Healthgrades collect malpractice history?Healthgrades collects malpractice information from California, Colorado, Connecticut, Florida, Georgia, Illinois, Massachusetts, Nevada, New Jersey, New York, North Carolina, Oregon, Tennessee, Texas, Vermont, Virginia and West Virginia. If your doctor has a malpractice claim, evaluate the information and determine if the action could potentially impact your quality of care. Sometimes multiple states report the same claim. If a provider practices in a state where data is unavailable, please reach out to your local state legislature to help make this data publicly available.0SanctionsNo sanctions history found for the years that Healthgrades collects data.What is a sanction or disciplinary actionWhat is a sanction or disciplinary action?A sanction, also known as a disciplinary action, is an action taken to punish or restrict a doctor who has demonstrated professional misconduct. Sanctions may be imposed by a state medical board, professional medical licensing organization, or the U.S. Department of Health and Human Services.If my doctor has sanction history, does that mean he or she is a poor-quality doctor?If a doctor has a sanction, it does not necessarily mean that he or she is a poor-quality doctor. Some sanctions are not related to medical care, and involve a doctor’s finances or administrative activities. Before you make any choices about changing your doctor, we recommend that you evaluate the doctor’s sanction information and determine how severe or relevant you think the sanction cause and action were.How far back does Healthgrades sanction history go?Healthgrades reports state and federal sanctions from the previous five years, except when a doctor's license has been revoked or surrendered. Healthgrades displays all actions for doctors whose licenses have been revoked or surrendered.For which states does Healthgrades collect sanction history?Healthgrades collects sanction history from all 50 U.S. states. Physicians with a disciplinary action in one state may move to another state where they have a clean record. Since Healthgrades painstakingly compiles disciplinary action information from all 50 states, Healthgrades website will show if a physician has a disciplinary action in more than one state.0Board ActionsNo board actions found for the years that Healthgrades collects data.What are board actionsWhat are board actions?Board actions are non-disciplinary actions imposed upon a doctor based on a complaint investigation. A patient or medical colleague may file a complaint with that state medical board or professional licensing organization, which then investigates the complaint. Board actions are intended to ensure that a doctor is able to perform safe medical and health care tasks.Types of non-disciplinary actions include an advisory letter, a corrective action agreement, a limitation or restriction on the medical or healthcare tasks a doctor can perform, or a voluntary agreement by the doctor not to practice. A board action can also include a termination of a corrective action agreement or voluntary agreement, which allows the doctor to return to full practice.If my doctor has a board action, does that mean he or she is a poor-quality doctor?If a doctor has a board action, it means he or she has had a non-disciplinary action imposed upon him or her. It does not necessarily mean that he or she is a poor quality doctor. Before you make any choices about changing your doctor, evaluate the doctor's board action information and determine how severe or relevant you think the cause and action were.How far back does Healthgrades non-disciplinary board action history go?Healthgrades reports non-disciplinary board action history from for the previous five years, except when a doctor's license has been revoked or surrendered. Healthgrades displays all actions for doctors whose licenses have been revoked or surrendered.For which states does Healthgrades collect non-disciplinary board actions?Healthgrades collects non-disciplinary board actions from all 50 U.S. states.Board CertificationsWhy It Matters: Dr. Huffman's Board CertificationsBoard certification should be one of your top considerations when choosing a doctor. Board certification is an official recognition given to doctors who have met specific requirements set by national medical specialty boards in the... More Why It Matters: Dr. Huffman's Board CertificationsBoard certification should be one of your top considerations when choosing a doctor. Board certification is an official recognition given to doctors who have met specific requirements set by national medical specialty boards in the United States.Board certification indicates that a doctor is highly qualified in the medical field in which he or she practices. A board-certified doctor is more likely than a non-board-certified doctor to have the most current skills and knowledge about how to treat your medical condition.Dr. Huffman is Board Certified in:Diabetes, Metabolism & EndocrinologyAccrediting Board: American Board of Medical Specialties *Internal MedicineAccrediting Board: American Board of Internal Medicine ** This information is proprietary data maintained in a copyrighted database compilation owned by the American Board of Medical Specialties. Copyright 2017 American Board of Medical Specialties. All rights reserved.Less Diabetes, Metabolism & EndocrinologyInternal MedicineConditions TreatedAdrenal Gland DiseasesAdrenal IncidentalomaAdrenal InsufficiencyAutoimmune DiseasesAutoimmune Thyroid DiseasesBenign TumorBone DisordersCalcium Metabolism DisordersCancerCongenital Adrenal Hyperplasia (CAH)CraniopharyngiomaDiabetes InsipidusDiabetes Type 1Diabetes Type 2Diabetes With Renal ManifestationsDiabetic RetinopathyEndocrine DisordersFemale InfertilityGoiterGraves' DiseaseGrowth Hormone DeficiencyHashimoto's DiseaseHypercalcemiaHyperlipidemiaHyperparathyroidismHyperthyroidismHypoglycemiaHypokalemiaHypoparathyroidismHypopituitarismHypothyroidismKidney DiseaseLipid DisordersMalaise and FatigueMenstrual Disorders (incl. Dysmenorrhea)Metabolic DisordersObesityOsteopeniaOsteoporosisPituitary DiseasePolycystic Ovarian SyndromeProteinuriaRetina DiseasesSymptomatic MenopauseTesticular DysfunctionThyroid CancerThyroid DiseaseThyroiditisVitamin D DeficiencyEducationMedical SchoolUniversity Of North Carolina, Chapel Hill, School Of MedicineGraduated in 1982Internship HospitalBaylor College Of MedCompleted in 1984Residency HospitalBaylor College Med Affil HospsCompleted in 1986Fellowship HospitalBaylor College Of MedCompleted in 1989Undergraduate SchoolNcGraduated in 1977University NcLanguages SpokenEnglishMemberships & Professional AffiliationsDr. Huffman does not have any memberships or affiliations listed. If you are Dr. Huffman and would like to add memberships or affiliations, please update your free profile.SpecialtiesEndocrinology, Diabetes & MetabolismProcedures PerformedContinuous Glucose MonitoringDiabetes CounselingInsulin Pump TherapyThyroid Nodule EvaluationUltrasound, ThyroidUltrasound-Guided Fine Needle Aspiration






















David in Huffman, Texas with Reviews - YP.comStart your search by typing in the business name below.
What do you want to find?

Where?My current locationSearchHomeHuffman, TXDavidHuffman, TX DavidAbout Search ResultsAbout Search ResultsYP - The Real Yellow PagesSM - helps you find the right local businesses to meet your specific needs. Search results are sorted by a combination of factors to give you a set of choices in response to your search criteria. These factors are similar to those you might use to determine which business to select from a local Yellow Pages directory, including proximity to where you are searching, expertise in the specific services or products you need, and comprehensive business information to help evaluate a business's suitability for you. “Preferred” listings, or those with featured website buttons, indicate YP advertisers who directly provide information about their businesses to help consumers make more informed buying decisions. YP advertisers receive higher placement in the default ordering of search results and may appear in sponsored listings on the top, side, or bottom of the search results page.Sort:DefaultDefaultDistanceRatingName (A - Z)Sponsored LinksAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection1. Advocate David3347 Golden Willow DrKingwood, TX 77339(281) 360-8057AttorneysGeneral Practice AttorneysAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection2. Cruz & David's Hair Salon(3)5324 Atascocita Rd Ste DHumble, TX 77346(281) 852-0464Beauty SalonsBarbersHair BraidingWebsiteMake an AppointmentHave to keep the hair looking good and Norma takes care of that for me. She is busy but always works me in. ThanksAdd to mybookRemove from mybookAdded to your health collection!Error when adding to health collectionThis business was removed from the health collection3. Villacres, David MD3407 River Edge TrailHumble, TX 77339(281) 361-5990Physicians & SurgeonsServicesAdd to mybookRemove from mybookAdded to your health collection!Error when adding to health collectionThis business was removed from the health collection4. Weed, David H, MD2755 W Lake Houston PkwyKingwood, TX 77339(713) 442-2100Physicians & Surgeons, PediatricsFrom Business: Dr. Weed received his undergraduate degree in Biochemistry from Ohio State University. He earned his medical degree from Ohio State University College of Medicine and completed his residency in Pediatrics at Baylor College of Medicine.Add to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection5. Cruz & David's Hair & Nail Salon5520 Fm 1960 Rd E Ste AHumble, TX 77346(281) 358-1261Beauty SalonsNail SalonsHair StylistsMake an AppointmentAdd to mybookRemove from mybookAdded to your health collection!Error when adding to health collectionThis business was removed from the health collection6. Dr. David W Olson, MD5603 Beaver Lodge DrKingwood, TX 77345(281) 361-5266Physicians & SurgeonsAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection7. G Carpenter David DO Jd7821 Fm 1960 Rd EHumble, TX 77346(281) 812-1352General ContractorsAdd to mybookRemove from mybookAdded to your health collection!Error when adding to health collectionThis business was removed from the health collection8. David Anderson7651 Fm 1960 Rd EHumble, TX 77346(281) 812-1085DentistsAdd to mybookRemove from mybookAdded to your health collection!Error when adding to health collectionThis business was removed from the health collection9. David Beltran19121 W Lake Houston Pkwy Ste EHumble, TX 77346(281) 446-2153DentistsAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection10. David Weekley Homes1910 Leatherstem LnKingwood, TX 77345(281) 361-3168Home BuildersReal Estate DevelopersWebsiteAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection11. David Najera4596 Dunnam PlHouston, TX 77365(713) 283-3833PlumbersWebsiteAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection12. David Patrick PC3523 Deerbrook DrKingwood, TX 77339(281) 999-7543Accountants-Certified PublicWebsiteAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection13. David Weekley Homes13706 Overton Woods DrHumble, TX 77346(832) 243-4157Home BuildersReal Estate DevelopersWebsiteAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection14. David R. Brewer, Attorney At Law, PLLCFour Kingwood Place, 900 Rockmead, Suite 132Kingwood, TX 77339(281) 359-8686AttorneysBusiness Law AttorneysWebsiteAdd to mybookRemove from mybookAdded to your other collection!Error when adding to other collectionThis business was removed from the other collection15. David V Janiga5727 Woodland Creek DrKingwood, TX 77345(281) 360-9919No Internet Heading AssignedAdd to mybookRemove from mybookAdded to your other collection!Error when adding to other collectionThis business was removed from the other collection16. Gross David4315 Brook Shadow DrKingwood, TX 77345(281) 360-8258No Internet Heading AssignedAdd to mybookRemove from mybookAdded to your other collection!Error when adding to other collectionThis business was removed from the other collection17. Walrath David J17522 Bering Bridge LnHumble, TX 77346(832) 445-0327No Internet Heading AssignedAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection18. Nancy David Realty-RE/MAX Associates Northeast II18700 W Lake Houston PkwyHumble, TX 77346(281) 812-6748Real Estate InvestingReal Estate AgentsWebsiteAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection19. David Weekley Homes17630 Bridger Bend LnHumble, TX 77346(281) 812-1369Home BuildersConstruction ManagementWebsiteServicesAdd to mybookRemove from mybookAdded to your health collection!Error when adding to health collectionThis business was removed from the health collection20. Barnhart David A Md20035 W Lake Houston Pkwy Ste 100Humble, TX 77346(281) 359-1000Physicians & Surgeons, PediatricsAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection21. David Weekley Homes18402 Wide Brim CtHumble, TX 77346(281) 852-4623General ContractorsDrywall ContractorsWebsiteServicesAdd to mybookRemove from mybookAdded to your other collection!Error when adding to other collectionThis business was removed from the other collection22. Beck David20806 Lake Park TrlHumble, TX 77346(832) 644-1463No Internet Heading AssignedAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection23. David Rush Construction404 Gum Gully RdCrosby, TX 77532(281) 324-4980Building ContractorsAdd to mybookRemove from mybookAdded to your other collection!Error when adding to other collectionThis business was removed from the other collection24. Short David Equipment Company Inc168 County Road 6057Dayton, TX 77535(936) 258-3775Oil Field EquipmentAdd to mybookRemove from mybookAdded to your home collection!Error when adding to home collectionThis business was removed from the home collection25. David C Newell3119 Foley RdCrosby, TX 77532(281) 462-8822Land SurveyorsAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection26. Booher David2218 Northpark DrKingwood, TX 77339(281) 359-4224InsuranceAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection27. Singleton David MD3525 Fm 1960 Rd EHumble, TX 77338(281) 358-2049Skin CareWebsiteAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection28. David Booher Ins1801 Kingwood DrKingwood, TX 77339(281) 359-4224InsuranceAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection29. David R. Brewer, Attorney At Law, PLLC900 Rockmead Dr Ste 132Kingwood, TX 77339(281) 359-8686AttorneysGeneral Practice AttorneysWebsiteAdd to mybookRemove from mybookAdded to your services collection!Error when adding to services collectionThis business was removed from the services collection30. Edward Jones - Financial Advisor: David M Harris800 Rockmead Dr Ste 112Kingwood, TX 77339(281) 407-8104Financial Planning ConsultantsWebsiteDirectionsFrom Business: Edward Jones is an investment firm that believes your financial goals deserve a face to face conversation. We can help you prepare for retirement, save for education and be a tax-smart investor.Sponsored LinksShowing1-30
of 1227results12345NextRelated Articlesfor Davidmore related articles »Which Roof Types Are Suitable for Solar Installations? »How to Conduct a DIY Roof Inspection »A simple roof inspection can be completed with a pair of binoculars. Learn what to look for.Types of Roofing Material »Roofing materials vary in terms of price, weight and maintenance. Learn the basics and choose which type is best for your home.Map ViewSponsoredCrim Law Firm PC The4900 Travis St, Houston, TX 77002(832) 919-8978WebsiteDirectionsMore InfoFabio & Merrill Attys at Law(1)12 Greenway Plz, Houston, TX 77046(832) 460-2237Call Today For The Highest Quality Legal Service!WebsiteDirectionsMore InfoPackard, LaPray Attorney At Law1240 Orleans St, Beaumont, TX 77701(409) 242-2931WebsiteDirectionsMore InfoHoyt & Kahn Attorney Office116 S Avenue C, Humble, TX 77338(832) 995-0509WebsiteDirectionsMore InfoHolladay Law Firm, PLLC9595 Six Pines Dr Ste 8210, Spring, TX 77380(281) 771-1331WebsiteDirectionsMore InfoTerry & Thweatt, P.C. Attorneys At Law(43)1 Greenway Plz, Houston, TX 77046(713) 481-7984Galveston 409-945-2000 or Toll Free 866-870-0008WebsiteDirectionsVideoMore InfoDeGuerin Dickson & Ward(2)1018 Preston St, Houston, TX 77002(713) 714-5088WebsiteServices OfferedDirectionsMore InfoLaw Offices Of Steve Lee5823 Gulf Fwy, Houston, TX 77023(713) 999-4216SERVING THE HOUSTON AREA FOR 40 YEARSWebsiteDirectionsMore InfoRegan Armstrong & Associates PLLC12777 Jones Rd, Houston, TX 77070(281) 894-1590Caring, Capable Resource for Your Family Law NeedsWebsiteServices OfferedDirectionsMore InfoLyons & Marek14505 Torrey Chase Blvd Ste 300, Houston, TX 77014(281) 583-2777Please Contact Us to Schedule A ConsultationWebsiteYP AdDirectionsMore InfoFealy Law Firm P.C.1235 North Loop W, Houston, TX 77008(832) 460-2411WebsiteSubmit FormDirectionsVideoMore InfoThompson & Knight333 Clay St, Houston, TX 77002(713) 654-8111Contact Us For InformationWebsiteDirectionsMore InfoSloan Firm3000 Smith St, Houston, TX 77006(713) 999-5175Because What Matters to YOU Matters to US.WebsiteDirectionsMore InfoAnderson Terence J Attorney at Law601 Sawyer St Ste 200, Houston, TX 77007(713) 522-6886WebsiteDirectionsMore InfoMaida Law Firm4320 Calder Ave, Beaumont, TX 77706(409) 898-8200WebsiteVirtual TourDirectionsVideoMore InfoRon Helson Attorney at Law124 W Myrtle St, Angleton, TX 77515(713) 666-6961WebsiteDirectionsMore InfoLaw Offices of David M White(12)1500 Industrial Blvd Ste 303, Abilene, TX 79602(325) 437-3311Helping the injured, defending the accused.WebsiteServices OfferedDirectionsMore InfoThe Law Offices of Juan Hernandez(7)Serving the Huffman area.(888) 287-0349Call Now, Free 24/7 ConsultationWebsiteMore InfoLegalFoxServing the Huffman area.(888) 981-8587Find a Local AttorneyWebsiteMore InfoA TRAFFIC TICKET ATTORNEYS CALIFORNIA & NATIONWIDE(37)Serving the Huffman area.(877) 787-2334Commercial - Taxi - DUI - Or ANY Driver, California & NationwideWebsiteMore InfoLegal Aid Legal Services Corp.Serving the Huffman area.(888) 877-7150WebsiteMore InfoTVC Pro DriverServing the Huffman area.(877) 623-3533WebsiteMore Info24/7 LegalServing the Huffman area.(888) 659-7710WebsiteContact UsMore InfoCommunity Legal SolutionsServing the Huffman area.(888) 280-0993We can help! Contact us today.WebsiteContact UsMore Info24/7 Legal(3)Serving the Huffman area.(888) 659-7710Need A Lawyer?  Call 24/7 Legal.  Get Help Now.WebsiteContact UsMore InfoDidn't find what you were looking for?magnifying glassDid You Know?Different Types of Roofing ServicesThe type of roofer you will need will depend on whether you require a brand new roof or small repairs. You need to find a contractor with a history of excellent service in a particular roofing project.Installation - A roofer can put a brand new roof on your home or commercial building. When this is done as part of original construction, the main contractor will probably perform the task, or may hire a specialist. This is a big project so you may want to find a company who can offer a crew to install your roof over a shorter time period.Since this is a more involved process, it is also more expensive. Home Advisor says you should expect to pay between $4,000 to $9,000 for a roof installation. This, of course, depends on what type of material the roofer uses, the size of your roof, the code requirements of your area and even the angle - or pitch - of your roof.Replacement - If you want to replace your current roof, you can hire a company or individual to strip your roof and install new shingles or panels. Some roofing options, such as metal tiles, can be placed on top of existing shingles and are easy enough to install that a single contractor can perform the task efficiently.If the contractor can install the new roof without removing the old one, it is going to cost you less than a complete replacement. Installing lighter materials will also save you money because it requires fewer people and can be completed faster.Repair - When you have a leak, it may be time to contact a local roofer. Some problems are obvious and you'll want to call a contractor quickly before damage spreads. A roofer will replace holes in your roof after severe weather or fix your gutters when they stop draining properly.The average roof repair project will cost you a few hundred dollars. Each tasks is different and you should get a price breakdown for each roof repair project. Make sure you are charged a fair price for materials and time.Maintenance - There are a variety of tasks that can help maintain the quality of your roofing materials. You may want to invest in sealant or coatings that prevent moisture accumulation or reflect sunlight.A popular project these days is installing solar panels or other environmentally conscious solutions to save money and make your property greener. When pricing these options, make sure to compare how much the project costs to how much the contractor projects you should save on utilities and heating expesnes.Inspection - If you are selling your home or business, you'll probably want to contact a roofer to inspect your property for any issues that may affect the value. Whatever your reason for inspections, a roofing expert can tell you possible causes for concerns or provide ideas for future maintenance.Depending on your area, you may be able to find a roofing contractor who will give you a free evaluation, but you should expect to pay a couple hundred dollars for a full inspection.When is a New Roof or Repair Necessary?Certain events demand you reach out to a roofing expert - such as when a tree crashes through your home. If, however, you're unsure a visit from a contractor is warranted, here are a few signs it's time to give your roof some attention:Obvious Damage
The easiest way to tell if your roof needs repair or replacement is to look at it. You should be able to spot broken or missing shingles. Look for subtler signs, such as roof tiles curling or damage around corners and chimneys. Inconsistent coloring could indicate moss or water damage.If you really want to give your roof a thorough inspection, Good Housekeeping advises property owners to check gutters for broken down granules or interiors for structural sagging.Not Doing its Job
Your roof is supposed to protect you from environmental elements. If you find water in your home after a rainstorm or feel a draft on cold days, it may mean there are gaps in your roof panels or other problems. A good place to check is your basement or the uppermost level of your property. Try turning out the lights during the day and checking for signs of sunlight breaking through your roof.You can also check your energy bill. When you feel like you pay too much for heat or cooling, roof repairs could help you cut costs.After a Major Weather Event
You should always inspect your roof for signs of damage after a major weather event. Wind can blow shingles off your property and hail is a common cause of dents or breakage. If your home suffers from a fire, you need to check for internal structural damage.Warranty Expires
All roof materials - even metal shingles and panels - have an expiration date. If you've had your roof for over 20 years, you should consider having a professional give it a full inspection. Problems aren't always obvious, but you can check your original contractor statements for when it's time to reevaluate the work.Questions to Ask Before Hiring Roofing ContractorAfter doing your research, you should talk to a roofer in person and ask specific questions before hiring him or her for your project:What Are Your Qualifications?
The first thing you need to check for is a local address. Its easier to trust a roofer with a history in the community that proudly displays contact information. Not all states require a roofing license, but you should at least check to see if a company or contractor belongs to a national roofing organization or the local chamber of commerce. Ask about any award or certifications the company has earned in previous years.Can I Have a Cost Report?
No matter how dependable a roofer may seem, never depend on a verbal agreement. You want to get every promise down in writing. An extensive job cost report shows you exactly what work a roofer will perform and how much each service will cost you.Go line by line and ask about every expense. You should also check to make sure the job completely covers the necessary task, so ask about additional services the contractor could perform for water protection or extended warranty.What Are Your Plans for Emergencies?
Roofing can be dangerous. You need to make sure that the company or contractor carries liability insurance so you are not held responsible for accidents. Ask to see the proper paperwork and inquire about safety precautions. You should also check if the roofer has contingencies for smaller emergencies, such as rain delays.How Will You Protect My Property?
Roofing projects can lead to debris and even structural damage. You should invite a roofer to inspect your property before beginning a project so you can point out landscaping or other external features they should look out for. If the contractor doesn't seem to care about aesthetical concerns, you shouldn't expect the finished product to help your curb appeal.What's Your Warranty?
You need to inquire about how long the work should last. A major factor in longevity will be the materials used. You should research online about how long metal or concrete shingles should perform and make sure the business's guaranteed warranty is in line with industry standards.Can I Get a Reference?
Not only should you ask to get in contact with past clients, Time Magazine advises homeowners to visit projects currently underway to see how the contractor or company actually performs during a job. You can also go online and check out other customer reviews.How Does One Know What Type of Roof is Suitable?Roofing materials are made out of concrete, metal, plastic and a variety of other substances. Before you settle on material and a design, you should consider these factors:Environmental Concerns
If you live in an area prone to wildfires, you may want to avoid flammable roofing materials. You have to be aware of your risks for damage and prevent common causes of trouble in your initial buying. This doesn't disqualify all materials. For example, if you worry about moisture but want asphalt shingles, you can purchase algae-resistant versions. If you are designing a home, a pitched roof is better at preventing moisture acclimation than a flat one.Duration
The longer you plan to live in your current home or occupy a commercial space, the more you should invest in a durable roof. The roof of your structure will be a major selling point should you decide to move.Aesthetics
Some roofing materials, such as metal and plastic, have a variety of available looks and designs, while clay tile is very popular for an authentic appearance. You must consider how the roof should look alongside the rest of your architecture and talk to your contractor about the aesthetic values of high-quality materials. You should also consider skylights, chimneys or other obstacles to consistent patterns when planning a design.Cost
Once you consider your regional area, necessary durability and aesthetic appeal, you should take your needs to a roofer who can give you the most cost-efficient option that will satisfy your concerns. You should always create a budget for your roof installation or repair.Prevention TipsIf you wish to keep visits from a professional roofer to a minimum, there are some steps you can take to maintain your roof on your own:Perform Simple Maintenance
Water in your roof is a major cause of mold and leaks. Regularly cleaning your gutters to prevent debris build-up is a best practice. You should also check your flashing and shingle alignment for holes or cracks in the surface. When a roofer installs new materials, ask about specific maintenance you should perform.Be Aware of Your Risks
If you have a tree near your home, you have to look out for wet leaves accumulating on your roof and the potential danger of falling branches. Homes in colder climates may suffer from freezing, so you want to make sure your roof is properly insulated and prevent heavy snows from weighing down your structure. Before any major weather event, you should inspect your home for possible weaknesses and contact a roofer to prevent trouble.Stay Vigilant
The primary solution for any possible roof damage is prevention. You can't take your property for granted. You should inspect your roof at regular intervals. Have a professional roofing contractor evaluate your structure after major incidents and well before your warranty expires. 
We just redesigned yp.com! Do you like it?×            LikeNot a Fan×            Thank You!Feedback
